{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#ray-tracing-practice","title":"Ray Tracing Practice","text":"<ul> <li>Ray Tracing in One Weekend</li> </ul>"},{"location":"one_weekend/adding_a_sphere/","title":"Adding a Sphere","text":"<p>Let\u2019s add a single object to our ray tracer. People often use spheres in ray tracers because calculating whether a ray hits a sphere is relatively simple.</p>"},{"location":"one_weekend/adding_a_sphere/#ray-sphere-intersection","title":"Ray-Sphere Intersection","text":"<p>The equation for a sphere of radius $r$ that is centered at the origin is an important mathematical equation:</p> <pre><code>$$ x^2 + y^2 + z^2 = r^2 $$\n</code></pre> <p>You can also think of this as saying that if a given point $(x,y,z)$ is on the sphere, then $x^2 + y^2 + z^2 = r^2$. If a given point $(x,y,z)$ is inside the sphere, then $x^2 + y^2 + z^2 &lt; r^2$, and if a given point $(x,y,z)$ is outside the sphere, then $x^2 + y^2 + z^2 &gt; r^2$.</p> <p>If we want to allow the sphere center to be at an arbitrary point $(C_x, C_y, C_z)$, then the equation becomes a lot less nice:</p> <p>$$ (x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2 = r^2 $$</p> <p>In graphics, you almost always want your formulas to be in terms of vectors so that all the $x$/$y$/$z$ stuff can be simply represented using a <code>vec3</code> class. You might note that the vector from center $\\mathbf{C} = (C_x, C_y, C_z)$ to point $\\mathbf{P} = (x,y,z)$ is $(\\mathbf{P} - \\mathbf{C})$. If we use the definition of the dot product:</p> <p>$$ (\\mathbf{P} - \\mathbf{C}) \\cdot (\\mathbf{P} - \\mathbf{C})      = (x - C_x)^2 + (y - C_y)^2 + (z - C_z)^2   $$</p> <p>Then we can rewrite the equation of the sphere in vector form as:</p> <p>$$ (\\mathbf{P} - \\mathbf{C}) \\cdot (\\mathbf{P} - \\mathbf{C}) = r^2 $$</p> <p>We can read this as \u201cany point $\\mathbf{P}$ that satisfies this equation is on the sphere\u201d. We want to know if our ray $\\mathbf{P}(t) = \\mathbf{A} + t\\mathbf{b}$ ever hits the sphere anywhere. If it does hit the sphere, there is some $t$ for which $\\mathbf{P}(t)$ satisfies the sphere equation. So we are looking for any $t$ where this is true:</p> <p>$$ (\\mathbf{P}(t) - \\mathbf{C}) \\cdot (\\mathbf{P}(t) - \\mathbf{C}) = r^2 $$</p> <p>which can be found by replacing $\\mathbf{P}(t)$ with its expanded form:</p> <p>$$ ((\\mathbf{A} + t \\mathbf{b}) - \\mathbf{C})       \\cdot ((\\mathbf{A} + t \\mathbf{b}) - \\mathbf{C}) = r^2 $$</p> <p>We have three vectors on the left dotted by three vectors on the right. If we solved for the full dot product we would get nine vectors. You can definitely go through and write everything out, but we don't need to work that hard. If you remember, we want to solve for $t$, so we'll separate the terms based on whether there is a $t$ or not:</p> <p>$$ (t \\mathbf{b} + (\\mathbf{A} - \\mathbf{C}))       \\cdot (t \\mathbf{b} + (\\mathbf{A} - \\mathbf{C})) = r^2 $$</p> <p>And now we follow the rules of vector algebra to distribute the dot product:</p> <p>$$ t^2 \\mathbf{b} \\cdot \\mathbf{b}      + 2t \\mathbf{b} \\cdot (\\mathbf{A}-\\mathbf{C})      + (\\mathbf{A}-\\mathbf{C}) \\cdot (\\mathbf{A}-\\mathbf{C}) = r^2   $$</p> <p>Move the square of the radius over to the left hand side:</p> <p>$$ t^2 \\mathbf{b} \\cdot \\mathbf{b}      + 2t \\mathbf{b} \\cdot (\\mathbf{A}-\\mathbf{C})      + (\\mathbf{A}-\\mathbf{C}) \\cdot (\\mathbf{A}-\\mathbf{C}) - r^2 = 0   $$</p> <p>It's hard to make out what exactly this equation is, but the vectors and $r$ in that equation are all constant and known. Furthermore, the only vectors that we have are reduced to scalars by dot product. The only unknown is $t$, and we have a $t^2$, which means that this equation is quadratic. You can solve for a quadratic equation by using the quadratic formula:</p> <p>$$ \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$</p> <p>Where for ray-sphere intersection the $a$/$b$/$c$ values are:</p> <p>$$ a = \\mathbf{b} \\cdot \\mathbf{b} $$   $$ b = 2 \\mathbf{b} \\cdot (\\mathbf{A}-\\mathbf{C}) $$   $$ c = (\\mathbf{A}-\\mathbf{C}) \\cdot (\\mathbf{A}-\\mathbf{C}) - r^2 $$</p> <p>Using all of the above you can solve for $t$, but there is a square root part that can be either positive (meaning two real solutions), negative (meaning no real solutions), or zero (meaning one real solution). In graphics, the algebra almost always relates very directly to the geometry. What we have is:</p> <p></p>"},{"location":"one_weekend/adding_a_sphere/#creating-our-first-raytraced-image","title":"Creating Our First Raytraced Image","text":"<p>If we take that math and hard-code it into our program, we can test our code by placing a small sphere at -1 on the z-axis and then coloring red any pixel that intersects it.</p> Rendering a red sphere<pre><code>bool hit_sphere(const point3&amp; center, double radius, const ray&amp; r) {\nvec3 oc = r.origin() - center;\nauto a = dot(r.direction(), r.direction());\nauto b = 2.0 * dot(oc, r.direction());\nauto c = dot(oc, oc) - radius*radius;\nauto discriminant = b*b - 4*a*c;\nreturn (discriminant &gt;= 0);\n}\ncolor ray_color(const ray&amp; r) {\nif (hit_sphere(point3(0,0,-1), 0.5, r))\nreturn color(1, 0, 0);\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n</code></pre> <p>What we get is this:</p> <p></p> <p>Now this lacks all sorts of things -- like shading, reflection rays, and more than one object -- but we are closer to halfway done than we are to our start! One thing to be aware of is that we are testing to see if a ray intersects with the sphere by solving the quadratic equation and seeing if a solution exists, but solutions with negative values of $t$ work just fine. If you change your sphere center to $z = +1$ you will get exactly the same picture because this solution doesn't distinguish between objects in front of the camera and objects behind the camera. This is not a feature! We\u2019ll fix those issues next.</p>"},{"location":"one_weekend/antialiasing/","title":"Antialiasing","text":"<p>If you zoom into the rendered images so far, you might notice the harsh \"stair step\" nature of edges in our rendered images. This stair-stepping is commonly referred to as \"aliasing\", or \"jaggies\". When a real camera takes a picture, there are usually no jaggies along edges, because the edge pixels are a blend of some foreground and some background. Consider that unlike our rendered images, a true image of the world is continuous. Put another way, the world (and any true image of it) has effectively infinite resolution. We can get the same effect by averaging a bunch of samples for each pixel.</p> <p>With a single ray through the center of each pixel, we are performing what is commonly called point sampling. The problem with point sampling can be illustrated by rendering a small checkerboard far away. If this checkerboard consists of an 8\u00d78 grid of black and white tiles, but only four rays hit it, then all four rays might intersect only white tiles, or only black, or some odd combination. In the real world, when we perceive a checkerboard far away with our eyes, we perceive it as a gray color, instead of sharp points of black and white. That's because our eyes are naturally doing what we want our ray tracer to do: integrate the (continuous function of) light falling on a particular (discrete) region of our rendered image.</p> <p>Clearly we don't gain anything by just resampling the same ray through the pixel center multiple times -- we'd just get the same result each time. Instead, we want to sample the light falling around the pixel, and then integrate those samples to approximate the true continuous result. So, how do we integrate the light falling around the pixel?</p> <p>We'll adopt the simplest model: sampling the square region centered at the pixel that extends halfway to each of the four neighboring pixels. This is not the optimal approach, but it is the most straight-forward. (See [A Pixel is Not a Little Square][square-pixels] for a deeper dive into this topic.)</p> <p></p>"},{"location":"one_weekend/antialiasing/#some-random-number-utilities","title":"Some Random Number Utilities","text":"<p>We're going to need need a random number generator that returns real random numbers. This function should return a canonical random number, which by convention falls in the range $0 \u2264 n &lt; 1$. The \u201cless than\u201d before the 1 is important, as we will sometimes take advantage of that.</p> <p>A simple approach to this is to use the <code>rand()</code> function that can be found in <code>&lt;cstdlib&gt;</code>, which returns a random integer in the range 0 and <code>RAND_MAX</code>. Hence we can get a real random number as desired with the following code snippet, added to <code>rtweekend.h</code>:</p> random_double() functions<pre><code>#include &lt;cmath&gt;\n#include &lt;cstdlib&gt;\n#include &lt;limits&gt;\n#include &lt;memory&gt;\n...\n// Utility Functions\ninline double degrees_to_radians(double degrees) {\nreturn degrees * pi / 180.0;\n}\ninline double random_double() {\n// Returns a random real in [0,1).\nreturn rand() / (RAND_MAX + 1.0);\n}\ninline double random_double(double min, double max) {\n// Returns a random real in [min,max).\nreturn min + (max-min)*random_double();\n}\n</code></pre> <p>C++ did not traditionally have a standard random number generator, but newer versions of C++ have addressed this issue with the <code>&lt;random&gt;</code> header (if imperfectly according to some experts). If you want to use this, you can obtain a random number with the conditions we need as follows:</p> random_double(), alternate implemenation<pre><code>#include &lt;random&gt;\ninline double random_double() {\nstatic std::uniform_real_distribution&lt;double&gt; distribution(0.0, 1.0);\nstatic std::mt19937 generator;\nreturn distribution(generator);\n}\n</code></pre>"},{"location":"one_weekend/antialiasing/#generating-pixels-with-multiple-samples","title":"Generating Pixels with Multiple Samples","text":"<p>For a single pixel composed of multiple samples, we'll select samples from the area surrounding the pixel and average the resulting light (color) values together.</p> <p>First we'll update the <code>write_color()</code> function to account for the number of samples we use: we need to find the average across all of the samples that we take. To do this, we'll add the full color from each iteration, and then finish with a single division (by the number of samples) at the end, before writing out the color. To ensure that the color components of the final result remain within the proper $[0,1]$ bounds, we'll add and use a small helper function: <code>interval::clamp(x)</code>.</p> The interval::clamp() utility function<pre><code>class interval {\npublic:\n...\nbool surrounds(double x) const {\nreturn min &lt; x &amp;&amp; x &lt; max;\n}\ndouble clamp(double x) const {\nif (x &lt; min) return min;\nif (x &gt; max) return max;\nreturn x;\n}\n...\n};\n</code></pre> <p>And here's the updated <code>write_color()</code> function that takes the sum total of all light for the pixel and the number of samples involved:</p> The multi-sample write_color() function<pre><code>void write_color(std::ostream &amp;out, color pixel_color, int samples_per_pixel) {\nauto r = pixel_color.x();\nauto g = pixel_color.y();\nauto b = pixel_color.z();\n// Divide the color by the number of samples.\nauto scale = 1.0 / samples_per_pixel;\nr *= scale;\ng *= scale;\nb *= scale;\n// Write the translated [0,255] value of each color component.\nstatic const interval intensity(0.000, 0.999);\nout &lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(r)) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(g)) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(b)) &lt;&lt; '\\n';\n}\n</code></pre> <p>Now let's update the camera class to define and use a new <code>camera::get_ray(i,j)</code> function, which will generate different samples for each pixel. This function will use a new helper function <code>pixel_sample_square()</code> that generates a random sample point within the unit square centered at the origin. We then transform the random sample from this ideal square back to the particular pixel we're currently sampling.</p> Camera with samples-per-pixel parameter<pre><code>class camera {\npublic:\ndouble aspect_ratio      = 1.0;  // Ratio of image width over height\nint    image_width       = 100;  // Rendered image width in pixel count\nint    samples_per_pixel = 10;   // Count of random samples for each pixel\nvoid render(const hittable&amp; world) {\ninitialize();\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\ncolor pixel_color(0,0,0);\nfor (int sample = 0; sample &lt; samples_per_pixel; ++sample) {\nray r = get_ray(i, j);\npixel_color += ray_color(r, world);\n}\nwrite_color(std::cout, pixel_color, samples_per_pixel);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n...\nprivate:\n...\nvoid initialize() {\n...\n}\nray get_ray(int i, int j) const {\n// Get a randomly sampled camera ray for the pixel at location i,j.\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto pixel_sample = pixel_center + pixel_sample_square();\nauto ray_origin = center;\nauto ray_direction = pixel_sample - ray_origin;\nreturn ray(ray_origin, ray_direction);\n}\nvec3 pixel_sample_square() const {\n// Returns a random point in the square surrounding a pixel at the origin.\nauto px = -0.5 + random_double();\nauto py = -0.5 + random_double();\nreturn (px * pixel_delta_u) + (py * pixel_delta_v);\n}\n...\n};\n#endif\n</code></pre> <p>(In addition to the new <code>pixel_sample_square()</code> function above, you'll also find the function <code>pixel_sample_disk()</code> in the Github source code. This is included in case you'd like to experiment with non-square pixels, but we won't be using it in this book. <code>pixel_sample_disk()</code> depends on the function <code>random_in_unit_disk()</code> which is defined later on.)</p> <p>Main is updated to set the new camera parameter.</p> Setting the new samples-per-pixel parameter<pre><code>int main() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.render(world);\n}\n</code></pre> <p>Zooming into the image that is produced, we can see the difference in edge pixels.</p> <p></p>"},{"location":"one_weekend/defocus_blur/","title":"Defocus Blur","text":"<p>Now our final feature: defocus blur. Note, photographers call this depth of field, so be sure to only use the term defocus blur among your raytracing friends.</p> <p>The reason we have defocus blur in real cameras is because they need a big hole (rather than just a pinhole) through which to gather light. A large hole would defocus everything, but if we stick a lens in front of the film/sensor, there will be a certain distance at which everything is in focus. Objects placed at that distance will appear in focus and will linearly appear blurrier the further they are from that distance. You can think of a lens this way: all light rays coming from a specific point at the focus distance -- and that hit the lens -- will be bent back to a single point on the image sensor.</p> <p>We call the distance between the camera center and the plane where everything is in perfect focus the focus distance. Be aware that the focus distance is not usually the same as the focal length -- the focal length is the distance between the camera center and the image plane. For our model, however, these two will have the same value, as we will put our pixel grid right on the focus plane, which is focus distance away from the camera center.</p> <p>In a physical camera, the focus distance is controlled by the distance between the lens and the film/sensor. That is why you see the lens move relative to the camera when you change what is in focus (that may happen in your phone camera too, but the sensor moves). The \u201caperture\u201d is a hole to control how big the lens is effectively. For a real camera, if you need more light you make the aperture bigger, and will get more blur for objects away from the focus distance. For our virtual camera, we can have a perfect sensor and never need more light, so we only use an aperture when we want defocus blur.</p>"},{"location":"one_weekend/defocus_blur/#a-thin-lens-approximation","title":"A Thin Lens Approximation","text":"<p>A real camera has a complicated compound lens. For our code, we could simulate the order: sensor, then lens, then aperture. Then we could figure out where to send the rays, and flip the image after it's computed (the image is projected upside down on the film). Graphics people, however, usually use a thin lens approximation:</p> <p></p> <p>We don\u2019t need to simulate any of the inside of the camera -- for the purposes of rendering an image outside the camera, that would be unnecessary complexity. Instead, I usually start rays from an infinitely thin circular \"lens\", and send them toward the pixel of interest on the focus plane (<code>focal_length</code> away from the lens), where everything on that plane in the 3D world is in perfect focus.</p> <p>In practice, we accomplish this by placing the viewport in this plane. Putting everything together:</p> <ol> <li>The focus plane is orthogonal to the camera view direction.</li> <li>The focus distance is the distance between the camera center and the focus plane.</li> <li>The viewport lies on the focus plane, centered on the camera view direction vector.</li> <li>The grid of pixel locations lies inside the viewport (located in the 3D world).</li> <li>Random image sample locations are chosen from the region around the current pixel location.</li> <li>The camera fires rays from random points on the lens through the current image sample location.</li> </ol> <p></p>"},{"location":"one_weekend/defocus_blur/#generating-sample-rays","title":"Generating Sample Rays","text":"<p>Without defocus blur, all scene rays originate from the camera center (or <code>lookfrom</code>). In order to accomplish defocus blur, we construct a disk centered at the camera center. The larger the radius, the greater the defocus blur. You can think of our original camera as having a defocus disk of radius zero (no blur at all), so all rays originated at the disk center (<code>lookfrom</code>).</p> <p>So, how large should the defocus disk be? Since the size of this disk controls how much defocus blur we get, that should be a parameter of the camera class. We could just take the radius of the disk as a camera parameter, but the blur would vary depending on the projection distance. A slightly easier parameter is to specify the angle of the cone with apex at viewport center and base (defocus disk) at the camera center. This should give you more consistent results as you vary the focus distance for a given shot.</p> <p>Since we'll be choosing random points from the defocus disk, we'll need a function to do that: <code>random_in_unit_disk()</code>. This function works using the same kind of method we use in <code>random_in_unit_sphere()</code>, just for two dimensions.</p> Generate random point inside unit disk<pre><code>inline vec3 unit_vector(vec3 u) {\nreturn v / v.length();\n}\ninline vec3 random_in_unit_disk() {\nwhile (true) {\nauto p = vec3(random_double(-1,1), random_double(-1,1), 0);\nif (p.length_squared() &lt; 1)\nreturn p;\n}\n}\n</code></pre> <p>Now let's update the camera to originate rays from the defocus disk:</p> Camera with adjustable depth-of-field (dof)<pre><code>class camera {\npublic:\ndouble aspect_ratio      = 1.0;  // Ratio of image width over height\nint    image_width       = 100;  // Rendered image width in pixel count\nint    samples_per_pixel = 10;   // Count of random samples for each pixel\nint    max_depth         = 10;   // Maximum number of ray bounces into scene\ndouble vfov     = 90;              // Vertical view angle (field of view)\npoint3 lookfrom = point3(0,0,-1);  // Point camera is looking from\npoint3 lookat   = point3(0,0,0);   // Point camera is looking at\nvec3   vup      = vec3(0,1,0);     // Camera-relative \"up\" direction\ndouble defocus_angle = 0;  // Variation angle of rays through each pixel\ndouble focus_dist = 10;    // Distance from camera lookfrom point to plane of perfect focus\n...\nprivate:\nint    image_height;    // Rendered image height\npoint3 center;          // Camera center\npoint3 pixel00_loc;     // Location of pixel 0, 0\nvec3   pixel_delta_u;   // Offset to pixel to the right\nvec3   pixel_delta_v;   // Offset to pixel below\nvec3   u, v, w;         // Camera frame basis vectors\nvec3   defocus_disk_u;  // Defocus disk horizontal radius\nvec3   defocus_disk_v;  // Defocus disk vertical radius\nvoid initialize() {\nimage_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\ncenter = lookfrom;\n// Determine viewport dimensions.\n// auto focal_length = (lookfrom - lookat).length();\nauto theta = degrees_to_radians(vfov);\nauto h = tan(theta/2);\nauto viewport_height = 2 * h * focus_dist;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\n// Calculate the u,v,w unit basis vectors for the camera coordinate frame.\nw = unit_vector(lookfrom - lookat);\nu = unit_vector(cross(vup, w));\nv = cross(w, u);\n// Calculate the vectors across the horizontal and down the vertical viewport edges.\nvec3 viewport_u = viewport_width * u;    // Vector across viewport horizontal edge\nvec3 viewport_v = viewport_height * -v;  // Vector down viewport vertical edge\n// Calculate the horizontal and vertical delta vectors to the next pixel.\npixel_delta_u = viewport_u / image_width;\npixel_delta_v = viewport_v / image_height;\n// Calculate the location of the upper left pixel.\nauto viewport_upper_left = center - (focus_dist * w) - viewport_u/2 - viewport_v/2;\npixel00_loc = viewport_upper_left + 0.5 * (pixel_delta_u + pixel_delta_v);\n// Calculate the camera defocus disk basis vectors.\nauto defocus_radius = focus_dist * tan(degrees_to_radians(defocus_angle / 2));\ndefocus_disk_u = u * defocus_radius;\ndefocus_disk_v = v * defocus_radius;\n}\nray get_ray(int i, int j) const {\n// Get a randomly-sampled camera ray for the pixel at location i,j, originating from\n// the camera defocus disk.\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto pixel_sample = pixel_center + pixel_sample_square();\nauto ray_origin = (defocus_angle &lt;= 0) ? center : defocus_disk_sample();\nauto ray_direction = pixel_sample - ray_origin;\nreturn ray(ray_origin, ray_direction);\n}\n...\npoint3 defocus_disk_sample() const {\n// Returns a random point in the camera defocus disk.\nauto p = random_in_unit_disk();\nreturn center + (p[0] * defocus_disk_u) + (p[1] * defocus_disk_v);\n}\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\n...\n};\n</code></pre> <p>Using a large aperture:</p> Scene camera with depth-of-field<pre><code>int main() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 20;\ncam.lookfrom = point3(-2,2,1);\ncam.lookat   = point3(0,0,-1);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 10.0;\ncam.focus_dist    = 3.4;\ncam.render(world);\n}\n</code></pre> <p>We get:</p> <p></p>"},{"location":"one_weekend/dielectrics/","title":"Dielectrics","text":"<p>Clear materials such as water, glass, and diamond are dielectrics. When a light ray hits them, it splits into a reflected ray and a refracted (transmitted) ray. We\u2019ll handle that by randomly choosing between reflection and refraction, only generating one scattered ray per interaction.</p>"},{"location":"one_weekend/dielectrics/#refraction","title":"Refraction","text":"<p>The hardest part to debug is the refracted ray. I usually first just have all the light refract if there is a refraction ray at all. For this project, I tried to put two glass balls in our scene, and I got this (I have not told you how to do this right or wrong yet, but soon!):</p> <p></p> <p>Is that right? Glass balls look odd in real life. But no, it isn\u2019t right. The world should be flipped upside down and no weird black stuff. I just printed out the ray straight through the middle of the image and it was clearly wrong. That often does the job.</p>"},{"location":"one_weekend/dielectrics/#snells-law","title":"Snell's Law","text":"<p>The refraction is described by Snell\u2019s law:</p> <p>$$ \\eta \\cdot \\sin\\theta = \\eta' \\cdot \\sin\\theta' $$</p> <p>Where $\\theta$ and $\\theta'$ are the angles from the normal, and $\\eta$ and $\\eta'$ (pronounced \"eta\" and \"eta prime\") are the refractive indices (typically air = 1.0, glass = 1.3\u20131.7, diamond = 2.4). The geometry is:</p> <p></p> <p>In order to determine the direction of the refracted ray, we have to solve for $\\sin\\theta'$:</p> <p>$$ \\sin\\theta' = \\frac{\\eta}{\\eta'} \\cdot \\sin\\theta $$</p> <p>On the refracted side of the surface there is a refracted ray $\\mathbf{R'}$ and a normal $\\mathbf{n'}$, and there exists an angle, $\\theta'$, between them. We can split $\\mathbf{R'}$ into the parts of the ray that are perpendicular to $\\mathbf{n'}$ and parallel to $\\mathbf{n'}$:</p> <p>$$ \\mathbf{R'} = \\mathbf{R'}{\\bot} + \\mathbf{R'} $$</p> <p>If we solve for $\\mathbf{R'}{\\bot}$ and $\\mathbf{R'}$ we get:</p> <p>$$ \\mathbf{R'}{\\bot} = \\frac{\\eta}{\\eta'} (\\mathbf{R} + \\cos\\theta \\mathbf{n}) $$   $$ \\mathbf{R'} = -\\sqrt{1 - |\\mathbf{R'}_{\\bot}|^2} \\mathbf{n} $$</p> <p>You can go ahead and prove this for yourself if you want, but we will treat it as fact and move on. The rest of the book will not require you to understand the proof.</p> <p>We know the value of every term on the right-hand side except for $\\cos\\theta$. It is well known that the dot product of two vectors can be explained in terms of the cosine of the angle between them:</p> <p>$$ \\mathbf{a} \\cdot \\mathbf{b} = |\\mathbf{a}| |\\mathbf{b}| \\cos\\theta $$</p> <p>If we restrict $\\mathbf{a}$ and $\\mathbf{b}$ to be unit vectors:</p> <p>$$ \\mathbf{a} \\cdot \\mathbf{b} = \\cos\\theta $$</p> <p>We can now rewrite $\\mathbf{R'}_{\\bot}$ in terms of known quantities:</p> <p>$$ \\mathbf{R'}_{\\bot} =      \\frac{\\eta}{\\eta'} (\\mathbf{R} + (\\mathbf{-R} \\cdot \\mathbf{n}) \\mathbf{n}) $$</p> <p>When we combine them back together, we can write a function to calculate $\\mathbf{R'}$:</p> Refraction function<pre><code>...\ninline vec3 reflect(const vec3&amp; v, const vec3&amp; n) {\nreturn v - 2*dot(v,n)*n;\n}\ninline vec3 refract(const vec3&amp; uv, const vec3&amp; n, double etai_over_etat) {\nauto cos_theta = fmin(dot(-uv, n), 1.0);\nvec3 r_out_perp =  etai_over_etat * (uv + cos_theta*n);\nvec3 r_out_parallel = -sqrt(fabs(1.0 - r_out_perp.length_squared())) * n;\nreturn r_out_perp + r_out_parallel;\n}\n</code></pre> <p>And the dielectric material that always refracts is:</p> Dielectric material class that always refracts<pre><code>...\nclass metal : public material {\n...\n};\nclass dielectric : public material {\npublic:\ndielectric(double index_of_refraction) : ir(index_of_refraction) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nattenuation = color(1.0, 1.0, 1.0);\ndouble refraction_ratio = rec.front_face ? (1.0/ir) : ir;\nvec3 unit_direction = unit_vector(r_in.direction());\nvec3 refracted = refract(unit_direction, rec.normal, refraction_ratio);\nscattered = ray(rec.p, refracted);\nreturn true;\n}\nprivate:\ndouble ir; // Index of Refraction\n};\n</code></pre> <p>Now we'll update the scene to change the left and center spheres to glass:</p> Changing left and center spheres to glass<pre><code>auto material_ground = make_shared&lt;lambertian&gt;(color(0.8, 0.8, 0.0));\nauto material_center = make_shared&lt;dielectric&gt;(1.5);\nauto material_left   = make_shared&lt;dielectric&gt;(1.5);\nauto material_right  = make_shared&lt;metal&gt;(color(0.8, 0.6, 0.2), 1.0);\n</code></pre> <p>This gives us the following result:</p> <p></p>"},{"location":"one_weekend/dielectrics/#total-internal-reflection","title":"Total Internal Reflection","text":"<p>That definitely doesn't look right. One troublesome practical issue is that when the ray is in the material with the higher refractive index, there is no real solution to Snell\u2019s law, and thus there is no refraction possible. If we refer back to Snell's law and the derivation of $\\sin\\theta'$:</p> <p>$$ \\sin\\theta' = \\frac{\\eta}{\\eta'} \\cdot \\sin\\theta $$</p> <p>If the ray is inside glass and outside is air ($\\eta = 1.5$ and $\\eta' = 1.0$):</p> <p>$$ \\sin\\theta' = \\frac{1.5}{1.0} \\cdot \\sin\\theta $$</p>  The value of $\\sin\\theta'$ cannot be greater than 1. So, if,    $$ \\frac{1.5}{1.0} \\cdot \\sin\\theta &gt; 1.0 $$  the equality between the two sides of the equation is broken, and a solution cannot exist. If a solution does not exist, the glass cannot refract, and therefore must reflect the ray:  Determining if the ray can refract<pre><code>if (refraction_ratio * sin_theta &gt; 1.0) {\n// Must Reflect\n...\n} else {\n// Can Refract\n...\n}\n</code></pre>  Here all the light is reflected, and because in practice that is usually inside solid objects, it is called \u201ctotal internal reflection\u201d. This is why sometimes the water-air boundary acts as a perfect mirror when you are submerged.  We can solve for `sin_theta` using the trigonometric qualities:    $$ \\sin\\theta  = \\sqrt{1 - \\cos^2\\theta} $$  and    $$ \\cos\\theta = \\mathbf{R} \\cdot \\mathbf{n} $$  Determining if the ray can refract<pre><code>double cos_theta = fmin(dot(-unit_direction, rec.normal), 1.0);\ndouble sin_theta = sqrt(1.0 - cos_theta*cos_theta);\nif (refraction_ratio * sin_theta &gt; 1.0) {\n// Must Reflect\n...\n} else {\n// Can Refract\n...\n}\n</code></pre>  And the dielectric material that always refracts (when possible) is:  Dielectric material class with reflection<pre><code>class dielectric : public material {\npublic:\ndielectric(double index_of_refraction) : ir(index_of_refraction) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nattenuation = color(1.0, 1.0, 1.0);\ndouble refraction_ratio = rec.front_face ? (1.0/ir) : ir;\nvec3 unit_direction = unit_vector(r_in.direction());\ndouble cos_theta = fmin(dot(-unit_direction, rec.normal), 1.0);\ndouble sin_theta = sqrt(1.0 - cos_theta*cos_theta);\nbool cannot_refract = refraction_ratio * sin_theta &gt; 1.0;\nvec3 direction;\nif (cannot_refract)\ndirection = reflect(unit_direction, rec.normal);\nelse\ndirection = refract(unit_direction, rec.normal, refraction_ratio);\nscattered = ray(rec.p, direction);\nreturn true;\n}\nprivate:\ndouble ir; // Index of Refraction\n};\n</code></pre>  Attenuation is always 1 -- the glass surface absorbs nothing. If we try that out with these parameters:  Scene with dielectric and shiny sphere<pre><code>auto material_ground = make_shared&lt;lambertian&gt;(color(0.8, 0.8, 0.0));\nauto material_center = make_shared&lt;lambertian&gt;(color(0.1, 0.2, 0.5));\nauto material_left   = make_shared&lt;dielectric&gt;(1.5);\nauto material_right  = make_shared&lt;metal&gt;(color(0.8, 0.6, 0.2), 0.0);\n</code></pre>  We get:  ![Glass sphere that sometimes refracts](https://raytracing.github.io/images/img-1.17-glass-sometimes-refract.png)  ### Schlick Approximation Now real glass has reflectivity that varies with angle -- look at a window at a steep angle and it becomes a mirror. There is a big ugly equation for that, but almost everybody uses a cheap and surprisingly accurate polynomial approximation by Christophe Schlick. This yields our full glass material:  Full glass material<pre><code>class dielectric : public material {\npublic:\ndielectric(double index_of_refraction) : ir(index_of_refraction) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nattenuation = color(1.0, 1.0, 1.0);\ndouble refraction_ratio = rec.front_face ? (1.0/ir) : ir;\nvec3 unit_direction = unit_vector(r_in.direction());\ndouble cos_theta = fmin(dot(-unit_direction, rec.normal), 1.0);\ndouble sin_theta = sqrt(1.0 - cos_theta*cos_theta);\nbool cannot_refract = refraction_ratio * sin_theta &gt; 1.0;\nvec3 direction;\nif (cannot_refract || reflectance(cos_theta, refraction_ratio) &gt; random_double())\ndirection = reflect(unit_direction, rec.normal);\nelse\ndirection = refract(unit_direction, rec.normal, refraction_ratio);\nscattered = ray(rec.p, direction);\nreturn true;\n}\nprivate:\ndouble ir; // Index of Refraction\nstatic double reflectance(double cosine, double ref_idx) {\n// Use Schlick's approximation for reflectance.\nauto r0 = (1-ref_idx) / (1+ref_idx);\nr0 = r0*r0;\nreturn r0 + (1-r0)*pow((1 - cosine),5);\n}\n};\n</code></pre>  ### Modeling a Hollow Glass Sphere An interesting and easy trick with dielectric spheres is to note that if you use a negative radius, the geometry is unaffected, but the surface normal points inward. This can be used as a bubble to make a hollow glass sphere:  Scene with hollow glass sphere<pre><code>...\nworld.add(make_shared&lt;sphere&gt;(point3( 0.0, -100.5, -1.0), 100.0, material_ground));\nworld.add(make_shared&lt;sphere&gt;(point3( 0.0,    0.0, -1.0),   0.5, material_center));\nworld.add(make_shared&lt;sphere&gt;(point3(-1.0,    0.0, -1.0),   0.5, material_left));\nworld.add(make_shared&lt;sphere&gt;(point3(-1.0,    0.0, -1.0),  -0.4, material_left));\nworld.add(make_shared&lt;sphere&gt;(point3( 1.0,    0.0, -1.0),   0.5, material_right));\n...\n</code></pre>  This gives:  ![A hollow glass sphere](https://raytracing.github.io/images/img-1.18-glass-hollow.png)"},{"location":"one_weekend/diffuse_materials/","title":"Diffuse Materials","text":"<p>Now that we have objects and multiple rays per pixel, we can make some realistic looking materials. We\u2019ll start with diffuse materials (also called matte). One question is whether we mix and match geometry and materials (so that we can assign a material to multiple spheres, or vice versa) or if geometry and materials are tightly bound (which could be useful for procedural objects where the geometry and material are linked). We\u2019ll go with separate -- which is usual in most renderers -- but do be aware that there are alternative approaches.</p>"},{"location":"one_weekend/diffuse_materials/#a-simple-diffuse-material","title":"A Simple Diffuse Material","text":"<p>Diffuse objects that don\u2019t emit their own light merely take on the color of their surroundings, but they do modulate that with their own intrinsic color. Light that reflects off a diffuse surface has its direction randomized, so, if we send three rays into a crack between two diffuse surfaces they will each have different random behavior:</p> <p></p> <p>They might also be absorbed rather than reflected. The darker the surface, the more likely the ray is absorbed (that\u2019s why it's dark!). Really any algorithm that randomizes direction will produce surfaces that look matte. Let's start with the most intuitive: a surface that randomly bounces a ray equally in all directions. For this material, a ray that hits the surface has an equal probability of bouncing in any direction away from the surface.</p> <p></p> <p>This very intuitive material is the simplest kind of diffuse and -- indeed -- many of the first raytracing papers used this diffuse method (before adopting a more accurate method that we'll be implementing a little bit later). We don't currently have a way to randomly reflect a ray, so we'll need to add a few functions to our vector utility header. The first thing we need is the ability to generate arbitrary random vectors:</p> vec3 random utility functions<pre><code>class vec3 {\npublic:\n...\ndouble length_squared() const {\nreturn e[0]*e[0] + e[1]*e[1] + e[2]*e[2];\n}\nstatic vec3 random() {\nreturn vec3(random_double(), random_double(), random_double());\n}\nstatic vec3 random(double min, double max) {\nreturn vec3(random_double(min,max), random_double(min,max), random_double(min,max));\n}\n};\n</code></pre> <p>Then we need to figure out how to manipulate a random vector so that we only get results that are on the surface of a hemisphere. There are analytical methods of doing this, but they are actually surprisingly complicated to understand, and quite a bit complicated to implement. Instead, we'll use what is typically the easiest algorithm: A rejection method. A rejection method works by repeatedly generating random samples until we produce a sample that meets the desired criteria. In other words, keep rejecting samples until you find a good one.</p> <p>There are many equally valid ways of generating a random vector on a hemisphere using the rejection method, but for our purposes we will go with the simplest, which is:</p> <ol> <li>Generate a random vector inside of the unit sphere</li> <li>Normalize this vector</li> <li>Invert the normalized vector if it falls onto the wrong hemisphere</li> </ol> <p>First, we will use a rejection method to generate the random vector inside of the unit sphere. Pick a random point in the unit cube, where $x$, $y$, and $z$ all range from -1 to +1, and reject this point if it is outside the unit sphere.</p> <p></p> The random_in_unit_sphere() function<pre><code>...\ninline vec3 unit_vector(vec3 v) {\nreturn v / v.length();\n}\ninline vec3 random_in_unit_sphere() {\nwhile (true) {\nauto p = vec3::random(-1,1);\nif (p.length_squared() &lt; 1)\nreturn p;\n}\n}\n</code></pre> <p>Once we have a random vector in the unit sphere we need to normalize it to get a vector on the unit sphere.</p> <p></p> Random vector on the unit sphere<pre><code>...\ninline vec3 random_in_unit_sphere() {\nwhile (true) {\nauto p = vec3::random(-1,1);\nif (p.length_squared() &lt; 1)\nreturn p;\n}\n}\ninline vec3 random_unit_vector() {\nreturn unit_vector(random_in_unit_sphere());\n}\n</code></pre> <p>And now that we have a random vector on the surface of the unit sphere, we can determine if it is on the correct hemisphere by comparing against the surface normal:</p> <p></p> <p>We can take the dot product of the surface normal and our random vector to determine if it's in the correct hemisphere. If the dot product is positive, then the vector is in the correct hemisphere. If the dot product is negative, then we need to invert the vector.</p> The random_in_hemisphere() function<pre><code>...\ninline vec3 random_unit_vector() {\nreturn unit_vector(random_in_unit_sphere());\n}\ninline vec3 random_on_hemisphere(const vec3&amp; normal) {\nvec3 on_unit_sphere = random_unit_vector();\nif (dot(on_unit_sphere, normal) &gt; 0.0) // In the same hemisphere as the normal\nreturn on_unit_sphere;\nelse\nreturn -on_unit_sphere;\n}\n</code></pre> <p>If a ray bounces off of a material and keeps 100% of its color, then we say that the material is white. If a ray bounces off of a material and keeps 0% of its color, then we say that the material is black. As a first demonstration of our new diffuse material we'll set the <code>ray_color</code> function to return 50% of the color from a bounce. We should expect to get a nice gray color.</p> ray_color() using a random ray direction<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, const hittable&amp; world) const {\nhit_record rec;\nif (world.hit(r, interval(0, infinity), rec)) {\nvec3 direction = random_on_hemisphere(rec.normal);\nreturn 0.5 * ray_color(ray(rec.p, direction), world);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n</code></pre> <p>... Indeed we do get rather nice gray spheres:</p> <p></p>"},{"location":"one_weekend/diffuse_materials/#limiting-the-number-of-child-rays","title":"Limiting the Number of Child Rays","text":"<p>There's one potential problem lurking here. Notice that the <code>ray_color</code> function is recursive. When will it stop recursing? When it fails to hit anything. In some cases, however, that may be a long time \u2014 long enough to blow the stack. To guard against that, let's limit the maximum recursion depth, returning no light contribution at the maximum depth:</p> camera::ray_color() with depth limiting<pre><code>class camera {\npublic:\ndouble aspect_ratio      = 1.0;  // Ratio of image width over height\nint    image_width       = 100;  // Rendered image width in pixel count\nint    samples_per_pixel = 10;   // Count of random samples for each pixel\nint    max_depth         = 10;   // Maximum number of ray bounces into scene\nvoid render(const hittable&amp; world) {\ninitialize();\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\ncolor pixel_color(0,0,0);\nfor (int sample = 0; sample &lt; samples_per_pixel; ++sample) {\nray r = get_ray(i, j);\npixel_color += ray_color(r, max_depth, world);\n}\nwrite_color(std::cout, pixel_color, samples_per_pixel);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\nif (world.hit(r, interval(0, infinity), rec)) {\nvec3 direction = random_on_hemisphere(rec.normal);\nreturn 0.5 * ray_color(ray(rec.p, direction), depth-1, world);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n</code></pre> <p>Update the main() function to use this new depth limit:</p> Using the new ray depth limiting<pre><code>int main() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.render(world);\n}\n</code></pre> <p>For this very simple scene we should get basically the same result:</p> <p></p>"},{"location":"one_weekend/diffuse_materials/#fixing-shadow-acne","title":"Fixing Shadow Acne","text":"<p>There\u2019s also a subtle bug that we need to address. A ray will attempt to accurately calculate the intersection point when it intersects with a surface. Unfortunately for us, this calculation is susceptible to floating point rounding errors which can cause the intersection point to be ever so slightly off. This means that the origin of the next ray, the ray that is randomly scattered off of the surface, is unlikely to be perfectly flush with the surface. It might be just above the surface. It might be just below the surface. If the ray's origin is just below the surface then it could intersect with that surface again. Which means that it will find the nearest surface at $t=0.00000001$ or whatever floating point approximation the hit function gives us. The simplest hack to address this is just to ignore hits that are very close to the calculated intersection point:</p> Calculating reflected ray origins with tolerance<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\nif (world.hit(r, interval(0.001, infinity), rec)) {\nvec3 direction = random_on_hemisphere(rec.normal);\nreturn 0.5 * ray_color(ray(rec.p, direction), depth-1, world);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n</code></pre> <p>This gets rid of the shadow acne problem. Yes it is really called that. Here's the result:</p> <p></p>"},{"location":"one_weekend/diffuse_materials/#true-lambertian-reflection","title":"True Lambertian Reflection","text":"<p>Scattering reflected rays evenly about the hemisphere produces a nice soft diffuse model, but we can definitely do better. A more accurate representation of real diffuse objects is the Lambertian distribution. This distribution scatters reflected rays in a manner that is proportional to $\\cos (\\phi)$, where $\\phi$ is the angle between the reflected ray and the surface normal. This means that a reflected ray is most likely to scatter in a direction near the surface normal, and less likely to scatter in directions away from the normal. This non-uniform Lambertian distribution does a better job of modeling material reflection in the real world than our previous uniform scattering.</p> <p>We can create this distribution by adding a random unit vector to the normal vector. At the point of intersection on a surface there is the hit point, $\\mathbf{p}$, and there is the normal of the surface, $\\mathbf{n}$. At the point of intersection, this surface has exactly two sides, so there can only be two unique unit spheres tangent to any intersection point (one unique sphere for each side of the surface). These two unit spheres will be displaced from the surface by the length of their radius, which is exactly one for a unit sphere.</p> <p>One sphere will be displaced in the direction of the surface's normal ($\\mathbf{n}$) and one sphere will be displaced in the opposite direction ($\\mathbf{-n}$). This leaves us with two spheres of unit size that will only be just touching the surface at the intersection point. From this, one of the spheres will have its center at $(\\mathbf{P} + \\mathbf{n})$ and the other sphere will have its center at $(\\mathbf{P} - \\mathbf{n})$. The sphere with a center at $(\\mathbf{P} - \\mathbf{n})$ is considered inside the surface, whereas the sphere with center $(\\mathbf{P} + \\mathbf{n})$ is considered outside the surface.</p> <p>We want to select the tangent unit sphere that is on the same side of the surface as the ray origin. Pick a random point $\\mathbf{S}$ on this unit radius sphere and send a ray from the hit point $\\mathbf{P}$ to the random point $\\mathbf{S}$ (this is the vector $(\\mathbf{S}-\\mathbf{P})$):</p> <p></p> <p>The change is actually fairly minimal:</p> ray_color() with replacement diffuse<pre><code>class camera {\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\nif (world.hit(r, interval(0.001, infinity), rec)) {\nvec3 direction = rec.normal + random_unit_vector();\nreturn 0.5 * ray_color(ray(rec.p, direction), depth-1, world);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n</code></pre> <p>After rendering we get a similar image:</p> <p></p> <p>It's hard to tell the difference between these two diffuse methods, given that our scene of two spheres is so simple, but you should be able to notice two important visual differences:</p> <ol> <li>The shadows are more pronounced after the change</li> <li>Both spheres are tinted blue from the sky after the change</li> </ol> <p>Both of these changes are due to the less uniform scattering of the light rays--more rays are scattering toward the normal. This means that for diffuse objects, they will appear darker because less light bounces toward the camera. For the shadows, more light bounces straight-up, so the area underneath the sphere is darker.</p> <p>Not a lot of common, everyday objects are perfectly diffuse, so our visual intuition of how these objects behave under light can be poorly formed. As scenes become more complicated over the course of the book, you are encouraged to switch between the different diffuse renderers presented here. Most scenes of interest will contain a large amount of diffuse materials. You can gain valuable insight by understanding the effect of different diffuse methods on the lighting of a scene.</p>"},{"location":"one_weekend/diffuse_materials/#using-gamma-correction-for-accurate-color-intensity","title":"Using Gamma Correction for Accurate Color Intensity","text":"<p>Note the shadowing under the sphere. The picture is very dark, but our spheres only absorb half the energy of each bounce, so they are 50% reflectors. The spheres should look pretty bright (in real life, a light grey) but they appear to be rather dark. We can see this more clearly if we walk through the full brightness gamut for our diffuse material. We start by setting the reflectance of the <code>ray_color</code> function from <code>0.5</code> (50%) to <code>0.1</code> (10%):</p> ray_color() with 10% reflectance<pre><code>class camera {\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\nif (world.hit(r, interval(0.001, infinity), rec)) {\nvec3 direction = rec.normal + random_unit_vector();\nreturn 0.1 * ray_color(ray(rec.p, direction), depth-1, world);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n</code></pre> <p>We render out at this new 10% reflectance. We then set reflectance to 30% and render again. We repeat for 50%, 70%, and finally 90%. You can overlay these images from left to right in the photo editor of your choice and you should get a very nice visual representation of the increasing brightness of your chosen gamut. This is the one that we've been working with so far: </p> <p></p> <p>If you look closely, or if you use a color picker, you should notice that the 50% reflectance render (the one in the middle) is far too dark to be half-way between white and black (middle-gray). Indeed, the 70% reflector is closer to middle-gray. The reason for this is that almost all computer programs assume that an image is \u201cgamma corrected\u201d before being written into an image file. This means that the 0 to 1 values have some transform applied before being stored as a byte. Images with data that are written without being transformed are said to be in linear space, whereas images that are transformed are said to be in gamma space. It is likely that the image viewer you are using is expecting an image in gamma space, but we are giving it an image in linear space. This is the reason why our image appears inaccurately dark.</p> <p>There are many good reasons for why images should be stored in gamma space, but for our purposes we just need to be aware of it. We are going to transform our data into gamma space so that our image viewer can more accurately display our image. As a simple approximation, we can use \u201cgamma 2\u201d as our transform, which is the power that you use when going from gamma space to linear space. We need to go from linear space to gamma space, which means taking the inverse of \"gamma 2\", which means an exponent of $1/\\mathit{gamma}$, which is just the square-root.</p> write_color(), with gamma correction<pre><code>inline double linear_to_gamma(double linear_component)\n{\nreturn sqrt(linear_component);\n}\nvoid write_color(std::ostream &amp;out, color pixel_color, int samples_per_pixel) {\nauto r = pixel_color.x();\nauto g = pixel_color.y();\nauto b = pixel_color.z();\n// Divide the color by the number of samples.\nauto scale = 1.0 / samples_per_pixel;\nr *= scale;\ng *= scale;\nb *= scale;\n// Apply the linear to gamma transform.\nr = linear_to_gamma(r);\ng = linear_to_gamma(g);\nb = linear_to_gamma(b);\n// Write the translated [0,255] value of each color component.\nstatic const interval intensity(0.000, 0.999);\nout &lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(r)) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(g)) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(b)) &lt;&lt; '\\n';\n}\n</code></pre> <p>Using this gamma correction, we now get a much more consistent ramp from darkness to lightness:</p> <p></p>"},{"location":"one_weekend/in_one_weekend/","title":"Ray Tracing in One Weekend","text":""},{"location":"one_weekend/in_one_weekend/#overview","title":"Overview","text":"<p>I\u2019ve taught many graphics classes over the years. Often I do them in ray tracing, because you are forced to write all the code, but you can still get cool images with no API. I decided to adapt my course notes into a how-to, to get you to a cool program as quickly as possible. It will not be a full-featured ray tracer, but it does have the indirect lighting which has made ray tracing a staple in movies. Follow these steps, and the architecture of the ray tracer you produce will be good for extending to a more extensive ray tracer if you get excited and want to pursue that.</p> <p>When somebody says \u201cray tracing\u201d it could mean many things. What I am going to describe is technically a path tracer, and a fairly general one. While the code will be pretty simple (let the computer do the work!) I think you\u2019ll be very happy with the images you can make.</p> <p>I\u2019ll take you through writing a ray tracer in the order I do it, along with some debugging tips. By the end, you will have a ray tracer that produces some great images. You should be able to do this in a weekend. If you take longer, don\u2019t worry about it. I use C++ as the driving language, but you don\u2019t need to. However, I suggest you do, because it\u2019s fast, portable, and most production movie and video game renderers are written in C++. Note that I avoid most \u201cmodern features\u201d of C++, but inheritance and operator overloading are too useful for ray tracers to pass on.</p> <p>I do not provide the code online, but the code is real and I show all of it except for a few straightforward operators in the vec3 class. I am a big believer in typing in code to learn it, but when code is available I use it, so I only practice what I preach when the code is not available. So don\u2019t ask!</p> <p>I have left that last part in because it is funny what a 180 I have done. Several readers ended up with subtle errors that were helped when we compared code. So please do type in the code, but you can find the finished source for each book in the RayTracing project on GitHub.</p> <p>A note on the implementing code for these books \u2014 our philosophy for the included code prioritizes the following goals: - The code should implement the concepts covered in the books. - We use C++, but as simple as possible. Our programming style is very C-like, but we take advantage of modern features where it makes the code easier to use or understand. - Our coding style continues the style established from the original books as much as possible, for continuity. - Line length is kept to 96 characters per line, to keep lines consistent between the codebase and code listings in the books.</p> <p>The code thus provides a baseline implementation, with tons of improvements left for the reader to enjoy. There are endless ways one can optimize and modernize the code; we prioritize the simple solution.</p> <p>We assume a little bit of familiarity with vectors (like dot product and vector addition). If you don\u2019t know that, do a little review. If you need that review, or to learn it for the first time, check out the online Graphics Codex by Morgan McGuire, Fundamentals of Computer Graphics by Steve Marschner and Peter Shirley, or Fundamentals of Interactive Computer Graphics by J.D. Foley and Andy Van Dam.</p> <p>Peter maintains a site related to this book series at https://in1weekend.blogspot.com/, which includes further reading and links to resources.</p> <p>If you want to communicate with us, feel free to send us an email at:</p> <ul> <li>Peter Shirley, ptrshrl@gmail.com</li> <li>Steve Hollasch, steve@hollasch.net</li> <li>Trevor David Black, trevordblack@trevord.black</li> </ul> <p>Finally, if you run into problems with your implementation, have general questions, or would like to share your own ideas or work, see the GitHub Discussions forum on the GitHub project.</p> <p>Thanks to everyone who lent a hand on this project. You can find them in the acknowledgments section at the end of this book.</p> <p>Let\u2019s get on with it!</p>"},{"location":"one_weekend/metal/","title":"Metal","text":""},{"location":"one_weekend/metal/#an-abstract-class-for-materials","title":"An Abstract Class for Materials","text":"<p>If we want different objects to have different materials, we have a design decision. We could have a universal material type with lots of parameters so any individual material type could just ignore the parameters that don't affect it. This is not a bad approach. Or we could have an abstract material class that encapsulates unique behavior. I am a fan of the latter approach. For our program the material needs to do two things:</p> <ol> <li>Produce a scattered ray (or say it absorbed the incident ray).</li> <li>If scattered, say how much the ray should be attenuated.</li> </ol> <p>This suggests the abstract class:</p> The material class<pre><code>#ifndef MATERIAL_H\n#define MATERIAL_H\n#include \"rtweekend.h\"\nclass hit_record;\nclass material {\npublic:\nvirtual ~material() = default;\nvirtual bool scatter(\nconst ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered) const = 0;\n};\n#endif\n</code></pre>"},{"location":"one_weekend/metal/#a-data-structure-to-describe-ray-object-intersections","title":"A Data Structure to Describe Ray-Object Intersections","text":"<p>The <code>hit_record</code> is to avoid a bunch of arguments so we can stuff whatever info we want in there. You can use arguments instead of an encapsulated type, it\u2019s just a matter of taste. Hittables and materials need to be able to reference the other's type in code so there is some circularity of the references. In C++ we add the line <code>class material;</code> to tell the compiler that <code>material</code> is a class that will be defined later. Since we're just specifying a pointer to the class, the compiler doesn't need to know the details of the class, solving the circular reference issue.</p> Hit record with added material pointer<pre><code>#include \"rtweekend.h\"\nclass material;\nclass hit_record {\npublic:\npoint3 p;\nvec3 normal;\nshared_ptr&lt;material&gt; mat;\ndouble t;\nbool front_face;\nvoid set_face_normal(const ray&amp; r, const vec3&amp; outward_normal) {\nfront_face = dot(r.direction(), outward_normal) &lt; 0;\nnormal = front_face ? outward_normal : -outward_normal;\n}\n};\n</code></pre> <p><code>hit_record</code> is just a way to stuff a bunch of arguments into a class so we can send them as a group. When a ray hits a surface (a particular sphere for example), the material pointer in the <code>hit_record</code> will be set to point at the material pointer the sphere was given when it was set up in <code>main()</code> when we start. When the <code>ray_color()</code> routine gets the <code>hit_record</code> it can call member functions of the material pointer to find out what ray, if any, is scattered.</p> <p>To achieve this, <code>hit_record</code> needs to be told the material that is assigned to the sphere.</p> Ray-sphere intersection with added material information<pre><code>class sphere : public hittable {\npublic:\nsphere(point3 _center, double _radius, shared_ptr&lt;material&gt; _material)\n: center(_center), radius(_radius), mat(_material) {}\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n...\nrec.t = root;\nrec.p = r.at(rec.t);\nvec3 outward_normal = (rec.p - center) / radius;\nrec.set_face_normal(r, outward_normal);\nrec.mat = mat;\nreturn true;\n}\nprivate:\npoint3 center;\ndouble radius;\nshared_ptr&lt;material&gt; mat;\n};\n</code></pre>"},{"location":"one_weekend/metal/#modeling-light-scatter-and-reflectance","title":"Modeling Light Scatter and Reflectance","text":"<p>For the Lambertian (diffuse) case we already have, it can either always scatter and attenuate by its reflectance $R$, or it can sometimes scatter (with probabilty $1-R$) with no attenuation (where a ray that isn't scattered is just absorbed into the material). It could also be a mixture of both those strategies. We will choose to always scatter, so Lambertian materials become this simple class:</p> The new lambertian material class<pre><code>class material {\n...\n};\nclass lambertian : public material {\npublic:\nlambertian(const color&amp; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nauto scatter_direction = rec.normal + random_unit_vector();\nscattered = ray(rec.p, scatter_direction);\nattenuation = albedo;\nreturn true;\n}\nprivate:\ncolor albedo;\n};\n</code></pre> <p>Note the third option that we could scatter with some fixed probability $p$ and have attenuation be $\\mathit{albedo}/p$. Your choice.</p> <p>If you read the code above carefully, you'll notice a small chance of mischief. If the random unit vector we generate is exactly opposite the normal vector, the two will sum to zero, which will result in a zero scatter direction vector. This leads to bad scenarios later on (infinities and NaNs), so we need to intercept the condition before we pass it on.</p> <p>In service of this, we'll create a new vector method -- <code>vec3::near_zero()</code> -- that returns true if the vector is very close to zero in all dimensions.</p> The vec3::near_zero() method<pre><code>class vec3 {\n...\ndouble length_squared() const {\nreturn e[0]*e[0] + e[1]*e[1] + e[2]*e[2];\n}\nbool near_zero() const {\n// Return true if the vector is close to zero in all dimensions.\nauto s = 1e-8;\nreturn (fabs(e[0]) &lt; s) &amp;&amp; (fabs(e[1]) &lt; s) &amp;&amp; (fabs(e[2]) &lt; s);\n}\n...\n};\n</code></pre> Lambertian scatter, bullet-proof<pre><code>class lambertian : public material {\npublic:\nlambertian(const color&amp; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nauto scatter_direction = rec.normal + random_unit_vector();\n// Catch degenerate scatter direction\nif (scatter_direction.near_zero())\nscatter_direction = rec.normal;\nscattered = ray(rec.p, scatter_direction);\nattenuation = albedo;\nreturn true;\n}\nprivate:\ncolor albedo;\n};\n</code></pre>"},{"location":"one_weekend/metal/#mirrored-light-reflection","title":"Mirrored Light Reflection","text":"<p>For polished metals the ray won\u2019t be randomly scattered. The key question is: How does a ray get reflected from a metal mirror? Vector math is our friend here:</p> <p></p> <p>The reflected ray direction in red is just $\\mathbf{v} + 2\\mathbf{b}$. In our design, $\\mathbf{n}$ is a unit vector, but $\\mathbf{v}$ may not be. The length of $\\mathbf{b}$ should be $\\mathbf{v} \\cdot \\mathbf{n}$. Because $\\mathbf{v}$ points in, we will need a minus sign, yielding:</p> vec3 reflection function<pre><code>...\ninline vec3 random_on_hemisphere(const vec3&amp; normal) {\n...\n}\nvec3 reflect(const vec3&amp; v, const vec3&amp; n) {\nreturn v - 2*dot(v,n)*n;\n}\n...\n</code></pre> <p>The metal material just reflects rays using that formula:</p> Metal material with reflectance function<pre><code>...\nclass lambertian : public material {\n...\n};\nclass metal : public material {\npublic:\nmetal(const color&amp; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nvec3 reflected = reflect(unit_vector(r_in.direction()), rec.normal);\nscattered = ray(rec.p, reflected);\nattenuation = albedo;\nreturn true;\n}\nprivate:\ncolor albedo;\n};\n</code></pre> <p>We need to modify the <code>ray_color()</code> function for all of our changes:</p> Ray color with scattered reflectance<pre><code>...\n#include \"rtweekend.h\"\n#include \"color.h\"\n#include \"hittable.h\"\n#include \"material.h\"\n...\nclass camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\nif (world.hit(r, interval(0.001, infinity), rec)) {\nray scattered;\ncolor attenuation;\nif (rec.mat-&gt;scatter(r, rec, attenuation, scattered))\nreturn attenuation * ray_color(scattered, depth-1, world);\nreturn color(0,0,0);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n</code></pre>"},{"location":"one_weekend/metal/#a-scene-with-metal-spheres","title":"A Scene with Metal Spheres","text":"<p>Now let\u2019s add some metal spheres to our scene:</p> Scene with metal spheres<pre><code>#include \"rtweekend.h\"\n#include \"camera.h\"\n#include \"color.h\"\n#include \"hittable_list.h\"\n#include \"material.h\"\n#include \"sphere.h\"\nint main() {\nhittable_list world;\nauto material_ground = make_shared&lt;lambertian&gt;(color(0.8, 0.8, 0.0));\nauto material_center = make_shared&lt;lambertian&gt;(color(0.7, 0.3, 0.3));\nauto material_left   = make_shared&lt;metal&gt;(color(0.8, 0.8, 0.8));\nauto material_right  = make_shared&lt;metal&gt;(color(0.8, 0.6, 0.2));\nworld.add(make_shared&lt;sphere&gt;(point3( 0.0, -100.5, -1.0), 100.0, material_ground));\nworld.add(make_shared&lt;sphere&gt;(point3( 0.0,    0.0, -1.0),   0.5, material_center));\nworld.add(make_shared&lt;sphere&gt;(point3(-1.0,    0.0, -1.0),   0.5, material_left));\nworld.add(make_shared&lt;sphere&gt;(point3( 1.0,    0.0, -1.0),   0.5, material_right));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.render(world);\n}\n</code></pre> <p>Which gives:</p> <p></p>"},{"location":"one_weekend/metal/#fuzzy-reflection","title":"Fuzzy Reflection","text":"<p>We can also randomize the reflected direction by using a small sphere and choosing a new endpoint for the ray. We'll use a random point from the surface of a sphere centered on the original endpoint, scaled by the fuzz factor.</p> <p></p> <p>The bigger the sphere, the fuzzier the reflections will be. This suggests adding a fuzziness parameter that is just the radius of the sphere (so zero is no perturbation). The catch is that for big spheres or grazing rays, we may scatter below the surface. We can just have the surface absorb those.</p> Metal material fuzziness<pre><code>class metal : public material {\npublic:\nmetal(const color&amp; a, double f) : albedo(a), fuzz(f &lt; 1 ? f : 1) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nvec3 reflected = reflect(unit_vector(r_in.direction()), rec.normal);\nscattered = ray(rec.p, reflected + fuzz*random_unit_vector());\nattenuation = albedo;\nreturn (dot(scattered.direction(), rec.normal) &gt; 0);\n}\nprivate:\ncolor albedo;\ndouble fuzz;\n};\n</code></pre> <p>We can try that out by adding fuzziness 0.3 and 1.0 to the metals:</p> Metal spheres with fuzziness<pre><code>int main() {\n...\nauto material_ground = make_shared&lt;lambertian&gt;(color(0.8, 0.8, 0.0));\nauto material_center = make_shared&lt;lambertian&gt;(color(0.7, 0.3, 0.3));\nauto material_left   = make_shared&lt;metal&gt;(color(0.8, 0.8, 0.8), 0.3);\nauto material_right  = make_shared&lt;metal&gt;(color(0.8, 0.6, 0.2), 1.0);\n...\n}\n</code></pre> <p></p>"},{"location":"one_weekend/moving_camera_code_into_its_own_class/","title":"Moving Camera Code Into Its Own Class","text":"<p>Before continuing, now is a good time to consolidate our camera and scene-render code into a single new class: the <code>camera</code> class. The camera class will be responsible for two important jobs:</p> <ol> <li>Construct and dispatch rays into the world.</li> <li>Use the results of these rays to construct the rendered image.</li> </ol> <p>In this refactoring, we'll collect the <code>ray_color()</code> function, along with the image, camera, and render sections of our main program. The new camera class will contain two public methods <code>initialize()</code> and <code>render()</code>, plus two private helper methods <code>get_ray()</code> and <code>ray_color()</code>.</p> <p>Ultimately, the camera will follow the simplest usage pattern that we could think of: it will be default constructed no arguments, then the owning code will modify the camera's public variables through simple assignment, and finally everything is initialized by a call to the <code>initialize()</code> function. This pattern is chosen instead of the owner calling a constructor with a ton of parameters or by defining and calling a bunch of setter methods. Instead, the owning code only needs to set what it explicitly cares about. Finally, we could either have the owning code call <code>initialize()</code>, or just have the camera call this function automatically at the start of <code>render()</code>. We'll use the second approach.</p> <p>After main creates a camera and sets default values, it will call the <code>render()</code> method. The <code>render()</code> method will prepare the camera for rendering and then execute the render loop.</p> <p>Here's the skeleton of our new <code>camera</code> class:</p> The camera class skeleton<pre><code>#ifndef CAMERA_H\n#define CAMERA_H\n#include \"rtweekend.h\"\n#include \"color.h\"\n#include \"hittable.h\"\nclass camera {\npublic:\n/* Public Camera Parameters Here */\nvoid render(const hittable&amp; world) {\n...\n}\nprivate:\n/* Private Camera Variables Here */\nvoid initialize() {\n...\n}\ncolor ray_color(const ray&amp; r, const hittable&amp; world) const {\n...\n}\n};\n#endif\n</code></pre> <p>To begin with, let's fill in the <code>ray_color()</code> function from <code>main.cc</code>:</p> The camera::ray_color function<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, const hittable&amp; world) const {\nhit_record rec;\nif (world.hit(r, interval(0, infinity), rec)) {\nreturn 0.5 * (rec.normal + color(1,1,1));\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n};\n#endif\n</code></pre> <p>Now we move almost everything from the <code>main()</code> function into our new camera class. The only thing remaining in the <code>main()</code> function is the world construction. Here's the camera class with newly migrated code:</p> The working camera class<pre><code>...\n#include \"rtweekend.h\"\n#include \"color.h\"\n#include \"hittable.h\"\n#include &lt;iostream&gt;\nclass camera {\npublic:\ndouble aspect_ratio = 1.0;  // Ratio of image width over height\nint    image_width  = 100;  // Rendered image width in pixel count\nvoid render(const hittable&amp; world) {\ninitialize();\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto ray_direction = pixel_center - center;\nray r(center, ray_direction);\ncolor pixel_color = ray_color(r, world);\nwrite_color(std::cout, pixel_color);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\nprivate:\nint    image_height;   // Rendered image height\npoint3 center;         // Camera center\npoint3 pixel00_loc;    // Location of pixel 0, 0\nvec3   pixel_delta_u;  // Offset to pixel to the right\nvec3   pixel_delta_v;  // Offset to pixel below\nvoid initialize() {\nimage_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\ncenter = point3(0, 0, 0);\n// Determine viewport dimensions.\nauto focal_length = 1.0;\nauto viewport_height = 2.0;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\n// Calculate the vectors across the horizontal and down the vertical viewport edges.\nauto viewport_u = vec3(viewport_width, 0, 0);\nauto viewport_v = vec3(0, -viewport_height, 0);\n// Calculate the horizontal and vertical delta vectors from pixel to pixel.\npixel_delta_u = viewport_u / image_width;\npixel_delta_v = viewport_v / image_height;\n// Calculate the location of the upper left pixel.\nauto viewport_upper_left =\ncenter - vec3(0, 0, focal_length) - viewport_u/2 - viewport_v/2;\npixel00_loc = viewport_upper_left + 0.5 * (pixel_delta_u + pixel_delta_v);\n}\ncolor ray_color(const ray&amp; r, const hittable&amp; world) const {\n...\n}\n};\n#endif\n</code></pre> <p>And here's the much reduced main:</p> The new main, using the new camera<pre><code>#include \"rtweekend.h\"\n#include \"camera.h\"\n#include \"hittable_list.h\"\n#include \"sphere.h\"\nint main() {\nhittable_list world;\nworld.add(make_shared&lt;sphere&gt;(point3(0,0,-1), 0.5));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-100.5,-1), 100));\ncamera cam;\ncam.aspect_ratio = 16.0 / 9.0;\ncam.image_width  = 400;\ncam.render(world);\n}\n</code></pre> <p>Running this newly refactored program should give us the same rendered image as before.</p>"},{"location":"one_weekend/output_an_image/","title":"Output an image","text":""},{"location":"one_weekend/output_an_image/#the-ppm-image-format","title":"The PPM Image Format","text":"<p>Whenever you start a renderer, you need a way to see an image. The most straightforward way is to write it to a file. The catch is, there are so many formats. Many of those are complex. I always start with a plain text ppm file. Here\u2019s a nice description from Wikipedia:</p> <p> </p> PPM Example <p>Let\u2019s make some code to output such a thing:</p> C++Rust Creating your first image<pre><code>#include &lt;iostream&gt;\nint main() {\n// Image\nint image_width = 256;\nint image_height = 256;\n// Render\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nfor (int i = 0; i &lt; image_width; ++i) {\nauto r = double(i) / (image_width-1);\nauto g = double(j) / (image_height-1);\nauto b = 0;\nint ir = static_cast&lt;int&gt;(255.999 * r);\nint ig = static_cast&lt;int&gt;(255.999 * g);\nint ib = static_cast&lt;int&gt;(255.999 * b);\nstd::cout &lt;&lt; ir &lt;&lt; ' ' &lt;&lt; ig &lt;&lt; ' ' &lt;&lt; ib &lt;&lt; '\\n';\n}\n}\n}\n</code></pre> <pre><code>fn main() {\nlet img_width = 256;\nlet img_height = 256;\nprintln!(\"P3\\n{} {}\\n255\\n\", img_width, img_height);\nfor j in (0..img_height).rev() {\nfor i in 0..img_width {\nlet r = i as f32 / (img_width - 1) as f32;\nlet g = j as f32 / (img_height - 1) as f32;\nlet b = 0.25;\nlet ir = (255.99 * r) as i32;\nlet ig = (255.99 * g) as i32;\nlet ib = (255.99 * b) as i32;\nprintln!(\"{} {} {}\", ir, ig, ib);\n}\n}\n}\n</code></pre> <p>There are some things to note in this code:</p> <ol> <li> <p>The pixels are written out in rows.</p> </li> <li> <p>Every row of pixels is written out left to right.</p> </li> <li> <p>These rows are written out from top to bottom.</p> </li> <li> <p>By convention, each of the red/green/blue components are represented internally by real-valued      variables that range from 0.0 to 1.0. These must be scaled to integer values between 0 and 255      before we print them out.</p> </li> <li> <p>Red goes from fully off (black) to fully on (bright red) from left to right, and green goes      from fully off at the top to black at the bottom. Adding red and green light together make      yellow so we should expect the bottom right corner to be yellow.</p> </li> </ol>"},{"location":"one_weekend/output_an_image/#creating-an-image-file","title":"Creating an Image File","text":"<p>Because the file is written to the standard output stream, you'll need to redirect it to an image file. Typically this is done from the command-line by using the <code>&gt;</code> redirection operator, like so:</p> <pre><code>build\\Release\\inOneWeekend.exe &gt; image.ppm\n</code></pre> <p>(This example assumes that you are building with CMake, using the same approach as the <code>CMakeLists.txt</code> file in the included source. Use whatever build environment (and language) you're comfortable with.)</p> <p>This is how things would look on Windows with CMake. On Mac or Linux, it might look like this:</p> <pre><code>build/inOneWeekend &gt; image.ppm\n</code></pre> <p>Opening the output file (in <code>ToyViewer</code> on my Mac, but try it in your favorite image viewer and Google \u201cppm viewer\u201d if your viewer doesn\u2019t support it) shows this result:</p> <p></p> <p>Hooray! This is the graphics \u201chello world\u201d. If your image doesn\u2019t look like that, open the output file in a text editor and see what it looks like. It should start something like this:</p> Creating your first image<pre><code>P3\n256 256\n255\n0 0 0\n1 0 0\n2 0 0\n3 0 0\n4 0 0\n5 0 0\n6 0 0\n7 0 0\n8 0 0\n9 0 0\n10 0 0\n11 0 0\n12 0 0\n...\n</code></pre> <p>If your PPM file doesn't look like this, then double-check your formatting code. If it does look like this but fails to render, then you may have line-ending differences or something similar that is confusing your image viewer. To help debug this, you can find a file <code>test.ppm</code> in the <code>images</code> directory of the Github project. This should help to ensure that your viewer can handle the PPM format and to use as a comparison against your generated PPM file.</p> <p>Some readers have reported problems viewing their generated files on Windows. In this case, the problem is often that the PPM is written out as UTF-16, often from PowerShell. If you run into this problem, see Discussion 1114 for help with this issue.</p> <p>If everything displays correctly, then you're pretty much done with system and IDE issues -- everything in the remainder of this series uses this same simple mechanism for generated rendered images.</p> <p>If you want to produce other image formats, I am a fan of <code>stb_image.h</code>, a header-only image library available on GitHub at https://github.com/nothings/stb.</p>"},{"location":"one_weekend/output_an_image/#adding-a-progress-indicator","title":"Adding a Progress Indicator","text":"<p>Before we continue, let's add a progress indicator to our output. This is a handy way to track the progress of a long render, and also to possibly identify a run that's stalled out due to an infinite loop or other problem.</p> <p>Our program outputs the image to the standard output stream (<code>std::cout</code>), so leave that alone and instead write to the logging output stream (<code>std::clog</code>):</p> C++Rust Main render loop with progress reporting<pre><code>for (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\nauto r = double(i) / (image_width-1);\nauto g = double(j) / (image_height-1);\nauto b = 0;\nint ir = static_cast&lt;int&gt;(255.999 * r);\nint ig = static_cast&lt;int&gt;(255.999 * g);\nint ib = static_cast&lt;int&gt;(255.999 * b);\nstd::cout &lt;&lt; ir &lt;&lt; ' ' &lt;&lt; ig &lt;&lt; ' ' &lt;&lt; ib &lt;&lt; '\\n';\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n</code></pre> <pre><code>for j in (0..img_height).rev() {\n// \u8fd9\u4e2a \\r \u53ef\u4ee5\u6e05\u7a7a\u5f53\u524d\u4e00\u884c\neprint!(\"\\rScanlines remaining: {} \", j);\nfor i in 0..img_width {\nlet r = i as f32 / (img_width - 1) as f32;\nlet g = j as f32 / (img_height - 1) as f32;\nlet b = 0.25;\nlet ir = (255.99 * r) as i32;\nlet ig = (255.99 * g) as i32;\nlet ib = (255.99 * b) as i32;\nprintln!(\"{} {} {}\", ir, ig, ib);\n}\n}\n// clear\neprint!(\"\\r\"); </code></pre> <p>Now when running, you'll see a running count of the number of scanlines remaining. Hopefully this runs so fast that you don't even see it! Don't worry -- you'll have lots of time in the future to watch a slowly updating progress line as we expand our ray tracer.</p>"},{"location":"one_weekend/positionable_camera/","title":"Positionable Camera","text":"<p>Cameras, like dielectrics, are a pain to debug, so I always develop mine incrementally. First, let\u2019s allow for an adjustable field of view (fov). This is the visual angle from edge to edge of the rendered image. Since our image is not square, the fov is different horizontally and vertically. I always use vertical fov. I also usually specify it in degrees and change to radians inside a constructor -- a matter of personal taste.</p>"},{"location":"one_weekend/positionable_camera/#camera-viewing-geometry","title":"Camera Viewing Geometry","text":"<p>First, we'll keep the rays coming from the origin and heading to the $z = -1$ plane. We could make it the $z = -2$ plane, or whatever, as long as we made $h$ a ratio to that distance. Here is our setup:</p> <p></p> <p>This implies $h = \\tan(\\frac{\\theta}{2})$. Our camera now becomes:</p> Camera with adjustable field-of-view (fov)<pre><code>class camera {\npublic:\ndouble aspect_ratio      = 1.0;  // Ratio of image width over height\nint    image_width       = 100;  // Rendered image width in pixel count\nint    samples_per_pixel = 10;   // Count of random samples for each pixel\nint    max_depth         = 10;   // Maximum number of ray bounces into scene\ndouble vfov = 90;  // Vertical view angle (field of view)\nvoid render(const hittable&amp; world) {\n...\nprivate:\n...\nvoid initialize() {\nimage_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\ncenter = point3(0, 0, 0);\n// Determine viewport dimensions.\nauto focal_length = 1.0;\nauto theta = degrees_to_radians(vfov);\nauto h = tan(theta/2);\nauto viewport_height = 2 * h * focal_length;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\n// Calculate the vectors across the horizontal and down the vertical viewport edges.\nauto viewport_u = vec3(viewport_width, 0, 0);\nauto viewport_v = vec3(0, -viewport_height, 0);\n// Calculate the horizontal and vertical delta vectors from pixel to pixel.\npixel_delta_u = viewport_u / image_width;\npixel_delta_v = viewport_v / image_height;\n// Calculate the location of the upper left pixel.\nauto viewport_upper_left =\ncenter - vec3(0, 0, focal_length) - viewport_u/2 - viewport_v/2;\npixel00_loc = viewport_upper_left + 0.5 * (pixel_delta_u + pixel_delta_v);\n}\n...\n};\n</code></pre> <p>We'll test out these changes with a simple scene of two touching spheres, using a 90\u00b0 field of view.</p> Scene with wide-angle camera<pre><code>int main() {\nhittable_list world;\nauto R = cos(pi/4);\nauto material_left  = make_shared&lt;lambertian&gt;(color(0,0,1));\nauto material_right = make_shared&lt;lambertian&gt;(color(1,0,0));\nworld.add(make_shared&lt;sphere&gt;(point3(-R, 0, -1), R, material_left));\nworld.add(make_shared&lt;sphere&gt;(point3( R, 0, -1), R, material_right));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov = 90;\ncam.render(world);\n}\n</code></pre> <p>This gives us the rendering:</p> <p></p>"},{"location":"one_weekend/positionable_camera/#positioning-and-orienting-the-camera","title":"Positioning and Orienting the Camera","text":"<p>To get an arbitrary viewpoint, let\u2019s first name the points we care about. We\u2019ll call the position where we place the camera lookfrom, and the point we look at lookat. (Later, if you want, you could define a direction to look in instead of a point to look at.)</p> <p>We also need a way to specify the roll, or sideways tilt, of the camera: the rotation around the lookat-lookfrom axis. Another way to think about it is that even if you keep <code>lookfrom</code> and <code>lookat</code> constant, you can still rotate your head around your nose. What we need is a way to specify an \u201cup\u201d vector for the camera.</p> <p></p> <p>We can specify any up vector we want, as long as it's not parallel to the view direction. Project this up vector onto the plane orthogonal to the view direction to get a camera-relative up vector. I use the common convention of naming this the \u201cview up\u201d (vup) vector. After a few cross products and vector normalizations, we now have a complete orthonormal basis $(u,v,w)$ to describe our camera\u2019s orientation. $u$ will be the unit vector pointing to camera right, $v$ is the unit vector pointing to camera up, $w$ is the unit vector pointing opposite the view direction (since we use right-hand coordinates), and the camera center is at the origin.</p> <p></p> <p>Like before, when our fixed camera faced $-Z$, our arbitrary view camera faces $-w$. Keep in mind that we can -- but we don\u2019t have to -- use world up $(0,1,0)$ to specify vup. This is convenient and will naturally keep your camera horizontally level until you decide to experiment with crazy camera angles.</p> Positionable and orientable camera<pre><code>class camera {\npublic:\ndouble aspect_ratio      = 1.0;  // Ratio of image width over height\nint    image_width       = 100;  // Rendered image width in pixel count\nint    samples_per_pixel = 10;   // Count of random samples for each pixel\nint    max_depth         = 10;   // Maximum number of ray bounces into scene\ndouble vfov     = 90;              // Vertical view angle (field of view)\npoint3 lookfrom = point3(0,0,-1);  // Point camera is looking from\npoint3 lookat   = point3(0,0,0);   // Point camera is looking at\nvec3   vup      = vec3(0,1,0);     // Camera-relative \"up\" direction\n...\nprivate:\nint    image_height;   // Rendered image height\npoint3 center;         // Camera center\npoint3 pixel00_loc;    // Location of pixel 0, 0\nvec3   pixel_delta_u;  // Offset to pixel to the right\nvec3   pixel_delta_v;  // Offset to pixel below\nvec3   u, v, w;        // Camera frame basis vectors\nvoid initialize() {\nimage_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\ncenter = lookfrom;\n// Determine viewport dimensions.\nauto focal_length = (lookfrom - lookat).length();\nauto theta = degrees_to_radians(vfov);\nauto h = tan(theta/2);\nauto viewport_height = 2 * h * focal_length;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\n// Calculate the u,v,w unit basis vectors for the camera coordinate frame.\nw = unit_vector(lookfrom - lookat);\nu = unit_vector(cross(vup, w));\nv = cross(w, u);\n// Calculate the vectors across the horizontal and down the vertical viewport edges.\nvec3 viewport_u = viewport_width * u;    // Vector across viewport horizontal edge\nvec3 viewport_v = viewport_height * -v;  // Vector down viewport vertical edge\n// Calculate the horizontal and vertical delta vectors from pixel to pixel.\npixel_delta_u = viewport_u / image_width;\npixel_delta_v = viewport_v / image_height;\n// Calculate the location of the upper left pixel.\nauto viewport_upper_left = center - (focal_length * w) - viewport_u/2 - viewport_v/2;\npixel00_loc = viewport_upper_left + 0.5 * (pixel_delta_u + pixel_delta_v);\n}\n...\nprivate:\n};\n</code></pre> <p>We'll change back to the prior scene, and use the new viewpoint:</p> Scene with alternate viewpoint<pre><code>int main() {\nhittable_list world;\nauto material_ground = make_shared&lt;lambertian&gt;(color(0.8, 0.8, 0.0));\nauto material_center = make_shared&lt;lambertian&gt;(color(0.1, 0.2, 0.5));\nauto material_left   = make_shared&lt;dielectric&gt;(1.5);\nauto material_right  = make_shared&lt;metal&gt;(color(0.8, 0.6, 0.2), 0.0);\nworld.add(make_shared&lt;sphere&gt;(point3( 0.0, -100.5, -1.0), 100.0, material_ground));\nworld.add(make_shared&lt;sphere&gt;(point3( 0.0,    0.0, -1.0),   0.5, material_center));\nworld.add(make_shared&lt;sphere&gt;(point3(-1.0,    0.0, -1.0),   0.5, material_left));\nworld.add(make_shared&lt;sphere&gt;(point3(-1.0,    0.0, -1.0),  -0.4, material_left));\nworld.add(make_shared&lt;sphere&gt;(point3( 1.0,    0.0, -1.0),   0.5, material_right));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 90;\ncam.lookfrom = point3(-2,2,1);\ncam.lookat   = point3(0,0,-1);\ncam.vup      = vec3(0,1,0);\ncam.render(world);\n}\n</code></pre> <p>to get:</p> <p></p> <p>And we can change field of view:</p> Change field of view<pre><code>cam.vfov     = 20;\n</code></pre> <p>to get:</p> <p></p>"},{"location":"one_weekend/rays_a_simple_camera_and_background/","title":"Rays, a Simple Camera, and Background","text":""},{"location":"one_weekend/rays_a_simple_camera_and_background/#the-ray-class","title":"The ray Class","text":"<p>The one thing that all ray tracers have is a ray class and a computation of what color is seen along a ray. Let\u2019s think of a ray as a function $\\mathbf{P}(t) = \\mathbf{A} + t \\mathbf{b}$. Here $\\mathbf{P}$ is a 3D position along a line in 3D. $\\mathbf{A}$ is the ray origin and $\\mathbf{b}$ is the ray direction. The ray parameter $t$ is a real number (<code>double</code> in the code). Plug in a different $t$ and $\\mathbf{P}(t)$ moves the point along the ray. Add in negative $t$ values and you can go anywhere on the 3D line. For positive $t$, you get only the parts in front of $\\mathbf{A}$, and this is what is often called a half-line or a ray.</p> <p></p> <p>We can represent the idea of a ray as a class, and represent the function $\\mathbf{P}(t)$ as a function that we'll call <code>ray::at(t)</code>:</p> The ray class<pre><code>#ifndef RAY_H\n#define RAY_H\n#include \"vec3.h\"\nclass ray {\npublic:\nray() {}\nray(const point3&amp; origin, const vec3&amp; direction) : orig(origin), dir(direction) {}\npoint3 origin() const  { return orig; }\nvec3 direction() const { return dir; }\npoint3 at(double t) const {\nreturn orig + t*dir;\n}\nprivate:\npoint3 orig;\nvec3 dir;\n};\n#endif\n</code></pre>"},{"location":"one_weekend/rays_a_simple_camera_and_background/#sending-rays-into-the-scene","title":"Sending Rays Into the Scene","text":"<p>Now we are ready to turn the corner and make a ray tracer. At its core, a ray tracer sends rays through pixels and computes the color seen in the direction of those rays. The involved steps are</p> <pre><code>1. Calculate the ray from the \u201ceye\u201d through the pixel,\n2. Determine which objects the ray intersects, and\n3. Compute a color for the closest intersection point.\n</code></pre> <p>When first developing a ray tracer, I always do a simple camera for getting the code up and running.</p> <p>I\u2019ve often gotten into trouble using square images for debugging because I transpose $x$ and $y$ too often, so we\u2019ll use a non-square image. A square image has a 1\u22361 aspect ratio, because its width is the same as its height. Since we want a non-square image, we'll choose 16\u22369 because it's so common. A 16\u22369 aspect ratio means that the ratio of image width to image height is 16\u22369. Put another way, given an image with a 16\u22369 aspect ratio,</p> <p>$$\\text{width} / \\text{height} = 16 / 9 = 1.7778$$</p> <p>For a practical example, an image 800 pixels wide by 400 pixels high has a 2\u22361 aspect ratio.</p> <p>The image's aspect ratio can be determined from the ratio of its height to its width. However, since we have a given aspect ratio in mind, it's easier to set the image's width and the aspect ratio, and then using this to calculate for its height. This way, we can scale up or down the image by changing the image width, and it won't throw off our desired aspect ratio. We do have to make sure that when we solve for the image height the resulting height is at least 1.</p> <p>In addition to setting up the pixel dimensions for the rendered image, we also need to set up a virtual viewport through which to pass our scene rays. The viewport is a virtual rectangle in the 3D world that contains the grid of image pixel locations. If pixels are spaced the same distance horizontally as they are vertically, the viewport that bounds them will have the same aspect ratio as the rendered image. The distance between two adjacent pixels is called the pixel spacing, and square pixels is the standard.</p> <p>To start things off, we'll choose an arbitrary viewport height of 2.0, and scale the viewport width to give us the desired aspect ratio. Here's a snippet of what this code will look like:</p> Rendered image setup<pre><code>auto aspect_ratio = 16.0 / 9.0;\nint image_width = 400;\n// Calculate the image height, and ensure that it's at least 1.\nint image_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\n// Viewport widths less than one are ok since they are real valued.\nauto viewport_height = 2.0;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\n</code></pre> <p>If you're wondering why we don't just use <code>aspect_ratio</code> when computing <code>viewport_width</code>, it's because the value set to <code>aspect_ratio</code> is the ideal ratio, it may not be the actual ratio between <code>image_width</code> and <code>image_height</code>. If <code>image_height</code> was allowed to be real valued--rather than just an integer--then it would fine to use <code>aspect_ratio</code>. But the actual ratio between <code>image_width</code> and <code>image_height</code> can vary based on two parts of the code. First, <code>integer_height</code> is rounded down to the nearest integer, which can increase the ratio. Second, we don't allow <code>integer_height</code> to be less than one, which can also change the actual aspect ratio.</p> <p>Note that <code>aspect_ratio</code> is an ideal ratio, which we approximate as best as possible with the integer-based ratio of image width over image height. In order for our viewport proportions to exactly match our image proportions, we use the calculated image aspect ratio to determine our final viewport width.</p> <p>Next we will define the camera center: a point in 3D space from which all scene rays will originate (this is also commonly referred to as the eye point). The vector from the camera center to the viewport center will be orthogonal to the viewport. We'll initially set the distance between the viewport and the camera center point to be one unit. This distance is often referred to as the focal length.</p> <p>For simplicity we'll start with the camera center at $(0,0,0)$. We'll also have the y-axis go up, the x-axis to the right, and the negative z-axis pointing in the viewing direction. (This is commonly referred to as right-handed coordinates.)</p> <p></p> <p>Now the inevitable tricky part. While our 3D space has the conventions above, this conflicts with our image coordinates, where we want to have the zeroth pixel in the top-left and work our way down to the last pixel at the bottom right. This means that our image coordinate Y-axis is inverted: Y increases going down the image.</p> <p>As we scan our image, we will start at the upper left pixel (pixel $0,0$), scan left-to-right across each row, and then scan row-by-row, top-to-bottom. To help navigate the pixel grid, we'll use a vector from the left edge to the right edge ($\\mathbf{V_u}$), and a vector from the upper edge to the lower edge ($\\mathbf{V_v}$).</p> <p>Our pixel grid will be inset from the viewport edges by half the pixel-to-pixel distance. This way, our viewport area is evenly divided into width \u00d7 height identical regions. Here's what our viewport and pixel grid look like:</p> <p></p> <p>In this figure, we have the viewport, the pixel grid for a 7\u00d75 resolution image, the viewport upper left corner $\\mathbf{Q}$, the pixel $\\mathbf{P_{0,0}}$ location, the viewport vector $\\mathbf{V_u}$ (<code>viewport_u</code>), the viewport vector $\\mathbf{V_v}$ (<code>viewport_v</code>), and the pixel delta vectors $\\mathbf{\\Delta u}$ and $\\mathbf{\\Delta v}$.</p> <p>Drawing from all of this, here's the code that implements the camera. We'll stub in a function <code>ray_color(const ray&amp; r)</code> that returns the color for a given scene ray   -- which we'll set to always return black for now.</p> Creating scene rays<pre><code>#include \"color.h\"\n#include \"ray.h\"\n#include \"vec3.h\"\n#include &lt;iostream&gt;\ncolor ray_color(const ray&amp; r) {\nreturn color(0,0,0);\n}\nint main() {\n// Image\nauto aspect_ratio = 16.0 / 9.0;\nint image_width = 400;\n// Calculate the image height, and ensure that it's at least 1.\nint image_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\n// Camera\nauto focal_length = 1.0;\nauto viewport_height = 2.0;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\nauto camera_center = point3(0, 0, 0);\n// Calculate the vectors across the horizontal and down the vertical viewport edges.\nauto viewport_u = vec3(viewport_width, 0, 0);\nauto viewport_v = vec3(0, -viewport_height, 0);\n// Calculate the horizontal and vertical delta vectors from pixel to pixel.\nauto pixel_delta_u = viewport_u / image_width;\nauto pixel_delta_v = viewport_v / image_height;\n// Calculate the location of the upper left pixel.\nauto viewport_upper_left = camera_center\n- vec3(0, 0, focal_length) - viewport_u/2 - viewport_v/2;\nauto pixel00_loc = viewport_upper_left + 0.5 * (pixel_delta_u + pixel_delta_v);\n// Render\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; \" \" &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto ray_direction = pixel_center - camera_center;\nray r(camera_center, ray_direction);\ncolor pixel_color = ray_color(r);\nwrite_color(std::cout, pixel_color);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n</code></pre> <p>Notice that in the code above, I didn't make <code>ray_direction</code> a unit vector, because I think not doing that makes for simpler and slightly faster code.</p> <p>Now we'll fill in the <code>ray_color(ray)</code> function to implement a simple gradient. This function will linearly blend white and blue depending on the height of the $y$ coordinate after scaling the ray direction to unit length (so $-1.0 &lt; y &lt; 1.0$). Because we're looking at the $y$ height after normalizing the vector, you'll notice a horizontal gradient to the color in addition to the vertical gradient.</p> <p>I'll use a standard graphics trick to linearly scale $0.0 \u2264 a \u2264 1.0$. When $a = 1.0$, I want blue. When $a = 0.0$, I want white. In between, I want a blend. This forms a \u201clinear blend\u201d, or \u201clinear interpolation\u201d. This is commonly referred to as a lerp between two values. A lerp is always of the form</p> <p>$$ \\mathit{blendedValue} = (1-a)\\cdot\\mathit{startValue} + a\\cdot\\mathit{endValue}, $$</p> <p>with $a$ going from zero to one.</p> Rendering a blue-to-white gradient<pre><code>#include \"color.h\"\n#include \"ray.h\"\n#include \"vec3.h\"\n#include &lt;iostream&gt;\ncolor ray_color(const ray&amp; r) {\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n</code></pre> <p>In our case this produces:</p> <p></p>"},{"location":"one_weekend/surface_normals_and_multiple_objects/","title":"Surface Normals and Multiple Objects","text":""},{"location":"one_weekend/surface_normals_and_multiple_objects/#shading-with-surface-normals","title":"Shading with Surface Normals","text":"<p>First, let\u2019s get ourselves a surface normal so we can shade. This is a vector that is perpendicular to the surface at the point of intersection.</p> <p>We have a key design decision to make for normal vectors in our code: whether normal vectors will have an arbitrary length, or will be normalized to unit length.</p> <p>It is tempting to skip the expensive square root operation involved in normalizing the vector, in case it's not needed. In practice, however, there are three important observations. First, if a unit-length normal vector is ever required, then you might as well do it up front once, instead of over and over again \"just in case\" for every location where unit-length is required. Second, we do require unit-length normal vectors in several places. Third, if you require normal vectors to be unit length, then you can often efficiently generate that vector with an understanding of the specific geometry class, in its constructor, or in the <code>hit()</code> function. For example, sphere normals can be made unit length simply by dividing by the sphere radius, avoiding the square root entirely.</p> <p>Given all of this, we will adopt the policy that all normal vectors will be of unit length.</p> <p>For a sphere, the outward normal is in the direction of the hit point minus the center:</p> <p></p> <p>On the earth, this means that the vector from the earth\u2019s center to you points straight up. Let\u2019s throw that into the code now, and shade it. We don\u2019t have any lights or anything yet, so let\u2019s just visualize the normals with a color map. A common trick used for visualizing normals (because it\u2019s easy and somewhat intuitive to assume $\\mathbf{n}$ is a unit length vector -- so each component is between -1 and 1) is to map each component to the interval from 0 to 1, and then map $(x, y, z)$ to $(\\mathit{red}, \\mathit{green}, \\mathit{blue})$. For the normal, we need the hit point, not just whether we hit or not (which is all we're calculating at the moment). We only have one sphere in the scene, and it's directly in front of the camera, so we won't worry about negative values of $t$ yet. We'll just assume the closest hit point (smallest $t$) is the one that we want. These changes in the code let us compute and visualize $\\mathbf{n}$:</p> Rendering surface normals on a sphere<pre><code>double hit_sphere(const point3&amp; center, double radius, const ray&amp; r) {\nvec3 oc = r.origin() - center;\nauto a = dot(r.direction(), r.direction());\nauto b = 2.0 * dot(oc, r.direction());\nauto c = dot(oc, oc) - radius*radius;\nauto discriminant = b*b - 4*a*c;\nif (discriminant &lt; 0) {\nreturn -1.0;\n} else {\nreturn (-b - sqrt(discriminant) ) / (2.0*a);\n}\n}\ncolor ray_color(const ray&amp; r) {\nauto t = hit_sphere(point3(0,0,-1), 0.5, r);\nif (t &gt; 0.0) {\nvec3 N = unit_vector(r.at(t) - vec3(0,0,-1));\nreturn 0.5*color(N.x()+1, N.y()+1, N.z()+1);\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n</code></pre> <p>And that yields this picture:</p> <p></p>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#simplifying-the-ray-sphere-intersection-code","title":"Simplifying the Ray-Sphere Intersection Code","text":"<p>Let\u2019s revisit the ray-sphere function:</p> Ray-sphere intersection code (before)<pre><code>double hit_sphere(const point3&amp; center, double radius, const ray&amp; r) {\nvec3 oc = r.origin() - center;\nauto a = dot(r.direction(), r.direction());\nauto b = 2.0 * dot(oc, r.direction());\nauto c = dot(oc, oc) - radius*radius;\nauto discriminant = b*b - 4*a*c;\nif (discriminant &lt; 0) {\nreturn -1.0;\n} else {\nreturn (-b - sqrt(discriminant) ) / (2.0*a);\n}\n}\n</code></pre> <p>First, recall that a vector dotted with itself is equal to the squared length of that vector.</p> <p>Second, notice how the equation for <code>b</code> has a factor of two in it. Consider what happens to the quadratic equation if $b = 2h$:</p> <p>$$ \\frac{-b \\pm \\sqrt{b^2 - 4ac}}{2a} $$</p> <p>$$ = \\frac{-2h \\pm \\sqrt{(2h)^2 - 4ac}}{2a} $$</p> <p>$$ = \\frac{-2h \\pm 2\\sqrt{h^2 - ac}}{2a} $$</p> <p>$$ = \\frac{-h \\pm \\sqrt{h^2 - ac}}{a} $$</p> <p>Using these observations, we can now simplify the sphere-intersection code to this:</p> Ray-sphere intersection code (after)<pre><code>double hit_sphere(const point3&amp; center, double radius, const ray&amp; r) {\nvec3 oc = r.origin() - center;\nauto a = r.direction().length_squared();\nauto half_b = dot(oc, r.direction());\nauto c = oc.length_squared() - radius*radius;\nauto discriminant = half_b*half_b - a*c;\nif (discriminant &lt; 0) {\nreturn -1.0;\n} else {\nreturn (-half_b - sqrt(discriminant) ) / a;\n}\n}\n</code></pre>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#an-abstraction-for-hittable-objects","title":"An Abstraction for Hittable Objects","text":"<p>Now, how about more than one sphere? While it is tempting to have an array of spheres, a very clean solution is to make an \u201cabstract class\u201d for anything a ray might hit, and make both a sphere and a list of spheres just something that can be hit. What that class should be called is something of a quandary -- calling it an \u201cobject\u201d would be good if not for \u201cobject oriented\u201d programming. \u201cSurface\u201d is often used, with the weakness being maybe we will want volumes (fog, clouds, stuff like that). \u201chittable\u201d emphasizes the member function that unites them. I don\u2019t love any of these, but we'll go with \u201chittable\u201d.</p> <p>This <code>hittable</code> abstract class will have a <code>hit</code> function that takes in a ray. Most ray tracers have found it convenient to add a valid interval for hits $t_{\\mathit{min}}$ to $t_{\\mathit{max}}$, so the hit only \u201ccounts\u201d if $t_{\\mathit{min}} &lt; t &lt; t_{\\mathit{max}}$. For the initial rays this is positive $t$, but as we will see, it can simplify our code to have an interval $t_{\\mathit{min}}$ to $t_{\\mathit{max}}$. One design question is whether to do things like compute the normal if we hit something. We might end up hitting something closer as we do our search, and we will only need the normal of the closest thing. I will go with the simple solution and compute a bundle of stuff I will store in some structure. Here\u2019s the abstract class:</p> The hittable class<pre><code>#ifndef HITTABLE_H\n#define HITTABLE_H\n#include \"ray.h\"\nclass hit_record {\npublic:\npoint3 p;\nvec3 normal;\ndouble t;\n};\nclass hittable {\npublic:\nvirtual ~hittable() = default;\nvirtual bool hit(const ray&amp; r, double ray_tmin, double ray_tmax, hit_record&amp; rec) const = 0;\n};\n#endif\n</code></pre> <p>And here\u2019s the sphere:</p> The sphere class<pre><code>#ifndef SPHERE_H\n#define SPHERE_H\n#include \"hittable.h\"\n#include \"vec3.h\"\nclass sphere : public hittable {\npublic:\nsphere(point3 _center, double _radius) : center(_center), radius(_radius) {}\nbool hit(const ray&amp; r, double ray_tmin, double ray_tmax, hit_record&amp; rec) const override {\nvec3 oc = r.origin() - center;\nauto a = r.direction().length_squared();\nauto half_b = dot(oc, r.direction());\nauto c = oc.length_squared() - radius*radius;\nauto discriminant = half_b*half_b - a*c;\nif (discriminant &lt; 0) return false;\nauto sqrtd = sqrt(discriminant);\n// Find the nearest root that lies in the acceptable range.\nauto root = (-half_b - sqrtd) / a;\nif (root &lt;= ray_tmin || ray_tmax &lt;= root) {\nroot = (-half_b + sqrtd) / a;\nif (root &lt;= ray_tmin || ray_tmax &lt;= root)\nreturn false;\n}\nrec.t = root;\nrec.p = r.at(rec.t);\nrec.normal = (rec.p - center) / radius;\nreturn true;\n}\nprivate:\npoint3 center;\ndouble radius;\n};\n#endif\n</code></pre>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#front-faces-versus-back-faces","title":"Front Faces Versus Back Faces","text":"<p>The second design decision for normals is whether they should always point out. At present, the normal found will always be in the direction of the center to the intersection point (the normal points out). If the ray intersects the sphere from the outside, the normal points against the ray. If the ray intersects the sphere from the inside, the normal (which always points out) points with the ray. Alternatively, we can have the normal always point against the ray. If the ray is outside the sphere, the normal will point outward, but if the ray is inside the sphere, the normal will point inward.</p> <p></p> <p>We need to choose one of these possibilities because we will eventually want to determine which side of the surface that the ray is coming from. This is important for objects that are rendered differently on each side, like the text on a two-sided sheet of paper, or for objects that have an inside and an outside, like glass balls.</p> <p>If we decide to have the normals always point out, then we will need to determine which side the ray is on when we color it. We can figure this out by comparing the ray with the normal. If the ray and the normal face in the same direction, the ray is inside the object, if the ray and the normal face in the opposite direction, then the ray is outside the object. This can be determined by taking the dot product of the two vectors, where if their dot is positive, the ray is inside the sphere.</p> Comparing the ray and the normal<pre><code>if (dot(ray_direction, outward_normal) &gt; 0.0) {\n// ray is inside the sphere\n...\n} else {\n// ray is outside the sphere\n...\n}\n</code></pre> <p>If we decide to have the normals always point against the ray, we won't be able to use the dot product to determine which side of the surface the ray is on. Instead, we would need to store that information:</p> Remembering the side of the surface<pre><code>bool front_face;\nif (dot(ray_direction, outward_normal) &gt; 0.0) {\n// ray is inside the sphere\nnormal = -outward_normal;\nfront_face = false;\n} else {\n// ray is outside the sphere\nnormal = outward_normal;\nfront_face = true;\n}\n</code></pre> <p>We can set things up so that normals always point \u201coutward\u201d from the surface, or always point against the incident ray. This decision is determined by whether you want to determine the side of the surface at the time of geometry intersection or at the time of coloring. In this book we have more material types than we have geometry types, so we'll go for less work and put the determination at geometry time. This is simply a matter of preference, and you'll see both implementations in the literature.</p> <p>We add the <code>front_face</code> bool to the <code>hit_record</code> class. We'll also add a function to solve this calculation for us: <code>set_face_normal()</code>. For convenience we will assume that the vector passed to the new <code>set_face_normal()</code> function is of unit length. We could always normalize the parameter explicitly, but it's more efficient if the geometry code does this, as it's usually easier when you know more about the specific geometry.</p> Adding front-face tracking to hit_record<pre><code>class hit_record {\npublic:\npoint3 p;\nvec3 normal;\ndouble t;\nbool front_face;\nvoid set_face_normal(const ray&amp; r, const vec3&amp; outward_normal) {\n// Sets the hit record normal vector.\n// NOTE: the parameter `outward_normal` is assumed to have unit length.\nfront_face = dot(r.direction(), outward_normal) &lt; 0;\nnormal = front_face ? outward_normal : -outward_normal;\n}\n};\n</code></pre> <p>And then we add the surface side determination to the class:</p> The sphere class with normal determination<pre><code>class sphere : public hittable {\npublic:\n...\nbool hit(const ray&amp; r, double ray_tmin, double ray_tmax, hit_record&amp; rec) const {\n...\nrec.t = root;\nrec.p = r.at(rec.t);\nvec3 outward_normal = (rec.p - center) / radius;\nrec.set_face_normal(r, outward_normal);\nreturn true;\n}\n...\n};\n</code></pre>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#a-list-of-hittable-objects","title":"A List of Hittable Objects","text":"<p>We have a generic object called a <code>hittable</code> that the ray can intersect with. We now add a class that stores a list of <code>hittable</code>s:</p> The hittable_list class<pre><code>#ifndef HITTABLE_LIST_H\n#define HITTABLE_LIST_H\n#include \"hittable.h\"\n#include &lt;memory&gt;\n#include &lt;vector&gt;\nusing std::shared_ptr;\nusing std::make_shared;\nclass hittable_list : public hittable {\npublic:\nstd::vector&lt;shared_ptr&lt;hittable&gt;&gt; objects;\nhittable_list() {}\nhittable_list(shared_ptr&lt;hittable&gt; object) { add(object); }\nvoid clear() { objects.clear(); }\nvoid add(shared_ptr&lt;hittable&gt; object) {\nobjects.push_back(object);\n}\nbool hit(const ray&amp; r, double ray_tmin, double ray_tmax, hit_record&amp; rec) const override {\nhit_record temp_rec;\nbool hit_anything = false;\nauto closest_so_far = ray_tmax;\nfor (const auto&amp; object : objects) {\nif (object-&gt;hit(r, ray_tmin, closest_so_far, temp_rec)) {\nhit_anything = true;\nclosest_so_far = temp_rec.t;\nrec = temp_rec;\n}\n}\nreturn hit_anything;\n}\n};\n#endif\n</code></pre>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#some-new-c-features","title":"Some New C++ Features","text":"<p>The <code>hittable_list</code> class code uses two C++ features that may trip you up if you're not normally a C++ programmer: <code>vector</code> and <code>shared_ptr</code>.</p> <p><code>shared_ptr&lt;type&gt;</code> is a pointer to some allocated type, with reference-counting semantics. Every time you assign its value to another shared pointer (usually with a simple assignment), the reference count is incremented. As shared pointers go out of scope (like at the end of a block or function), the reference count is decremented. Once the count goes to zero, the object is safely deleted.</p> <p>Typically, a shared pointer is first initialized with a newly-allocated object, something like this:</p> An example allocation using shared_ptr<pre><code>shared_ptr&lt;double&gt; double_ptr = make_shared&lt;double&gt;(0.37);\nshared_ptr&lt;vec3&gt;   vec3_ptr   = make_shared&lt;vec3&gt;(1.414214, 2.718281, 1.618034);\nshared_ptr&lt;sphere&gt; sphere_ptr = make_shared&lt;sphere&gt;(point3(0,0,0), 1.0);\n</code></pre> <p><code>make_shared&lt;thing&gt;(thing_constructor_params ...)</code> allocates a new instance of type <code>thing</code>, using the constructor parameters. It returns a <code>shared_ptr&lt;thing&gt;</code>.</p> <p>Since the type can be automatically deduced by the return type of <code>make_shared&lt;type&gt;(...)</code>, the above lines can be more simply expressed using C++'s <code>auto</code> type specifier:</p> An example allocation using shared_ptr with auto type<pre><code>auto double_ptr = make_shared&lt;double&gt;(0.37);\nauto vec3_ptr   = make_shared&lt;vec3&gt;(1.414214, 2.718281, 1.618034);\nauto sphere_ptr = make_shared&lt;sphere&gt;(point3(0,0,0), 1.0);\n</code></pre> <p>We'll use shared pointers in our code, because it allows multiple geometries to share a common instance (for example, a bunch of spheres that all use the same color material), and because it makes memory management automatic and easier to reason about.</p> <p><code>std::shared_ptr</code> is included with the <code>&lt;memory&gt;</code> header.</p> <p>The second C++ feature you may be unfamiliar with is <code>std::vector</code>. This is a generic array-like collection of an arbitrary type. Above, we use a collection of pointers to <code>hittable</code>. <code>std::vector</code> automatically grows as more values are added: <code>objects.push_back(object)</code> adds a value to the end of the <code>std::vector</code> member variable <code>objects</code>.</p> <p><code>std::vector</code> is included with the <code>&lt;vector&gt;</code> header.</p> <p>Finally, the <code>using</code> statements in listing [hittable-list-initial] tell the compiler that we'll be getting <code>shared_ptr</code> and <code>make_shared</code> from the <code>std</code> library, so we don't need to prefix these with <code>std::</code> every time we reference them.</p>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#common-constants-and-utility-functions","title":"Common Constants and Utility Functions","text":"<p>We need some math constants that we conveniently define in their own header file. For now we only need infinity, but we will also throw our own definition of pi in there, which we will need later. There is no standard portable definition of pi, so we just define our own constant for it. We'll throw common useful constants and future utility functions in <code>rtweekend.h</code>, our general main header file.</p> The rtweekend.h common header<pre><code>#ifndef RTWEEKEND_H\n#define RTWEEKEND_H\n#include &lt;cmath&gt;\n#include &lt;limits&gt;\n#include &lt;memory&gt;\n// Usings\nusing std::shared_ptr;\nusing std::make_shared;\nusing std::sqrt;\n// Constants\nconst double infinity = std::numeric_limits&lt;double&gt;::infinity();\nconst double pi = 3.1415926535897932385;\n// Utility Functions\ninline double degrees_to_radians(double degrees) {\nreturn degrees * pi / 180.0;\n}\n// Common Headers\n#include \"ray.h\"\n#include \"vec3.h\"\n#endif\n</code></pre> <p>And the new main:</p> The new main with hittables<pre><code>#include \"rtweekend.h\"\n#include \"color.h\"\n#include \"hittable.h\"\n#include \"hittable_list.h\"\n#include \"sphere.h\"\n#include &lt;iostream&gt;\n//double hit_sphere(const point3&amp; center, double radius, const ray&amp; r) {\n//  ...\n//}\ncolor ray_color(const ray&amp; r, const hittable&amp; world) {\nhit_record rec;\nif (world.hit(r, 0, infinity, rec)) {\nreturn 0.5 * (rec.normal + color(1,1,1));\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\nint main() {\n// Image\nauto aspect_ratio = 16.0 / 9.0;\nint image_width = 400;\n// Calculate the image height, and ensure that it's at least 1.\nint image_height = static_cast&lt;int&gt;(image_width / aspect_ratio);\nimage_height = (image_height &lt; 1) ? 1 : image_height;\n// World\nhittable_list world;\nworld.add(make_shared&lt;sphere&gt;(point3(0,0,-1), 0.5));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-100.5,-1), 100));\n// Camera\nauto focal_length = 1.0;\nauto viewport_height = 2.0;\nauto viewport_width = viewport_height * (static_cast&lt;double&gt;(image_width)/image_height);\nauto camera_center = point3(0, 0, 0);\n// Calculate the vectors across the horizontal and down the vertical viewport edges.\nauto viewport_u = vec3(viewport_width, 0, 0);\nauto viewport_v = vec3(0, -viewport_height, 0);\n// Calculate the horizontal and vertical delta vectors from pixel to pixel.\nauto pixel_delta_u = viewport_u / image_width;\nauto pixel_delta_v = viewport_v / image_height;\n// Calculate the location of the upper left pixel.\nauto viewport_upper_left = camera_center\n- vec3(0, 0, focal_length) - viewport_u/2 - viewport_v/2;\nauto pixel00_loc = viewport_upper_left + 0.5 * (pixel_delta_u + pixel_delta_v);\n// Render\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto ray_direction = pixel_center - camera_center;\nray r(camera_center, ray_direction);\ncolor pixel_color = ray_color(r, world);\nwrite_color(std::cout, pixel_color);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n</code></pre> <p>This yields a picture that is really just a visualization of where the spheres are located along with their surface normal. This is often a great way to view any flaws or specific characteristics of a geometric model.</p> <p></p>"},{"location":"one_weekend/surface_normals_and_multiple_objects/#an-interval-class","title":"An Interval Class","text":"<p>Before we continue, we'll implement an interval class to manage real-valued intervals with a minimum and a maximum. We'll end up using this class quite often as we proceed.</p> Introducing the new interval class<pre><code>#ifndef INTERVAL_H\n#define INTERVAL_H\nclass interval {\npublic:\ndouble min, max;\ninterval() : min(+infinity), max(-infinity) {} // Default interval is empty\ninterval(double _min, double _max) : min(_min), max(_max) {}\nbool contains(double x) const {\nreturn min &lt;= x &amp;&amp; x &lt;= max;\n}\nbool surrounds(double x) const {\nreturn min &lt; x &amp;&amp; x &lt; max;\n}\nstatic const interval empty, universe;\n};\nconst static interval empty   (+infinity, -infinity);\nconst static interval universe(-infinity, +infinity);\n#endif\n</code></pre> Including the new interval class<pre><code>// Common Headers\n#include \"interval.h\"\n#include \"ray.h\"\n#include \"vec3.h\"\n</code></pre> hittable::hit() using interval<pre><code>class hittable {\npublic:\n...\nvirtual bool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const = 0;\n};\n</code></pre> hittable_list::hit() using interval<pre><code>class hittable_list : public hittable {\npublic:\n...\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\nhit_record temp_rec;\nbool hit_anything = false;\nauto closest_so_far = ray_t.max;\nfor (const auto&amp; object : objects) {\nif (object-&gt;hit(r, interval(ray_t.min, closest_so_far), temp_rec)) {\nhit_anything = true;\nclosest_so_far = temp_rec.t;\nrec = temp_rec;\n}\n}\nreturn hit_anything;\n}\n...\n};\n</code></pre> sphere using interval<pre><code>class sphere : public hittable {\npublic:\n...\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n...\n// Find the nearest root that lies in the acceptable range.\nauto root = (-half_b - sqrtd) / a;\nif (!ray_t.surrounds(root)) {\nroot = (-half_b + sqrtd) / a;\nif (!ray_t.surrounds(root))\nreturn false;\n}\n...\n}\n...\n};\n</code></pre> The new main using interval<pre><code>...\ncolor ray_color(const ray&amp; r, const hittable&amp; world) {\nhit_record rec;\nif (world.hit(r, interval(0, infinity), rec)) {\nreturn 0.5 * (rec.normal + color(1,1,1));\n}\nvec3 unit_direction = unit_vector(r.direction());\nauto a = 0.5*(unit_direction.y() + 1.0);\nreturn (1.0-a)*color(1.0, 1.0, 1.0) + a*color(0.5, 0.7, 1.0);\n}\n...\n</code></pre>"},{"location":"one_weekend/the_vec3_class/","title":"The vec3 Class","text":"<p>Almost all graphics programs have some class(es) for storing geometric vectors and colors. In many systems these vectors are 4D (3D position plus a homogeneous coordinate for geometry, or RGB plus an alpha transparency component for colors). For our purposes, three coordinates suffice. We\u2019ll use the same class <code>vec3</code> for colors, locations, directions, offsets, whatever. Some people don\u2019t like this because it doesn\u2019t prevent you from doing something silly, like subtracting a position from a color. They have a good point, but we\u2019re going to always take the \u201cless code\u201d route when not obviously wrong. In spite of this, we do declare two aliases for <code>vec3</code>: <code>point3</code> and <code>color</code>. Since these two types are just aliases for <code>vec3</code>, you won't get warnings if you pass a <code>color</code> to a function expecting a <code>point3</code>, and nothing is stopping you from adding a <code>point3</code> to a <code>color</code>, but it makes the code a little bit easier to read and to understand.</p> <p>We define the <code>vec3</code> class in the top half of a new <code>vec3.h</code> header file, and define a set of useful vector utility functions in the bottom half:</p> vec3 definitions and helper functions<pre><code>#ifndef VEC3_H\n#define VEC3_H\n#include &lt;cmath&gt;\n#include &lt;iostream&gt;\nusing std::sqrt;\nclass vec3 {\npublic:\ndouble e[3];\nvec3() : e{0,0,0} {}\nvec3(double e0, double e1, double e2) : e{e0, e1, e2} {}\ndouble x() const { return e[0]; }\ndouble y() const { return e[1]; }\ndouble z() const { return e[2]; }\nvec3 operator-() const { return vec3(-e[0], -e[1], -e[2]); }\ndouble operator[](int i) const { return e[i]; }\ndouble&amp; operator[](int i) { return e[i]; }\nvec3&amp; operator+=(const vec3 &amp;v) {\ne[0] += v.e[0];\ne[1] += v.e[1];\ne[2] += v.e[2];\nreturn *this;\n}\nvec3&amp; operator*=(double t) {\ne[0] *= t;\ne[1] *= t;\ne[2] *= t;\nreturn *this;\n}\nvec3&amp; operator/=(double t) {\nreturn *this *= 1/t;\n}\ndouble length() const {\nreturn sqrt(length_squared());\n}\ndouble length_squared() const {\nreturn e[0]*e[0] + e[1]*e[1] + e[2]*e[2];\n}\n};\n// point3 is just an alias for vec3, but useful for geometric clarity in the code.\nusing point3 = vec3;\n// Vector Utility Functions\ninline std::ostream&amp; operator&lt;&lt;(std::ostream &amp;out, const vec3 &amp;v) {\nreturn out &lt;&lt; v.e[0] &lt;&lt; ' ' &lt;&lt; v.e[1] &lt;&lt; ' ' &lt;&lt; v.e[2];\n}\ninline vec3 operator+(const vec3 &amp;u, const vec3 &amp;v) {\nreturn vec3(u.e[0] + v.e[0], u.e[1] + v.e[1], u.e[2] + v.e[2]);\n}\ninline vec3 operator-(const vec3 &amp;u, const vec3 &amp;v) {\nreturn vec3(u.e[0] - v.e[0], u.e[1] - v.e[1], u.e[2] - v.e[2]);\n}\ninline vec3 operator*(const vec3 &amp;u, const vec3 &amp;v) {\nreturn vec3(u.e[0] * v.e[0], u.e[1] * v.e[1], u.e[2] * v.e[2]);\n}\ninline vec3 operator*(double t, const vec3 &amp;v) {\nreturn vec3(t*v.e[0], t*v.e[1], t*v.e[2]);\n}\ninline vec3 operator*(const vec3 &amp;v, double t) {\nreturn t * v;\n}\ninline vec3 operator/(vec3 v, double t) {\nreturn (1/t) * v;\n}\ninline double dot(const vec3 &amp;u, const vec3 &amp;v) {\nreturn u.e[0] * v.e[0]\n+ u.e[1] * v.e[1]\n+ u.e[2] * v.e[2];\n}\ninline vec3 cross(const vec3 &amp;u, const vec3 &amp;v) {\nreturn vec3(u.e[1] * v.e[2] - u.e[2] * v.e[1],\nu.e[2] * v.e[0] - u.e[0] * v.e[2],\nu.e[0] * v.e[1] - u.e[1] * v.e[0]);\n}\ninline vec3 unit_vector(vec3 v) {\nreturn v / v.length();\n}\n#endif\n</code></pre> <p>We use <code>double</code> here, but some ray tracers use <code>float</code>. <code>double</code> has greater precision and range, but is twice the size compared to <code>float</code>. This increase in size may be important if you're programming in limited memory conditions (such as hardware shaders). Either one is fine -- follow your own tastes.</p>"},{"location":"one_weekend/the_vec3_class/#color-utility-functions","title":"Color Utility Functions","text":"<p>Using our new <code>vec3</code> class, we'll create a new <code>color.h</code> header file and define a utility function that writes a single pixel's color out to the standard output stream.</p> color utility functions<pre><code>#ifndef COLOR_H\n#define COLOR_H\n#include \"vec3.h\"\n#include &lt;iostream&gt;\nusing color = vec3;\nvoid write_color(std::ostream &amp;out, color pixel_color) {\n// Write the translated [0,255] value of each color component.\nout &lt;&lt; static_cast&lt;int&gt;(255.999 * pixel_color.x()) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(255.999 * pixel_color.y()) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(255.999 * pixel_color.z()) &lt;&lt; '\\n';\n}\n#endif\n</code></pre> <p>Now we can change our main to use both of these:</p> Final code for the first PPM image<pre><code>#include \"color.h\"\n#include \"vec3.h\"\n#include &lt;iostream&gt;\nint main() {\n// Image\nint image_width = 256;\nint image_height = 256;\n// Render\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\nauto pixel_color = color(double(i)/(image_width-1), double(j)/(image_height-1), 0);\nwrite_color(std::cout, pixel_color);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n</code></pre> <p>And you should get the exact same picture as before.</p>"},{"location":"one_weekend/where_next/","title":"Where Next?","text":""},{"location":"one_weekend/where_next/#a-final-render","title":"A Final Render","text":"<p>Let\u2019s make the image on the cover of this book -- lots of random spheres.</p> Final scene<pre><code>int main() {\nhittable_list world;\nauto ground_material = make_shared&lt;lambertian&gt;(color(0.5, 0.5, 0.5));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, ground_material));\nfor (int a = -11; a &lt; 11; a++) {\nfor (int b = -11; b &lt; 11; b++) {\nauto choose_mat = random_double();\npoint3 center(a + 0.9*random_double(), 0.2, b + 0.9*random_double());\nif ((center - point3(4, 0.2, 0)).length() &gt; 0.9) {\nshared_ptr&lt;material&gt; sphere_material;\nif (choose_mat &lt; 0.8) {\n// diffuse\nauto albedo = color::random() * color::random();\nsphere_material = make_shared&lt;lambertian&gt;(albedo);\nworld.add(make_shared&lt;sphere&gt;(center, 0.2, sphere_material));\n} else if (choose_mat &lt; 0.95) {\n// metal\nauto albedo = color::random(0.5, 1);\nauto fuzz = random_double(0, 0.5);\nsphere_material = make_shared&lt;metal&gt;(albedo, fuzz);\nworld.add(make_shared&lt;sphere&gt;(center, 0.2, sphere_material));\n} else {\n// glass\nsphere_material = make_shared&lt;dielectric&gt;(1.5);\nworld.add(make_shared&lt;sphere&gt;(center, 0.2, sphere_material));\n}\n}\n}\n}\nauto material1 = make_shared&lt;dielectric&gt;(1.5);\nworld.add(make_shared&lt;sphere&gt;(point3(0, 1, 0), 1.0, material1));\nauto material2 = make_shared&lt;lambertian&gt;(color(0.4, 0.2, 0.1));\nworld.add(make_shared&lt;sphere&gt;(point3(-4, 1, 0), 1.0, material2));\nauto material3 = make_shared&lt;metal&gt;(color(0.7, 0.6, 0.5), 0.0);\nworld.add(make_shared&lt;sphere&gt;(point3(4, 1, 0), 1.0, material3));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 1200;\ncam.samples_per_pixel = 500;\ncam.max_depth         = 50;\ncam.vfov     = 20;\ncam.lookfrom = point3(13,2,3);\ncam.lookat   = point3(0,0,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0.6;\ncam.focus_dist    = 10.0;\ncam.render(world);\n}\n</code></pre> <p>(Note that the code above differs slightly from the project sample code: the <code>samples_per_pixel</code> is set to 500 above for a high-quality image that will take quite a while to render. The sample code uses a value of 10 in the interest of reasonable run times while developing and validating.)</p> <p>This gives:</p> <p></p> <p>An interesting thing you might note is the glass balls don\u2019t really have shadows which makes them look like they are floating. This is not a bug -- you don\u2019t see glass balls much in real life, where they also look a bit strange, and indeed seem to float on cloudy days. A point on the big sphere under a glass ball still has lots of light hitting it because the sky is re-ordered rather than blocked.</p>"},{"location":"one_weekend/where_next/#next-steps","title":"Next Steps","text":"<p>You now have a cool ray tracer! What next?</p> <ol> <li> <p>Lights -- You can do this explicitly, by sending shadow rays to lights, or it can be done      implicitly by making some objects emit light, biasing scattered rays toward them, and then      downweighting those rays to cancel out the bias. Both work. I am in the minority in favoring      the latter approach.</p> </li> <li> <p>Triangles -- Most cool models are in triangle form. The model I/O is the worst and almost      everybody tries to get somebody else\u2019s code to do this.</p> </li> <li> <p>Surface Textures -- This lets you paste images on like wall paper. Pretty easy and a good thing      to do.</p> </li> <li> <p>Solid textures -- Ken Perlin has his code online. Andrew Kensler has some very cool info at his      blog.</p> </li> <li> <p>Volumes and Media -- Cool stuff and will challenge your software architecture. I favor making      volumes have the hittable interface and probabilistically have intersections based on density.      Your rendering code doesn\u2019t even have to know it has volumes with that method.</p> </li> <li> <p>Parallelism -- Run $N$ copies of your code on $N$ cores with different random seeds. Average      the $N$ runs. This averaging can also be done hierarchically where $N/2$ pairs can be averaged      to get $N/4$ images, and pairs of those can be averaged. That method of parallelism should      extend well into the thousands of cores with very little coding.</p> </li> </ol> <p>Have fun, and please send me your cool images!</p>"},{"location":"the_next_week/a_scene_testing_all_new_features/","title":"A Scene Testing All New Features","text":"<p>Let\u2019s put it all together, with a big thin mist covering everything, and a blue subsurface reflection sphere (we didn\u2019t implement that explicitly, but a volume inside a dielectric is what a subsurface material is). The biggest limitation left in the renderer is no shadow rays, but that is why we get caustics and subsurface for free. It\u2019s a double-edged design decision.</p> <p>Also note that we'll parameterize this final scene to support a lower quality render for quick testing.</p> Final scene<pre><code>#include \"bvh.h\"\n...\nvoid final_scene(int image_width, int samples_per_pixel, int max_depth) {\nhittable_list boxes1;\nauto ground = make_shared&lt;lambertian&gt;(color(0.48, 0.83, 0.53));\nint boxes_per_side = 20;\nfor (int i = 0; i &lt; boxes_per_side; i++) {\nfor (int j = 0; j &lt; boxes_per_side; j++) {\nauto w = 100.0;\nauto x0 = -1000.0 + i*w;\nauto z0 = -1000.0 + j*w;\nauto y0 = 0.0;\nauto x1 = x0 + w;\nauto y1 = random_double(1,101);\nauto z1 = z0 + w;\nboxes1.add(box(point3(x0,y0,z0), point3(x1,y1,z1), ground));\n}\n}\nhittable_list world;\nworld.add(make_shared&lt;bvh_node&gt;(boxes1));\nauto light = make_shared&lt;diffuse_light&gt;(color(7, 7, 7));\nworld.add(make_shared&lt;quad&gt;(point3(123,554,147), vec3(300,0,0), vec3(0,0,265), light));\nauto center1 = point3(400, 400, 200);\nauto center2 = center1 + vec3(30,0,0);\nauto sphere_material = make_shared&lt;lambertian&gt;(color(0.7, 0.3, 0.1));\nworld.add(make_shared&lt;sphere&gt;(center1, center2, 50, sphere_material));\nworld.add(make_shared&lt;sphere&gt;(point3(260, 150, 45), 50, make_shared&lt;dielectric&gt;(1.5)));\nworld.add(make_shared&lt;sphere&gt;(\npoint3(0, 150, 145), 50, make_shared&lt;metal&gt;(color(0.8, 0.8, 0.9), 1.0)\n));\nauto boundary = make_shared&lt;sphere&gt;(point3(360,150,145), 70, make_shared&lt;dielectric&gt;(1.5));\nworld.add(boundary);\nworld.add(make_shared&lt;constant_medium&gt;(boundary, 0.2, color(0.2, 0.4, 0.9)));\nboundary = make_shared&lt;sphere&gt;(point3(0,0,0), 5000, make_shared&lt;dielectric&gt;(1.5));\nworld.add(make_shared&lt;constant_medium&gt;(boundary, .0001, color(1,1,1)));\nauto emat = make_shared&lt;lambertian&gt;(make_shared&lt;image_texture&gt;(\"earthmap.jpg\"));\nworld.add(make_shared&lt;sphere&gt;(point3(400,200,400), 100, emat));\nauto pertext = make_shared&lt;noise_texture&gt;(0.1);\nworld.add(make_shared&lt;sphere&gt;(point3(220,280,300), 80, make_shared&lt;lambertian&gt;(pertext)));\nhittable_list boxes2;\nauto white = make_shared&lt;lambertian&gt;(color(.73, .73, .73));\nint ns = 1000;\nfor (int j = 0; j &lt; ns; j++) {\nboxes2.add(make_shared&lt;sphere&gt;(point3::random(0,165), 10, white));\n}\nworld.add(make_shared&lt;translate&gt;(\nmake_shared&lt;rotate_y&gt;(\nmake_shared&lt;bvh_node&gt;(boxes2), 15),\nvec3(-100,270,395)\n)\n);\ncamera cam;\ncam.aspect_ratio      = 1.0;\ncam.image_width       = image_width;\ncam.samples_per_pixel = samples_per_pixel;\ncam.max_depth         = max_depth;\ncam.background        = color(0,0,0);\ncam.vfov     = 40;\ncam.lookfrom = point3(478, 278, -600);\ncam.lookat   = point3(278, 278, 0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (0) {\ncase 1:  random_spheres();            break;\ncase 2:  two_spheres();               break;\ncase 3:  earth();                     break;\ncase 4:  two_perlin_spheres();        break;\ncase 5:  quads();                     break;\ncase 6:  simple_light();              break;\ncase 7:  cornell_box();               break;\ncase 8:  cornell_smoke();             break;\ncase 9:  final_scene(800, 10000, 40); break;\ndefault: final_scene(400,   250,  4); break;\n}\n}\n</code></pre> <p>Running it with 10,000 rays per pixel (sweet dreams) yields:</p> <p></p> <p>Now go off and make a really cool image of your own! See https://in1weekend.blogspot.com/ for pointers to further reading and features, and feel free to email questions, comments, and cool images to me at ptrshrl@gmail.com.</p>"},{"location":"the_next_week/bounding_volume_hierarchies/","title":"Bounding Volume Hierarchies","text":"<p>This part is by far the most difficult and involved part of the ray tracer we are working on. I am sticking it in this chapter so the code can run faster, and because it refactors <code>hittable</code> a little, and when I add rectangles and boxes we won't have to go back and refactor them.</p> <p>The ray-object intersection is the main time-bottleneck in a ray tracer, and the time is linear with the number of objects. But it\u2019s a repeated search on the same model, so we ought to be able to make it a logarithmic search in the spirit of binary search. Because we are sending millions to billions of rays on the same model, we can do an analog of sorting the model, and then each ray intersection can be a sublinear search. The two most common families of sorting are to 1) divide the space, and 2) divide the objects. The latter is usually much easier to code up and just as fast to run for most models.</p>"},{"location":"the_next_week/bounding_volume_hierarchies/#the-key-idea","title":"The Key Idea","text":"<p>The key idea of a bounding volume over a set of primitives is to find a volume that fully encloses (bounds) all the objects. For example, suppose you computed a sphere that bounds 10 objects. Any ray that misses the bounding sphere definitely misses all ten objects inside. If the ray hits the bounding sphere, then it might hit one of the ten objects. So the bounding code is always of the form:</p> <pre><code>if (ray hits bounding object)\nreturn whether ray hits bounded objects\nelse\nreturn false\n</code></pre> <p>A key thing is we are dividing objects into subsets. We are not dividing the screen or the volume. Any object is in just one bounding volume, but bounding volumes can overlap.</p>"},{"location":"the_next_week/bounding_volume_hierarchies/#hierarchies-of-bounding-volumes","title":"Hierarchies of Bounding Volumes","text":"<p>To make things sub-linear we need to make the bounding volumes hierarchical. For example, if we divided a set of objects into two groups, red and blue, and used rectangular bounding volumes, we\u2019d have:</p> <p></p> <p>Note that the blue and red bounding volumes are contained in the purple one, but they might overlap, and they are not ordered -- they are just both inside. So the tree shown on the right has no concept of ordering in the left and right children; they are simply inside. The code would be:</p> <pre><code>if (hits purple)\nhit0 = hits blue enclosed objects\nhit1 = hits red enclosed objects\nif (hit0 or hit1)\nreturn true and info of closer hit\nreturn false\n</code></pre>"},{"location":"the_next_week/bounding_volume_hierarchies/#axis-aligned-bounding-boxes-aabbs","title":"Axis-Aligned Bounding Boxes (AABBs)","text":"<p>To get that all to work we need a way to make good divisions, rather than bad ones, and a way to intersect a ray with a bounding volume. A ray bounding volume intersection needs to be fast, and bounding volumes need to be pretty compact. In practice for most models, axis-aligned boxes work better than the alternatives, but this design choice is always something to keep in mind if you encounter unusual types of models.</p> <p>From now on we will call axis-aligned bounding rectangular parallelepiped (really, that is what they need to be called if precise) axis-aligned bounding boxes, or AABBs. Any method you want to use to intersect a ray with an AABB is fine. And all we need to know is whether or not we hit it; we don\u2019t need hit points or normals or any of the stuff we need to display the object.</p> <p>Most people use the \u201cslab\u201d method. This is based on the observation that an n-dimensional AABB is just the intersection of $n$ axis-aligned intervals, often called \u201cslabs\u201d. An interval is just the points between two endpoints, e.g., $x$ such that $3 &lt; x &lt; 5$, or more succinctly $x$ in $(3,5)$. In 2D, two intervals overlapping makes a 2D AABB (a rectangle):</p> <p></p> <p>For a ray to hit one interval we first need to figure out whether the ray hits the boundaries. For example, again in 2D, this is the ray parameters $t_0$ and $t_1$. (If the ray is parallel to the plane, its intersection with the plane will be undefined.)</p> <p></p> <p>In 3D, those boundaries are planes. The equations for the planes are $x = x_0$ and $x = x_1$. Where does the ray hit that plane? Recall that the ray can be thought of as just a function that given a $t$ returns a location $\\mathbf{P}(t)$:</p> <p>$$ \\mathbf{P}(t) = \\mathbf{A} + t \\mathbf{b} $$</p> <p>This equation applies to all three of the x/y/z coordinates. For example, $x(t) = A_x + t b_x$. This ray hits the plane $x = x_0$ at the parameter $t$ that satisfies this equation:</p> <p>$$ x_0 = A_x + t_0 b_x $$</p> <p>Thus $t$ at that hitpoint is:</p> <p>$$ t_0 = \\frac{x_0 - A_x}{b_x} $$</p> <p>We get the similar expression for $x_1$:</p> <p>$$ t_1 = \\frac{x_1 - A_x}{b_x} $$</p> <p>The key observation to turn that 1D math into a hit test is that for a hit, the $t$-intervals need to overlap. For example, in 2D the green and blue overlapping only happens if there is a hit:</p> <p></p>"},{"location":"the_next_week/bounding_volume_hierarchies/#ray-intersection-with-an-aabb","title":"Ray Intersection with an AABB","text":"<p>The following pseudocode determines whether the $t$ intervals in the slab overlap:</p> <pre><code>compute (tx0, tx1)\ncompute (ty0, ty1)\nreturn overlap?( (tx0, tx1), (ty0, ty1))\n</code></pre> <p>That is awesomely simple, and the fact that the 3D version also works is why people love the slab method:</p> <pre><code>compute (tx0, tx1)\ncompute (ty0, ty1)\ncompute (tz0, tz1)\nreturn overlap ? ((tx0, tx1), (ty0, ty1), (tz0, tz1))\n</code></pre> <p>There are some caveats that make this less pretty than it first appears. First, suppose the ray is travelling in the negative $\\mathbf{x}$ direction. The interval $(t_{x0}, t_{x1})$ as computed above might be reversed, e.g. something like $(7, 3)$. Second, the divide in there could give us infinities. And if the ray origin is on one of the slab boundaries, we can get a <code>NaN</code>. There are many ways these issues are dealt with in various ray tracers\u2019 AABB. (There are also vectorization issues like SIMD which we will not discuss here. Ingo Wald\u2019s papers are a great place to start if you want to go the extra mile in vectorization for speed.) For our purposes, this is unlikely to be a major bottleneck as long as we make it reasonably fast, so let\u2019s go for simplest, which is often fastest anyway! First let\u2019s look at computing the intervals:</p> <p>$$ t_{x0} = \\frac{x_0 - A_x}{b_x} $$   $$ t_{x1} = \\frac{x_1 - A_x}{b_x} $$</p> <p>One troublesome thing is that perfectly valid rays will have $b_x = 0$, causing division by zero. Some of those rays are inside the slab, and some are not. Also, the zero will have a \u00b1 sign when using IEEE floating point. The good news for $b_x = 0$ is that $t_{x0}$ and $t_{x1}$ will both be +\u221e or both be -\u221e if not between $x_0$ and $x_1$. So, using min and max should get us the right answers:</p> <p>$$ t_{x0} = \\min(      \\frac{x_0 - A_x}{b_x},      \\frac{x_1 - A_x}{b_x})   $$</p> <p>$$ t_{x1} = \\max(      \\frac{x_0 - A_x}{b_x},      \\frac{x_1 - A_x}{b_x})   $$</p> <p>The remaining troublesome case if we do that is if $b_x = 0$ and either $x_0 - A_x = 0$ or $x_1 - A_x = 0$ so we get a <code>NaN</code>. In that case we can probably accept either hit or no hit answer, but we\u2019ll revisit that later.</p> <p>Now, let\u2019s look at that overlap function. Suppose we can assume the intervals are not reversed (so the first value is less than the second value in the interval) and we want to return true in that case. The boolean overlap that also computes the overlap interval $(f, F)$ of intervals $(d, D)$ and $(e, E)$ would be:</p> <pre><code>bool overlap(d, D, e, E, f, F)\nf = max(d, e)\nF = min(D, E)\nreturn (f &lt; F)\n</code></pre> <p>If there are any <code>NaN</code>s running around there, the compare will return false so we need to be sure our bounding boxes have a little padding if we care about grazing cases (and we probably should because in a ray tracer all cases come up eventually). Here's the implementation:</p> interval::expand() method<pre><code>class interval {\npublic:\n...\ndouble size() const {\nreturn max - min;\n}\ninterval expand(double delta) const {\nauto padding = delta/2;\nreturn interval(min - padding, max + padding);\n}\n...\n};\n</code></pre> Axis-aligned bounding box class<pre><code>#ifndef AABB_H\n#define AABB_H\n#include \"rtweekend.h\"\nclass aabb {\npublic:\ninterval x, y, z;\naabb() {} // The default AABB is empty, since intervals are empty by default.\naabb(const interval&amp; ix, const interval&amp; iy, const interval&amp; iz)\n: x(ix), y(iy), z(iz) { }\naabb(const point3&amp; a, const point3&amp; b) {\n// Treat the two points a and b as extrema for the bounding box, so we don't require a\n// particular minimum/maximum coordinate order.\nx = interval(fmin(a[0],b[0]), fmax(a[0],b[0]));\ny = interval(fmin(a[1],b[1]), fmax(a[1],b[1]));\nz = interval(fmin(a[2],b[2]), fmax(a[2],b[2]));\n}\nconst interval&amp; axis(int n) const {\nif (n == 1) return y;\nif (n == 2) return z;\nreturn x;\n}\nbool hit(const ray&amp; r, interval ray_t) const {\nfor (int a = 0; a &lt; 3; a++) {\nauto t0 = fmin((axis(a).min - r.origin()[a]) / r.direction()[a],\n(axis(a).max - r.origin()[a]) / r.direction()[a]);\nauto t1 = fmax((axis(a).min - r.origin()[a]) / r.direction()[a],\n(axis(a).max - r.origin()[a]) / r.direction()[a]);\nray_t.min = fmax(t0, ray_t.min);\nray_t.max = fmin(t1, ray_t.max);\nif (ray_t.max &lt;= ray_t.min)\nreturn false;\n}\nreturn true;\n}\n};\n#endif\n</code></pre>"},{"location":"the_next_week/bounding_volume_hierarchies/#an-optimized-aabb-hit-method","title":"An Optimized AABB Hit Method","text":"<p>In reviewing this intersection method, Andrew Kensler at Pixar tried some experiments and proposed the following version of the code. It works extremely well on many compilers, and I have adopted it as my go-to method:</p> Axis-aligned bounding box hit function<pre><code>class aabb {\npublic:\n...\nbool hit(const ray&amp; r, interval ray_t) const {\nfor (int a = 0; a &lt; 3; a++) {\nauto invD = 1 / r.direction()[a];\nauto orig = r.origin()[a];\nauto t0 = (axis(a).min - orig) * invD;\nauto t1 = (axis(a).max - orig) * invD;\nif (invD &lt; 0)\nstd::swap(t0, t1);\nif (t0 &gt; ray_t.min) ray_t.min = t0;\nif (t1 &lt; ray_t.max) ray_t.max = t1;\nif (ray_t.max &lt;= ray_t.min)\nreturn false;\n}\nreturn true;\n}\n...\n};\n</code></pre>"},{"location":"the_next_week/bounding_volume_hierarchies/#constructing-bounding-boxes-for-hittables","title":"Constructing Bounding Boxes for Hittables","text":"<p>We now need to add a function to compute the bounding boxes of all the hittables. Then we will make a hierarchy of boxes over all the primitives, and the individual primitives--like spheres--will live at the leaves.</p> <p>Recall that <code>interval</code> values constructed without arguments will be empty by default. Since an <code>aabb</code> object has an interval for each of its three dimensions, each of these will then be empty by default, and therefore <code>aabb</code> objects will be empty by default. Thus, some objects may have empty bounding volumes. For example, consider a <code>hittable_list</code> object with no children. Happily, the way we've designed our interval class, the math all works out.</p> <p>Finally, recall that some objects may be animated. Such objects should return their bounds over the entire range of motion, from time=0 to time=1.</p> Hittable class with bounding-box<pre><code>#include \"aabb.h\"\n...\nclass hittable {\npublic:\n...\nvirtual bool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const = 0;\nvirtual aabb bounding_box() const = 0;\n...\n};\n</code></pre> <p>For a stationary sphere, the <code>bounding_box</code> function is easy:</p> Sphere with bounding box<pre><code>class sphere : public hittable {\npublic:\n// Stationary Sphere\nsphere(point3 _center, double _radius, shared_ptr&lt;material&gt; _material)\n: center1(_center), radius(_radius), mat(_material), is_moving(false)\n{\nauto rvec = vec3(radius, radius, radius);\nbbox = aabb(center1 - rvec, center1 + rvec);\n}\n...\naabb bounding_box() const override { return bbox; }\nprivate:\npoint3 center1;\ndouble radius;\nshared_ptr&lt;material&gt; mat;\nbool is_moving;\nvec3 center_vec;\naabb bbox;\n...\n};\n</code></pre> <p>For a moving sphere, we want the bounds of its entire range of motion. To do this, we can take the box of the sphere at time=0, and the box of the sphere at time=1, and compute the box around those two boxes.</p> Moving sphere with bounding box<pre><code>class sphere : public hittable {\npublic:\n...\n// Moving Sphere\nsphere(point3 _center1, point3 _center2, double _radius, shared_ptr&lt;material&gt; _material)\n: center1(_center1), radius(_radius), mat(_material), is_moving(true)\n{\nauto rvec = vec3(radius, radius, radius);\naabb box1(_center1 - rvec, _center1 + rvec);\naabb box2(_center2 - rvec, _center2 + rvec);\nbbox = aabb(box1, box2);\ncenter_vec = _center2 - _center1;\n}\n...\n};\n</code></pre> <p>Now we need a new <code>aabb</code> constructor that takes two boxes as input. First, we'll add a new interval constructor that takes two intervals as input:</p> Interval constructor from two intervals<pre><code>class interval {\npublic:\n...\ninterval(const interval&amp; a, const interval&amp; b)\n: min(fmin(a.min, b.min)), max(fmax(a.max, b.max)) {}\n</code></pre> <p>Now we can use this to construct an axis-aligned bounding box from two input boxes.</p> AABB constructor from two AABB inputs<pre><code>class aabb {\npublic:\n...\naabb(const aabb&amp; box0, const aabb&amp; box1) {\nx = interval(box0.x, box1.x);\ny = interval(box0.y, box1.y);\nz = interval(box0.z, box1.z);\n}\n...\n};\n</code></pre>"},{"location":"the_next_week/bounding_volume_hierarchies/#creating-bounding-boxes-of-lists-of-objects","title":"Creating Bounding Boxes of Lists of Objects","text":"<p>Now we'll update the <code>hittable_list</code> object, computing the bounds of its children. We'll update the bounding box incrementally as each new child is added.</p> Hittable list with bounding box<pre><code>...\n#include \"aabb.h\"\n...\nclass hittable_list : public hittable {\npublic:\nstd::vector&lt;shared_ptr&lt;hittable&gt;&gt; objects;\n...\nvoid add(shared_ptr&lt;hittable&gt; object) {\nobjects.push_back(object);\nbbox = aabb(bbox, object-&gt;bounding_box());\n}\nbool hit(const ray&amp; r, double ray_tmin, double ray_tmax, hit_record&amp; rec) const override {\n...\n}\naabb bounding_box() const override { return bbox; }\nprivate:\naabb bbox;\n};\n</code></pre>"},{"location":"the_next_week/bounding_volume_hierarchies/#the-bvh-node-class","title":"The BVH Node Class","text":"<p>A BVH is also going to be a <code>hittable</code> -- just like lists of <code>hittable</code>s. It\u2019s really a container, but it can respond to the query \u201cdoes this ray hit you?\u201d. One design question is whether we have two classes, one for the tree, and one for the nodes in the tree; or do we have just one class and have the root just be a node we point to. The <code>hit</code> function is pretty straightforward: check whether the box for the node is hit, and if so, check the children and sort out any details.</p> <p>I am a fan of the one class design when feasible. Here is such a class:</p> Bounding volume hierarchy<pre><code>#ifndef BVH_H\n#define BVH_H\n#include \"rtweekend.h\"\n#include \"hittable.h\"\n#include \"hittable_list.h\"\nclass bvh_node : public hittable {\npublic:\nbvh_node(const hittable_list&amp; list) : bvh_node(list.objects, 0, list.objects.size()) {}\nbvh_node(const std::vector&lt;shared_ptr&lt;hittable&gt;&gt;&amp; src_objects, size_t start, size_t end) {\n// To be implemented later.\n}\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\nif (!box.hit(r, ray_t))\nreturn false;\nbool hit_left = left-&gt;hit(r, ray_t, rec);\nbool hit_right = right-&gt;hit(r, interval(ray_t.min, hit_left ? rec.t : ray_t.max), rec);\nreturn hit_left || hit_right;\n}\naabb bounding_box() const override { return bbox; }\nprivate:\nshared_ptr&lt;hittable&gt; left;\nshared_ptr&lt;hittable&gt; right;\naabb bbox;\n};\n#endif\n</code></pre>"},{"location":"the_next_week/bounding_volume_hierarchies/#splitting-bvh-volumes","title":"Splitting BVH Volumes","text":"<p>The most complicated part of any efficiency structure, including the BVH, is building it. We do this in the constructor. A cool thing about BVHs is that as long as the list of objects in a <code>bvh_node</code> gets divided into two sub-lists, the hit function will work. It will work best if the division is done well, so that the two children have smaller bounding boxes than their parent\u2019s bounding box, but that is for speed not correctness. I\u2019ll choose the middle ground, and at each node split the list along one axis. I\u2019ll go for simplicity:</p> <ol> <li>randomly choose an axis</li> <li>sort the primitives (<code>using std::sort</code>)</li> <li>put half in each subtree</li> </ol> <p>When the list coming in is two elements, I put one in each subtree and end the recursion. The traversal algorithm should be smooth and not have to check for null pointers, so if I just have one element I duplicate it in each subtree. Checking explicitly for three elements and just following one recursion would probably help a little, but I figure the whole method will get optimized later. The following code uses three methods--<code>box_x_compare</code>, <code>box_y_compare_</code>, and <code>box_z_compare</code>--that we haven't yet defined.</p> Bounding volume hierarchy node<pre><code>#include &lt;algorithm&gt;\nclass bvh_node : public hittable {\npublic:\n...\nbvh_node(const std::vector&lt;shared_ptr&lt;hittable&gt;&gt;&amp; src_objects, size_t start, size_t end) {\nauto objects = src_objects; // Create a modifiable array of the source scene objects\nint axis = random_int(0,2);\nauto comparator = (axis == 0) ? box_x_compare\n: (axis == 1) ? box_y_compare\n: box_z_compare;\nsize_t object_span = end - start;\nif (object_span == 1) {\nleft = right = objects[start];\n} else if (object_span == 2) {\nif (comparator(objects[start], objects[start+1])) {\nleft = objects[start];\nright = objects[start+1];\n} else {\nleft = objects[start+1];\nright = objects[start];\n}\n} else {\nstd::sort(objects.begin() + start, objects.begin() + end, comparator);\nauto mid = start + object_span/2;\nleft = make_shared&lt;bvh_node&gt;(objects, start, mid);\nright = make_shared&lt;bvh_node&gt;(objects, mid, end);\n}\nbbox = aabb(left-&gt;bounding_box(), right-&gt;bounding_box());\n}\n...\n};\n</code></pre> <p>This uses a new function: <code>random_int()</code>:</p> A function to return random integers in a range<pre><code>inline int random_int(int min, int max) {\n// Returns a random integer in [min,max].\nreturn static_cast&lt;int&gt;(random_double(min, max+1));\n}\n</code></pre> <p>The check for whether there is a bounding box at all is in case you sent in something like an infinite plane that doesn\u2019t have a bounding box. We don\u2019t have any of those primitives, so it shouldn\u2019t happen until you add such a thing.</p>"},{"location":"the_next_week/bounding_volume_hierarchies/#the-box-comparison-functions","title":"The Box Comparison Functions","text":"<p>Now we need to implement the box comparison functions, used by <code>std::sort()</code>. To do this, create a generic comparator returns true if the first argument is less than the second, given an additional axis index argument. Then define axis-specific comparison functions that use the generic comparison function.</p> BVH comparison function, X-axis<pre><code>class bvh_node : public hittable {\n...\nprivate:\n...\nstatic bool box_compare(\nconst shared_ptr&lt;hittable&gt; a, const shared_ptr&lt;hittable&gt; b, int axis_index\n) {\nreturn a-&gt;bounding_box().axis(axis_index).min &lt; b-&gt;bounding_box().axis(axis_index).min;\n}\nstatic bool box_x_compare (const shared_ptr&lt;hittable&gt; a, const shared_ptr&lt;hittable&gt; b) {\nreturn box_compare(a, b, 0);\n}\nstatic bool box_y_compare (const shared_ptr&lt;hittable&gt; a, const shared_ptr&lt;hittable&gt; b) {\nreturn box_compare(a, b, 1);\n}\nstatic bool box_z_compare (const shared_ptr&lt;hittable&gt; a, const shared_ptr&lt;hittable&gt; b) {\nreturn box_compare(a, b, 2);\n}\n};\n</code></pre> <p>At this point, we're ready to use our new BVH code. Let's use it on our random spheres scene.</p> Random spheres, using BVH<pre><code>...\nint main() {\n...\nauto material2 = make_shared&lt;lambertian&gt;(color(0.4, 0.2, 0.1));\nworld.add(make_shared&lt;sphere&gt;(point3(-4, 1, 0), 1.0, material2));\nauto material3 = make_shared&lt;metal&gt;(color(0.7, 0.6, 0.5), 0.0);\nworld.add(make_shared&lt;sphere&gt;(point3(4, 1, 0), 1.0, material3));\nworld = hittable_list(make_shared&lt;bvh_node&gt;(world));\ncamera cam;\n...\n}\n</code></pre>"},{"location":"the_next_week/instances/","title":"Instances","text":"<p>The Cornell Box usually has two blocks in it. These are rotated relative to the walls. First, let\u2019s create a function that returns a box, by creating a <code>hittable_list</code> of six rectangles:</p> A box object<pre><code>...\n#include \"hittable_list.h\"\n...\ninline shared_ptr&lt;hittable_list&gt; box(const point3&amp; a, const point3&amp; b, shared_ptr&lt;material&gt; mat)\n{\n// Returns the 3D box (six sides) that contains the two opposite vertices a &amp; b.\nauto sides = make_shared&lt;hittable_list&gt;();\n// Construct the two opposite vertices with the minimum and maximum coordinates.\nauto min = point3(fmin(a.x(), b.x()), fmin(a.y(), b.y()), fmin(a.z(), b.z()));\nauto max = point3(fmax(a.x(), b.x()), fmax(a.y(), b.y()), fmax(a.z(), b.z()));\nauto dx = vec3(max.x() - min.x(), 0, 0);\nauto dy = vec3(0, max.y() - min.y(), 0);\nauto dz = vec3(0, 0, max.z() - min.z());\nsides-&gt;add(make_shared&lt;quad&gt;(point3(min.x(), min.y(), max.z()),  dx,  dy, mat)); // front\nsides-&gt;add(make_shared&lt;quad&gt;(point3(max.x(), min.y(), max.z()), -dz,  dy, mat)); // right\nsides-&gt;add(make_shared&lt;quad&gt;(point3(max.x(), min.y(), min.z()), -dx,  dy, mat)); // back\nsides-&gt;add(make_shared&lt;quad&gt;(point3(min.x(), min.y(), min.z()),  dz,  dy, mat)); // left\nsides-&gt;add(make_shared&lt;quad&gt;(point3(min.x(), max.y(), max.z()),  dx, -dz, mat)); // top\nsides-&gt;add(make_shared&lt;quad&gt;(point3(min.x(), min.y(), min.z()),  dx,  dz, mat)); // bottom\nreturn sides;\n}\n</code></pre> <p>Now we can add two blocks (but not rotated).</p> Adding box objects<pre><code>void cornell_box() {\n...\nworld.add(make_shared&lt;quad&gt;(point3(555,0,0), vec3(0,555,0), vec3(0,0,555), green));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,0), vec3(0,555,0), vec3(0,0,555), red));\nworld.add(make_shared&lt;quad&gt;(point3(343, 554, 332), vec3(-130,0,0), vec3(0,0,-105), light));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,0), vec3(555,0,0), vec3(0,0,555), white));\nworld.add(make_shared&lt;quad&gt;(point3(555,555,555), vec3(-555,0,0), vec3(0,0,-555), white));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,555), vec3(555,0,0), vec3(0,555,0), white));\nworld.add(box(point3(130, 0, 65), point3(295, 165, 230), white));\nworld.add(box(point3(265, 0, 295), point3(430, 330, 460), white));\ncamera cam;\n...\n}\n</code></pre> <p>This gives:</p> <p></p> <p>Now that we have boxes, we need to rotate them a bit to have them match the real Cornell box. In ray tracing, this is usually done with an instance. An instance is a copy of a geometric primitive that has been placed into the scene. This instance is entirely independent of the other copies of the primitive and can be moved or rotated. In this case, our geometric primitive is our hittable <code>box</code> object, and we want to rotate it. This is especially easy in ray tracing because we don\u2019t actually need to move objects in the scene; instead we move the rays in the opposite direction. For example, consider a translation (often called a move). We could take the pink box at the origin and add two to all its x components, or (as we almost always do in ray tracing) leave the box where it is, but in its hit routine subtract two off the x-component of the ray origin.</p> <p></p>"},{"location":"the_next_week/instances/#instance-translation","title":"Instance Translation","text":"<p>Whether you think of this as a move or a change of coordinates is up to you. The way to reason about this is to think of moving the incident ray backwards the offset amount, determining if an intersection occurs, and then moving that intersection point forward the offset amount.</p> <p>We need to move the intersection point forward the offset amount so that the intersection is actually in the path of the incident ray. If we forgot to move the intersection point forward then the intersection would be in the path of the offset ray, which isn't correct. Let's add the code to make this happen.</p> Hittable translation hit function<pre><code>class translate : public hittable {\npublic:\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n// Move the ray backwards by the offset\nray offset_r(r.origin() - offset, r.direction(), r.time());\n// Determine where (if any) an intersection occurs along the offset ray\nif (!object-&gt;hit(offset_r, ray_t, rec))\nreturn false;\n// Move the intersection point forwards by the offset\nrec.p += offset;\nreturn true;\n}\nprivate:\nshared_ptr&lt;hittable&gt; object;\nvec3 offset;\n};\n</code></pre> <p>... and then flesh out the rest of the <code>translate</code> class:</p> Hittable translation class<pre><code>class translate : public hittable {\npublic:\ntranslate(shared_ptr&lt;hittable&gt; p, const vec3&amp; displacement)\n: object(p), offset(displacement)\n{\nbbox = object-&gt;bounding_box() + offset;\n}\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n// Move the ray backwards by the offset\nray offset_r(r.origin() - offset, r.direction(), r.time());\n// Determine where (if any) an intersection occurs along the offset ray\nif (!object-&gt;hit(offset_r, ray_t, rec))\nreturn false;\n// Move the intersection point forwards by the offset\nrec.p += offset;\nreturn true;\n}\naabb bounding_box() const override { return bbox; }\nprivate:\nshared_ptr&lt;hittable&gt; object;\nvec3 offset;\naabb bbox;\n};\n</code></pre> <p>We also need to remember to offset the bounding box, otherwise the incident ray might be looking in   the wrong place and trivially reject the intersection. The expression <code>object-&gt;bounding_box() + offset</code> above requires some additional support.</p> The aabb + offset operator<pre><code>class aabb {\n...\n};\naabb operator+(const aabb&amp; bbox, const vec3&amp; offset) {\nreturn aabb(bbox.x + offset.x(), bbox.y + offset.y(), bbox.z + offset.z());\n}\naabb operator+(const vec3&amp; offset, const aabb&amp; bbox) {\nreturn bbox + offset;\n}\n</code></pre> <p>Since each dimension of an <code>aabb</code> is represented as an interval, we'll need to extend <code>interval</code> with an addition operator as well.</p> The interval + displacement operator<pre><code>class interval {\n...\n};\nconst interval interval::empty    = interval(+infinity, -infinity);\nconst interval interval::universe = interval(-infinity, +infinity);\ninterval operator+(const interval&amp; ival, double displacement) {\nreturn interval(ival.min + displacement, ival.max + displacement);\n}\ninterval operator+(double displacement, const interval&amp; ival) {\nreturn ival + displacement;\n}\n</code></pre>"},{"location":"the_next_week/instances/#instance-rotation","title":"Instance Rotation","text":"<p>Rotation isn\u2019t quite as easy to understand or generate the formulas for. A common graphics tactic is to apply all rotations about the x, y, and z axes. These rotations are in some sense axis-aligned. First, let\u2019s rotate by theta about the z-axis. That will be changing only x and y, and in ways that don\u2019t depend on z.</p> <p></p> <p>This involves some basic trigonometry that uses formulas that I will not cover here. That gives you the correct impression it\u2019s a little involved, but it is straightforward, and you can find it in any graphics text and in many lecture notes. The result for rotating counter-clockwise about z is:</p> <p>$$ x' = \\cos(\\theta) \\cdot x - \\sin(\\theta) \\cdot y $$   $$ y' = \\sin(\\theta) \\cdot x + \\cos(\\theta) \\cdot y $$</p> <p>The great thing is that it works for any $\\theta$ and doesn\u2019t need any cases for quadrants or anything like that. The inverse transform is the opposite geometric operation: rotate by $-\\theta$. Here, recall that $\\cos(\\theta) = \\cos(-\\theta)$ and $\\sin(-\\theta) = -\\sin(\\theta)$, so the formulas are very simple.</p> <p>Similarly, for rotating about y (as we want to do for the blocks in the box) the formulas are:</p> <p>$$ x' =  \\cos(\\theta) \\cdot x + \\sin(\\theta) \\cdot z $$   $$ z' = -\\sin(\\theta) \\cdot x + \\cos(\\theta) \\cdot z $$</p> <p>And if we want to rotate about the x-axis:</p> <p>$$ y' = \\cos(\\theta) \\cdot y - \\sin(\\theta) \\cdot z $$   $$ z' = \\sin(\\theta) \\cdot y + \\cos(\\theta) \\cdot z $$</p> <p>Thinking of translation as a simple movement of the initial ray is a fine way to reason about what's going on. But, for a more complex operation like a rotation, it can be easy to accidentally get your terms crossed (or forget a negative sign), so it's better to consider a rotation as a change of coordinates.</p> <p>The pseudocode for the <code>translate::hit</code> function above describes the function in terms of moving:</p> <ol> <li>Move the ray backwards by the offset</li> <li>Determine whether an intersection exists along the offset ray (and if so, where)</li> <li>Move the intersection point forwards by the offset</li> </ol> <p>But this can also be thought of in terms of a changing of coordinates:</p> <ol> <li>Change the ray from world space to object space</li> <li>Determine whether an intersection exists in object space (and if so, where)</li> <li>Change the intersection point from object space to world space</li> </ol> <p>Rotating an object will not only change the point of intersection, but will also change the surface normal vector, which will change the direction of reflections and refractions. So we need to change the normal as well. Fortunately, the normal will rotate similarly to a vector, so we can use the same formulas as above.  While normals and vectors may appear identical for an object undergoing rotation and translation, an object undergoing scaling requires special attention to keep the normals orthogonal to the surface. We won't cover that here, but you should research surface normal transformations if you implement scaling.</p> <p>We need to start by changing the ray from world space to object space, which for rotation means rotating by $-\\theta$.</p> <p>$$ x' = \\cos(\\theta) \\cdot x - \\sin(\\theta) \\cdot z $$   $$ z' = \\sin(\\theta) \\cdot x + \\cos(\\theta) \\cdot z $$</p> <p>We can now create a class for y-rotation:</p> Hittable rotate-Y hit function<pre><code>class rotate_y : public hittable {\npublic:\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n// Change the ray from world space to object space\nauto origin = r.origin();\nauto direction = r.direction();\norigin[0] = cos_theta*r.origin()[0] - sin_theta*r.origin()[2];\norigin[2] = sin_theta*r.origin()[0] + cos_theta*r.origin()[2];\ndirection[0] = cos_theta*r.direction()[0] - sin_theta*r.direction()[2];\ndirection[2] = sin_theta*r.direction()[0] + cos_theta*r.direction()[2];\nray rotated_r(origin, direction, r.time());\n// Determine where (if any) an intersection occurs in object space\nif (!object-&gt;hit(rotated_r, ray_t, rec))\nreturn false;\n// Change the intersection point from object space to world space\nauto p = rec.p;\np[0] =  cos_theta*rec.p[0] + sin_theta*rec.p[2];\np[2] = -sin_theta*rec.p[0] + cos_theta*rec.p[2];\n// Change the normal from object space to world space\nauto normal = rec.normal\nnormal[0] =  cos_theta*rec.normal[0] + sin_theta*rec.normal[2];\nnormal[2] = -sin_theta*rec.normal[0] + cos_theta*rec.normal[2];\nrec.p = p;\nrec.normal = normal;\nreturn true;\n}\n};\n</code></pre> <p>... and now for the rest of the class:</p> Hittable rotate-Y class<pre><code>class rotate_y : public hittable {\npublic:\nrotate_y(shared_ptr&lt;hittable&gt; p, double angle) : object(p) {\nauto radians = degrees_to_radians(angle);\nsin_theta = sin(radians);\ncos_theta = cos(radians);\nbbox = object-&gt;bounding_box();\npoint3 min( infinity,  infinity,  infinity);\npoint3 max(-infinity, -infinity, -infinity);\nfor (int i = 0; i &lt; 2; i++) {\nfor (int j = 0; j &lt; 2; j++) {\nfor (int k = 0; k &lt; 2; k++) {\nauto x = i*bbox.x.max + (1-i)*bbox.x.min;\nauto y = j*bbox.y.max + (1-j)*bbox.y.min;\nauto z = k*bbox.z.max + (1-k)*bbox.z.min;\nauto newx =  cos_theta*x + sin_theta*z;\nauto newz = -sin_theta*x + cos_theta*z;\nvec3 tester(newx, y, newz);\nfor (int c = 0; c &lt; 3; c++) {\nmin[c] = fmin(min[c], tester[c]);\nmax[c] = fmax(max[c], tester[c]);\n}\n}\n}\n}\nbbox = aabb(min, max);\n}\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n...\n}\naabb bounding_box() const override { return bbox; }\nprivate:\nshared_ptr&lt;hittable&gt; object;\ndouble sin_theta;\ndouble cos_theta;\naabb bbox;\n};\n</code></pre> <p>And the changes to Cornell are:</p> Cornell scene with Y-rotated boxes<pre><code>void cornell_box() {\n...\nworld.add(make_shared&lt;quad&gt;(point3(0,0,555), vec3(555,0,0), vec3(0,555,0), white));\nshared_ptr&lt;hittable&gt; box1 = box(point3(0,0,0), point3(165,330,165), white);\nbox1 = make_shared&lt;rotate_y&gt;(box1, 15);\nbox1 = make_shared&lt;translate&gt;(box1, vec3(265,0,295));\nworld.add(box1);\nshared_ptr&lt;hittable&gt; box2 = box(point3(0,0,0), point3(165,165,165), white);\nbox2 = make_shared&lt;rotate_y&gt;(box2, -18);\nbox2 = make_shared&lt;translate&gt;(box2, vec3(130,0,65));\nworld.add(box2);\ncamera cam;\n...\n}\n</code></pre> <p>Which yields:</p> <p></p>"},{"location":"the_next_week/lights/","title":"Lights","text":"<p>Lighting is a key component of raytracing. Early simple raytracers used abstract light sources, like points in space, or directions. Modern approaches have more physically based lights, which have position and size. To create such light sources, we need to be able to take any regular object and turn it into something that emits light into our scene.</p>"},{"location":"the_next_week/lights/#emissive-materials","title":"Emissive Materials","text":"<p>First, let\u2019s make a light emitting material. We need to add an emitted function (we could also add it to <code>hit_record</code> instead -- that\u2019s a matter of design taste). Like the background, it just tells the ray what color it is and performs no reflection. It\u2019s very simple:</p> A diffuse light class<pre><code>class diffuse_light : public material {\npublic:\ndiffuse_light(shared_ptr&lt;texture&gt; a) : emit(a) {}\ndiffuse_light(color c) : emit(make_shared&lt;solid_color&gt;(c)) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nreturn false;\n}\ncolor emitted(double u, double v, const point3&amp; p) const override {\nreturn emit-&gt;value(u, v, p);\n}\nprivate:\nshared_ptr&lt;texture&gt; emit;\n};\n</code></pre> <p>So that I don\u2019t have to make all the non-emitting materials implement <code>emitted()</code>, I have the base class return black:</p> New emitted function in class material<pre><code>class material {\npublic:\n...\nvirtual color emitted(double u, double v, const point3&amp; p) const {\nreturn color(0,0,0);\n}\nvirtual bool scatter(\nconst ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered\n) const = 0;\n};\n</code></pre>"},{"location":"the_next_week/lights/#adding-background-color-to-the-ray-color-function","title":"Adding Background Color to the Ray Color Function","text":"<p>Next, we want a pure black background so the only light in the scene is coming from the emitters. To do this, we\u2019ll add a background color parameter to our <code>ray_color</code> function, and pay attention to the new <code>color_from_emission</code> value.</p> ray_color function with background and emitting materials<pre><code>class camera {\npublic:\ndouble aspect_ratio      = 1.0;  // Ratio of image width over height\nint    image_width       = 100;  // Rendered image width in pixel count\nint    samples_per_pixel = 10;   // Count of random samples for each pixel\nint    max_depth         = 10;   // Maximum number of ray bounces into scene\ncolor  background;               // Scene background color\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\n// If the ray hits nothing, return the background color.\nif (!world.hit(r, interval(0.001, infinity), rec))\nreturn background;\nray scattered;\ncolor attenuation;\ncolor color_from_emission = rec.mat-&gt;emitted(rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered))\nreturn color_from_emission;\ncolor color_from_scatter = attenuation * ray_color(scattered, depth-1, world);\nreturn color_from_emission + color_from_scatter;\n}\n};\n</code></pre> <p><code>main()</code> is updated to set the background color for the prior scenes:</p> Specifying new background color<pre><code>void random_spheres() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.background        = color(0.70, 0.80, 1.00);\n...\n}\nvoid two_spheres() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.background        = color(0.70, 0.80, 1.00);\n...\n}\nvoid earth() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.background        = color(0.70, 0.80, 1.00);\n...\n}\nvoid two_perlin_spheres() {\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.background        = color(0.70, 0.80, 1.00);\n...\n}\nvoid quads() {\n...\ncamera cam;\ncam.aspect_ratio      = 1.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.background        = color(0.70, 0.80, 1.00);\n...\n}\n</code></pre> <p>Since we're removing the code that we used to determine the color of the sky when a ray hit it, we need to pass in a new color value for our old scene renders. We've elected to stick with a flat bluish-white for the whole sky. You could always pass in a boolean to switch between the previous skybox code versus the new solid color background. We're keeping it simple here.</p>"},{"location":"the_next_week/lights/#turning-objects-into-lights","title":"Turning Objects into Lights","text":"<p>If we set up a rectangle as a light:</p> A simple rectangle light<pre><code>void simple_light() {\nhittable_list world;\nauto pertext = make_shared&lt;noise_texture&gt;(4);\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, make_shared&lt;lambertian&gt;(pertext)));\nworld.add(make_shared&lt;sphere&gt;(point3(0,2,0), 2, make_shared&lt;lambertian&gt;(pertext)));\nauto difflight = make_shared&lt;diffuse_light&gt;(color(4,4,4));\nworld.add(make_shared&lt;quad&gt;(point3(3,1,-2), vec3(2,0,0), vec3(0,2,0), difflight));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.background        = color(0,0,0);\ncam.vfov     = 20;\ncam.lookfrom = point3(26,3,6);\ncam.lookat   = point3(0,2,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (6) {\ncase 1:  random_spheres();     break;\ncase 2:  two_spheres();        break;\ncase 3:  earth();              break;\ncase 4:  two_perlin_spheres(); break;\ncase 5:  quads();              break;\ncase 6:  simple_light();       break;\n}\n}\n</code></pre> <p>We get:</p> <p></p> <p>Note that the light is brighter than $(1,1,1)$. This allows it to be bright enough to light things.</p> <p>Fool around with making some spheres lights too.</p> A simple rectangle light plus illuminating ball<pre><code>void simple_light() {\n...\nauto difflight = make_shared&lt;diffuse_light&gt;(color(4,4,4));\nworld.add(make_shared&lt;sphere&gt;(point3(0,7,0), 2, difflight));\nworld.add(make_shared&lt;quad&gt;(point3(3,1,-2), vec3(2,0,0), vec3(0,2,0), difflight));\n...\n}\n</code></pre> <p></p>"},{"location":"the_next_week/lights/#creating-an-empty-cornell-box","title":"Creating an Empty \u201cCornell Box\u201d","text":"<p>The \u201cCornell Box\u201d was introduced in 1984 to model the interaction of light between diffuse surfaces. Let\u2019s make the 5 walls and the light of the box:</p> Cornell box scene, empty<pre><code>void cornell_box() {\nhittable_list world;\nauto red   = make_shared&lt;lambertian&gt;(color(.65, .05, .05));\nauto white = make_shared&lt;lambertian&gt;(color(.73, .73, .73));\nauto green = make_shared&lt;lambertian&gt;(color(.12, .45, .15));\nauto light = make_shared&lt;diffuse_light&gt;(color(15, 15, 15));\nworld.add(make_shared&lt;quad&gt;(point3(555,0,0), vec3(0,555,0), vec3(0,0,555), green));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,0), vec3(0,555,0), vec3(0,0,555), red));\nworld.add(make_shared&lt;quad&gt;(point3(343, 554, 332), vec3(-130,0,0), vec3(0,0,-105), light));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,0), vec3(555,0,0), vec3(0,0,555), white));\nworld.add(make_shared&lt;quad&gt;(point3(555,555,555), vec3(-555,0,0), vec3(0,0,-555), white));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,555), vec3(555,0,0), vec3(0,555,0), white));\ncamera cam;\ncam.aspect_ratio      = 1.0;\ncam.image_width       = 600;\ncam.samples_per_pixel = 200;\ncam.max_depth         = 50;\ncam.background        = color(0,0,0);\ncam.vfov     = 40;\ncam.lookfrom = point3(278, 278, -800);\ncam.lookat   = point3(278, 278, 0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (7) {\ncase 1:  random_spheres();     break;\ncase 2:  two_spheres();        break;\ncase 3:  earth();              break;\ncase 4:  two_perlin_spheres(); break;\ncase 5:  quads();              break;\ncase 6:  simple_light();       break;\ncase 7:  cornell_box();        break;\n}\n}\n</code></pre> <p>We get:</p> <p></p> <p>This image is very noisy because the light is small.</p>"},{"location":"the_next_week/motion_blur/","title":"Motion Blur","text":"<p>When you decided to ray trace, you decided that visual quality was worth more than run-time. When rendering fuzzy reflection and defocus blur, we used multiple samples per pixel. Once you have taken a step down that road, the good news is that almost all effects can be similarly brute-forced. Motion blur is certainly one of those.</p> <p>In a real camera, the shutter remains open for a short time interval, during which the camera and objects in the world may move. To accurately reproduce such a camera shot, we seek an average of what the camera senses while its shutter is open to the world.</p>"},{"location":"the_next_week/motion_blur/#introduction-of-spacetime-ray-tracing","title":"Introduction of SpaceTime Ray Tracing","text":"<p>We can get a random estimate of a single (simplified) photon by sending a single ray at some random instant in time while the shutter is open. As long as we can determine where the objects are supposed to be at that instant, we can get an accurate measure of the light for that ray at that same instant. This is yet another example of how random (Monte Carlo) ray tracing ends up being quite simple. Brute force wins again!</p> <p>Since the \u201cengine\u201d of the ray tracer can just make sure the objects are where they need to be for each ray, the intersection guts don\u2019t change much. To accomplish this, we need to store the exact time for each ray:</p> Ray with time information<pre><code>class ray {\npublic:\nray() {}\nray(const point3&amp; origin, const vec3&amp; direction) : orig(origin), dir(direction), tm(0)\n{}\nray(const point3&amp; origin, const vec3&amp; direction, double time = 0.0)\n: orig(origin), dir(direction), tm(time)\n{}\npoint3 origin() const  { return orig; }\nvec3 direction() const { return dir; }\ndouble time() const    { return tm; }\npoint3 at(double t) const {\nreturn orig + t*dir;\n}\nprivate:\npoint3 orig;\nvec3 dir;\ndouble tm;\n};\n</code></pre>"},{"location":"the_next_week/motion_blur/#managing-time","title":"Managing Time","text":"<p>Before continuing, let's think about time, and how we might manage it across one or more successive renders. There are two aspects of shutter timing to think about: the time from one shutter opening to the next shutter opening, and how long the shutter stays open for each frame. Standard movie film used to be shot at 24 frames per second. Modern digital movies can be 24, 30, 48, 60, 120 or however many frames per second director wants.</p> <p>Each frame can have its own shutter speed. This shutter speed need not be -- and typically isn't -- the maximum duration of the entire frame. You could have the shutter open for 1/1000th of a second every frame, or 1/60th of a second.</p> <p>If you wanted to render a sequence of images, you would need to set up the camera with the appropriate shutter timings: frame-to-frame period, shutter/render duration, and the total number of frames (total shot time). If the camera is moving and the world is static, you're good to go. However, if anything in the world is moving, you would need to add a method to <code>hittable</code> so that every object could be made aware of the current frame's time period. This method would provide a way for all animated objects to set up their motion during that frame.</p> <p>This is fairly straight-forward, and definitely a fun avenue for you to experiment with if you wish. However, for our purposes right now, we're going to proceed with a much simpler model. We will render only a single frame, implicitly assuming a start at time = 0 and ending at time = 1. Our first task is to modify the camera to launch rays with random times in $[0,1]$, and our second task will be the creation of an animated sphere class.</p>"},{"location":"the_next_week/motion_blur/#updating-the-camera-to-simulate-motion-blur","title":"Updating the Camera to Simulate Motion Blur","text":"<p>We need to modify the camera to generate rays at a random instant between the start time and the end time. Should the camera keep track of the time interval, or should that be up to the user of the camera when a ray is created? When in doubt, I like to make constructors complicated if it makes calls simple, so I will make the camera keep track, but that\u2019s a personal preference. Not many changes are needed to camera because for now it is not allowed to move; it just sends out rays over a time period.</p> Camera with time information<pre><code>class camera {\n...\nprivate:\n...\nray get_ray(int i, int j) const {\n// Get a randomly-sampled camera ray for the pixel at location i,j, originating from\n// the camera defocus disk.\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto pixel_sample = pixel_center + pixel_sample_square();\nauto ray_origin = (defocus_angle &lt;= 0) ? center : defocus_disk_sample();\nauto ray_direction = pixel_sample - ray_origin;\nauto ray_time = random_double();\nreturn ray(ray_origin, ray_direction, ray_time);\n}\n...\n};\n</code></pre>"},{"location":"the_next_week/motion_blur/#adding-moving-spheres","title":"Adding Moving Spheres","text":"<p>Now to create a moving object. I\u2019ll update the sphere class so that its center moves linearly from <code>center1</code> at time=0 to <code>center2</code> at time=1. (It continues on indefinitely outside that time interval, so it really can be sampled at any time.)</p> A moving sphere<pre><code>class sphere : public hittable {\npublic:\n// Stationary Sphere\nsphere(point3 _center, double _radius, shared_ptr&lt;material&gt; _material)\n: center1(_center), radius(_radius), mat(_material), is_moving(false) {}\n// Moving Sphere\nsphere(point3 _center1, point3 _center2, double _radius, shared_ptr&lt;material&gt; _material)\n: center1(_center1), radius(_radius), mat(_material), is_moving(true)\n{\ncenter_vec = _center2 - _center1;\n}\n...\nprivate:\npoint3 center1;\ndouble radius;\nshared_ptr&lt;material&gt; mat;\nbool is_moving;\nvec3 center_vec;\npoint3 center(double time) const {\n// Linearly interpolate from center1 to center2 according to time, where t=0 yields\n// center1, and t=1 yields center2.\nreturn center0 + time*center_vec;\n}\n};\n#endif\n</code></pre> <p>An alternative to making special stationary spheres is to just make them all move, but stationary spheres have the same begin and end position. I\u2019m on the fence about that trade-off between simpler code and more efficient stationary spheres, so let your design taste guide you.</p> <p>The updated <code>sphere::hit()</code> function is almost identical to the old <code>sphere::hit()</code> function: <code>center</code> just needs to query a function <code>sphere_center(time)</code>:</p> Moving sphere hit function<pre><code>class sphere : public hittable {\npublic:\n...\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\npoint3 center = is_moving ? sphere_center(r.time()) : center1;\nvec3 oc = r.origin() - center;\nauto a = r.direction().length_squared();\nauto half_b = dot(oc, r.direction());\nauto c = oc.length_squared() - radius*radius;\n...\n}\n...\n};\n</code></pre> <p>We need to implement the new <code>interval::contains()</code> method mentioned above:</p> interval::contains() method<pre><code>class interval {\npublic:\n...\nbool contains(double x) const {\nreturn min &lt;= x &amp;&amp; x &lt;= max;\n}\n...\n};\n</code></pre>"},{"location":"the_next_week/motion_blur/#tracking-the-time-of-ray-intersection","title":"Tracking the Time of Ray Intersection","text":"<p>Now that rays have a time property, we need to update the <code>material::scatter()</code> methods to account for the time of intersection:</p> Handle ray time in the material::scatter() methods<pre><code>class lambertian : public material {\n...\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nauto scatter_direction = rec.normal + random_unit_vector();\n// Catch degenerate scatter direction\nif (scatter_direction.near_zero())\nscatter_direction = rec.normal;\nscattered = ray(rec.p, scatter_direction, r_in.time());\nattenuation = albedo;\nreturn true;\n}\n...\n};\nclass metal : public material {\n...\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nvec3 reflected = reflect(unit_vector(r_in.direction()), rec.normal);\nscattered = ray(rec.p, reflected + fuzz*random_in_unit_sphere(), r_in.time());\nattenuation = albedo;\nreturn (dot(scattered.direction(), rec.normal) &gt; 0);\n}\n...\n};\nclass dielectric : public material {\n...\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\n...\nscattered = ray(rec.p, direction, r_in.time());\nreturn true;\n}\n...\n};\n</code></pre>"},{"location":"the_next_week/motion_blur/#putting-everything-together","title":"Putting Everything Together","text":"<p>The code below takes the example diffuse spheres from the scene at the end of the last book, and makes them move during the image render. Each sphere moves from its center $\\mathbf{C}$ at time $t=0$ to $\\mathbf{C} + (0, r/2, 0)$ at time $t=1$:</p> Last book's final scene, but with moving spheres<pre><code>int main() {\nhittable_list world;\nauto ground_material = make_shared&lt;lambertian&gt;(color(0.5, 0.5, 0.5));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, ground_material));\nfor (int a = -11; a &lt; 11; a++) {\nfor (int b = -11; b &lt; 11; b++) {\nauto choose_mat = random_double();\npoint3 center(a + 0.9*random_double(), 0.2, b + 0.9*random_double());\nif ((center - point3(4, 0.2, 0)).length() &gt; 0.9) {\nshared_ptr&lt;material&gt; sphere_material;\nif (choose_mat &lt; 0.8) {\n// diffuse\nauto albedo = color::random() * color::random();\nsphere_material = make_shared&lt;lambertian&gt;(albedo);\nauto center2 = center + vec3(0, random_double(0,.5), 0);\nworld.add(make_shared&lt;sphere&gt;(center, center2, 0.2, sphere_material));\n} else if (choose_mat &lt; 0.95) {\n...\n}\n...\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 20;\ncam.lookfrom = point3(13,2,3);\ncam.lookat   = point3(0,0,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0.02;\ncam.focus_dist    = 10.0;\ncam.render(world);\n}\n</code></pre> <p>This gives the following result:</p> <p></p>"},{"location":"the_next_week/overview/","title":"Overview","text":"<p>In Ray Tracing in One Weekend, you built a simple brute force path tracer. In this installment we\u2019ll add textures, volumes (like fog), rectangles, instances, lights, and support for lots of objects using a BVH. When done, you\u2019ll have a \u201creal\u201d ray tracer.</p> <p>A heuristic in ray tracing that many people--including me--believe, is that most optimizations complicate the code without delivering much speedup. What I will do in this mini-book is go with the simplest approach in each design decision I make. Check https://in1weekend.blogspot.com/ for readings and references to a more sophisticated approach. However, I strongly encourage you to do no premature optimization; if it doesn\u2019t show up high in the execution time profile, it doesn\u2019t need optimization until all the features are supported!</p> <p>The two hardest parts of this book are the BVH and the Perlin textures. This is why the title suggests you take a week rather than a weekend for this endeavor. But you can save those for last if you want a weekend project. Order is not very important for the concepts presented in this book, and without BVH and Perlin texture you will still get a Cornell Box!</p> <p>Thanks to everyone who lent a hand on this project. You can find them in the acknowledgments section at the end of this book.</p>"},{"location":"the_next_week/perlin_noise/","title":"Perlin Noise","text":"<p>To get cool looking solid textures most people use some form of Perlin noise. These are named after their inventor Ken Perlin. Perlin texture doesn\u2019t return white noise like this:</p> <p></p> <p>Instead it returns something similar to blurred white noise:</p> <p></p> <p>A key part of Perlin noise is that it is repeatable: it takes a 3D point as input and always returns the same randomish number. Nearby points return similar numbers. Another important part of Perlin noise is that it be simple and fast, so it\u2019s usually done as a hack. I\u2019ll build that hack up incrementally based on Andrew Kensler\u2019s description.</p>"},{"location":"the_next_week/perlin_noise/#using-blocks-of-random-numbers","title":"Using Blocks of Random Numbers","text":"<p>We could just tile all of space with a 3D array of random numbers and use them in blocks. You get something blocky where the repeating is clear:</p> <p></p> <p>Let\u2019s just use some sort of hashing to scramble this, instead of tiling. This has a bit of support code to make it all happen:</p> A Perlin texture class and functions<pre><code>#ifndef PERLIN_H\n#define PERLIN_H\n#include \"rtweekend.h\"\nclass perlin {\npublic:\nperlin() {\nranfloat = new double[point_count];\nfor (int i = 0; i &lt; point_count; ++i) {\nranfloat[i] = random_double();\n}\nperm_x = perlin_generate_perm();\nperm_y = perlin_generate_perm();\nperm_z = perlin_generate_perm();\n}\n~perlin() {\ndelete[] ranfloat;\ndelete[] perm_x;\ndelete[] perm_y;\ndelete[] perm_z;\n}\ndouble noise(const point3&amp; p) const {\nauto i = static_cast&lt;int&gt;(4*p.x()) &amp; 255;\nauto j = static_cast&lt;int&gt;(4*p.y()) &amp; 255;\nauto k = static_cast&lt;int&gt;(4*p.z()) &amp; 255;\nreturn ranfloat[perm_x[i] ^ perm_y[j] ^ perm_z[k]];\n}\nprivate:\nstatic const int point_count = 256;\ndouble* ranfloat;\nint* perm_x;\nint* perm_y;\nint* perm_z;\nstatic int* perlin_generate_perm() {\nauto p = new int[point_count];\nfor (int i = 0; i &lt; perlin::point_count; i++)\np[i] = i;\npermute(p, point_count);\nreturn p;\n}\nstatic void permute(int* p, int n) {\nfor (int i = n-1; i &gt; 0; i--) {\nint target = random_int(0, i);\nint tmp = p[i];\np[i] = p[target];\np[target] = tmp;\n}\n}\n};\n#endif\n</code></pre> <p>Now if we create an actual texture that takes these floats between 0 and 1 and creates grey colors:</p> Noise texture<pre><code>#include \"perlin.h\"\nclass noise_texture : public texture {\npublic:\nnoise_texture() {}\ncolor value(double u, double v, const point3&amp; p) const override {\nreturn color(1,1,1) * noise.noise(p);\n}\nprivate:\nperlin noise;\n};\n</code></pre> <p>We can use that texture on some spheres:</p> Scene with two Perlin-textured spheres<pre><code>void two_perlin_spheres() {\nhittable_list world;\nauto pertext = make_shared&lt;noise_texture&gt;();\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, make_shared&lt;lambertian&gt;(pertext)));\nworld.add(make_shared&lt;sphere&gt;(point3(0,2,0), 2, make_shared&lt;lambertian&gt;(pertext)));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 20;\ncam.lookfrom = point3(13,2,3);\ncam.lookat   = point3(0,0,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (4) {\ncase 1:  random_spheres();     break;\ncase 2:  two_spheres();        break;\ncase 3:  earth();              break;\ncase 4:  two_perlin_spheres(); break;\n}\n}\n</code></pre> <p>Add the hashing does scramble as hoped:</p> <p></p>"},{"location":"the_next_week/perlin_noise/#smoothing-out-the-result","title":"Smoothing out the Result","text":"<p>To make it smooth, we can linearly interpolate:</p> Perlin with trilinear interpolation<pre><code>class perlin {\npublic:\n...\ndouble noise(const point3&amp; p) const {\nauto u = p.x() - floor(p.x());\nauto v = p.y() - floor(p.y());\nauto w = p.z() - floor(p.z());\nauto i = static_cast&lt;int&gt;(floor(p.x()));\nauto j = static_cast&lt;int&gt;(floor(p.y()));\nauto k = static_cast&lt;int&gt;(floor(p.z()));\ndouble c[2][2][2];\nfor (int di=0; di &lt; 2; di++)\nfor (int dj=0; dj &lt; 2; dj++)\nfor (int dk=0; dk &lt; 2; dk++)\nc[di][dj][dk] = ranfloat[\nperm_x[(i+di) &amp; 255] ^\nperm_y[(j+dj) &amp; 255] ^\nperm_z[(k+dk) &amp; 255]\n];\nreturn trilinear_interp(c, u, v, w);\n}\n...\nprivate:\n...\nstatic double trilinear_interp(double c[2][2][2], double u, double v, double w) {\nauto accum = 0.0;\nfor (int i=0; i &lt; 2; i++)\nfor (int j=0; j &lt; 2; j++)\nfor (int k=0; k &lt; 2; k++)\naccum += (i*u + (1-i)*(1-u))*\n(j*v + (1-j)*(1-v))*\n(k*w + (1-k)*(1-w))*c[i][j][k];\nreturn accum;\n}\n};\n</code></pre> <p>And we get:</p> <p></p>"},{"location":"the_next_week/perlin_noise/#improvement-with-hermitian-smoothing","title":"Improvement with Hermitian Smoothing","text":"<p>Smoothing yields an improved result, but there are obvious grid features in there. Some of it is Mach bands, a known perceptual artifact of linear interpolation of color. A standard trick is to use a Hermite cubic to round off the interpolation:</p> Perlin smoothed<pre><code>class perlin (\npublic:\n...\ndouble noise(const point3&amp; p) const {\nauto u = p.x() - floor(p.x());\nauto v = p.y() - floor(p.y());\nauto w = p.z() - floor(p.z());\nu = u*u*(3-2*u);\nv = v*v*(3-2*v);\nw = w*w*(3-2*w);\nauto i = static_cast&lt;int&gt;(floor(p.x()));\nauto j = static_cast&lt;int&gt;(floor(p.y()));\nauto k = static_cast&lt;int&gt;(floor(p.z()));\n...\n</code></pre> <p>This gives a smoother looking image:</p> <p></p>"},{"location":"the_next_week/perlin_noise/#tweaking-the-frequency","title":"Tweaking The Frequency","text":"<p>It is also a bit low frequency. We can scale the input point to make it vary more quickly:</p> Perlin smoothed, higher frequency<pre><code>class noise_texture : public texture {\npublic:\nnoise_texture() {}\nnoise_texture(double sc) : scale(sc) {}\ncolor value(double u, double v, const point3&amp; p) const override {\nreturn color(1,1,1) * noise.noise(scale * p);\n}\nprivate:\nperlin noise;\ndouble scale;\n};\n</code></pre> <p>We then add that scale to the <code>two_perlin_spheres()</code> scene description:</p> Perlin-textured spheres with a scale to the noise<pre><code>void two_perlin_spheres() {\n...\nauto pertext = make_shared&lt;noise_texture&gt;(4);\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, make_shared&lt;lambertian&gt;(pertext)));\nworld.add(make_shared&lt;sphere&gt;(point3(0, 2, 0), 2, make_shared&lt;lambertian&gt;(pertext)));\ncamera cam;\n..\n}\n</code></pre> <p>This yields the following result:</p> <p></p>"},{"location":"the_next_week/perlin_noise/#using-random-vectors-on-the-lattice-points","title":"Using Random Vectors on the Lattice Points","text":"<p>This is still a bit blocky looking, probably because the min and max of the pattern always lands exactly on the integer x/y/z. Ken Perlin\u2019s very clever trick was to instead put random unit vectors (instead of just floats) on the lattice points, and use a dot product to move the min and max off the lattice. So, first we need to change the random floats to random vectors. These vectors are any reasonable set of irregular directions, and I won't bother to make them exactly uniform:</p> Perlin with random unit translations<pre><code>class perlin {\npublic:\nperlin() {\nranvec = new vec3[point_count];\nfor (int i = 0; i &lt; point_count; ++i) {\nranvec[i] = unit_vector(vec3::random(-1,1));\n}\nperm_x = perlin_generate_perm();\nperm_y = perlin_generate_perm();\nperm_z = perlin_generate_perm();\n}\n~perlin() {\ndelete[] ranvec;\ndelete[] perm_x;\ndelete[] perm_y;\ndelete[] perm_z;\n}\n...\nprivate:\nstatic const int point_count = 256;\nvec3* ranvec;\nint* perm_x;\nint* perm_y;\nint* perm_z;\n...\n};\n</code></pre> <p>The Perlin class noise() method is now:</p> Perlin class with new noise() method<pre><code>class perlin {\npublic:\n...\ndouble noise(const point3&amp; p) const {\nauto u = p.x() - floor(p.x());\nauto v = p.y() - floor(p.y());\nauto w = p.z() - floor(p.z());\nauto i = static_cast&lt;int&gt;(floor(p.x()));\nauto j = static_cast&lt;int&gt;(floor(p.y()));\nauto k = static_cast&lt;int&gt;(floor(p.z()));\nvec3 c[2][2][2];\nfor (int di=0; di &lt; 2; di++)\nfor (int dj=0; dj &lt; 2; dj++)\nfor (int dk=0; dk &lt; 2; dk++)\nc[di][dj][dk] = ranvec[\nperm_x[(i+di) &amp; 255] ^\nperm_y[(j+dj) &amp; 255] ^\nperm_z[(k+dk) &amp; 255]\n];\nreturn perlin_interp(c, u, v, w);\n}\n...\n};\n</code></pre> <p>And the interpolation becomes a bit more complicated:</p> Perlin interpolation function so far<pre><code>class perlin {\n...\nprivate:\n...\nstatic double perlin_interp(vec3 c[2][2][2], double u, double v, double w) {\nauto uu = u*u*(3-2*u);\nauto vv = v*v*(3-2*v);\nauto ww = w*w*(3-2*w);\nauto accum = 0.0;\nfor (int i=0; i &lt; 2; i++)\nfor (int j=0; j &lt; 2; j++)\nfor (int k=0; k &lt; 2; k++) {\nvec3 weight_v(u-i, v-j, w-k);\naccum += (i*uu + (1-i)*(1-uu))\n* (j*vv + (1-j)*(1-vv))\n* (k*ww + (1-k)*(1-ww))\n* dot(c[i][j][k], weight_v);\n}\nreturn accum;\n}\n...\n};\n</code></pre> <p>The output of the perlin interpretation can return negative values. These negative values will be passed to the <code>sqrt()</code> function of our gamma function and get turned into <code>NaN</code>s. We will cast the perlin output back to between 0 and 1.</p> Perlin smoothed, higher frequency<pre><code>class noise_texture : public texture {\npublic:\nnoise_texture() {}\nnoise_texture(double sc) : scale(sc) {}\ncolor value(double u, double v, const point3&amp; p) const override {\nreturn color(1,1,1) * 0.5 * (1.0 + noise.noise(scale * p));\n}\nprivate:\nperlin noise;\ndouble scale;\n};\n</code></pre> <p>This finally gives something more reasonable looking:</p> <p></p>"},{"location":"the_next_week/perlin_noise/#introducing-turbulence","title":"Introducing Turbulence","text":"<p>Very often, a composite noise that has multiple summed frequencies is used. This is usually called turbulence, and is a sum of repeated calls to noise:</p> Turbulence function<pre><code>class perlin {\n...\npublic:\n...\ndouble turb(const point3&amp; p, int depth=7) const {\nauto accum = 0.0;\nauto temp_p = p;\nauto weight = 1.0;\nfor (int i = 0; i &lt; depth; i++) {\naccum += weight*noise(temp_p);\nweight *= 0.5;\ntemp_p *= 2;\n}\nreturn fabs(accum);\n}\n...\n</code></pre> <p>Here <code>fabs()</code> is the absolute value function defined in <code>&lt;cmath&gt;</code>.</p> Noise texture with turbulence<pre><code>class noise_texture : public texture {\npublic:\nnoise_texture() {}\nnoise_texture(double sc) : scale(sc) {}\ncolor value(double u, double v, const point3&amp; p) const override {\nauto s = scale * p;\nreturn color(1,1,1) * noise.turb(s);\n}\nprivate:\nperlin noise;\ndouble scale;\n};\n</code></pre> <p>Used directly, turbulence gives a sort of camouflage netting appearance:</p> <p></p>"},{"location":"the_next_week/perlin_noise/#adjusting-the-phase","title":"Adjusting the Phase","text":"<p>However, usually turbulence is used indirectly. For example, the \u201chello world\u201d of procedural solid textures is a simple marble-like texture. The basic idea is to make color proportional to something like a sine function, and use turbulence to adjust the phase (so it shifts $x$ in $\\sin(x)$) which makes the stripes undulate. Commenting out straight noise and turbulence, and giving a marble-like effect is:</p> Noise texture with marbled texture<pre><code>class noise_texture : public texture {\npublic:\nnoise_texture() {}\nnoise_texture(double sc) : scale(sc) {}\ncolor value(double u, double v, const point3&amp; p) const override {\nauto s = scale * p;\nreturn color(1,1,1) * 0.5 * (1 + sin(s.z() + 10*noise.turb(s)));\n}\nprivate:\nperlin noise;\ndouble scale;\n};\n</code></pre> <p>Which yields:</p> <p></p>"},{"location":"the_next_week/quadrilaterals/","title":"Quadrilaterals","text":"<p>We've managed to get more than half way through this three-book series using spheres as our only geometric primitive. Time to add our second primitive: the quadrilateral.</p>"},{"location":"the_next_week/quadrilaterals/#defining-the-quadrilateral","title":"Defining the Quadrilateral","text":"<p>Though we'll name our new primitive a <code>quad</code>, it will technically be a parallelogram (opposite sides are parallel) instead of a general quadrilateral. For our purposes, we'll use three geometric entities to define a quad:</p> <ol> <li>$\\mathbf{Q}$, the lower-left corner.</li> <li>$\\mathbf{u}$, a vector representing the first side.      $\\mathbf{Q} + \\mathbf{u}$ gives one of the corners adjacent to $\\mathbf{Q}$.</li> <li>$\\mathbf{v}$, a vector representing the second side.      $\\mathbf{Q} + \\mathbf{v}$ gives the other corner adjacent to $\\mathbf{Q}$.</li> </ol> <p>The corner of the quad opposite $\\mathbf{Q}$ is given by $\\mathbf{Q} + \\mathbf{u} + \\mathbf{v}$. These values are three-dimensional, even though a quad itself is a two-dimensional object. For example, a quad with corner at the origin and extending two units in the Z direction and one unit in the Y direction would have values $\\mathbf{Q} = (0,0,0), \\mathbf{u} = (0,0,2), \\text{and } \\mathbf{v} = (0,1,0)$.</p> <p>The following figure illustrates the quadrilateral components.</p> <p></p> <p>Quads are flat, so their axis-aligned bounding box will have zero thickness in one dimension if the quad lies in the XY, YZ, or ZX plane. This can lead to numerical problems with ray intersection, but we can address this by padding any zero-sized dimensions of the bounding box. Padding is fine because we aren't changing the intersection of the quad; we're only expanding its bounding box to remove the possibility of numerical problems, and the bounds are just a rough approximation to the actual shape anyway. To this end, we add a new <code>aabb::pad()</code> method that remedies this situation:</p> New aabb::pad() method<pre><code>...\nclass aabb {\npublic:\n...\naabb(const aabb&amp; box0, const aabb&amp; box1) {\nx = interval(box0.x, box1.x);\ny = interval(box0.y, box1.y);\nz = interval(box0.z, box1.z);\n}\naabb pad() {\n// Return an AABB that has no side narrower than some delta, padding if necessary.\ndouble delta = 0.0001;\ninterval new_x = (x.size() &gt;= delta) ? x : x.expand(delta);\ninterval new_y = (y.size() &gt;= delta) ? y : y.expand(delta);\ninterval new_z = (z.size() &gt;= delta) ? z : z.expand(delta);\nreturn aabb(new_x, new_y, new_z);\n}\n</code></pre> <p>Now we're ready for the first sketch of the new <code>quad</code> class:</p> 2D quadrilateral (parallelogram) class<pre><code>#ifndef QUAD_H\n#define QUAD_H\n#include \"rtweekend.h\"\n#include \"hittable.h\"\nclass quad : public hittable {\npublic:\nquad(const point3&amp; _Q, const vec3&amp; _u, const vec3&amp; _v, shared_ptr&lt;material&gt; m)\n: Q(_Q), u(_u), v(_v), mat(m)\n{\nset_bounding_box();\n}\nvirtual void set_bounding_box() {\nbbox = aabb(Q, Q + u + v).pad();\n}\naabb bounding_box() const override { return bbox; }\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\nreturn false; // To be implemented\n}\nprivate:\npoint3 Q;\nvec3 u, v;\nshared_ptr&lt;material&gt; mat;\naabb bbox;\n};\n#endif\n</code></pre>"},{"location":"the_next_week/quadrilaterals/#ray-plane-intersection","title":"Ray-Plane Intersection","text":"<p>As you can see in the prior listing, <code>quad::hit()</code> remains to be implemented. Just as for spheres, we need to determine whether a given ray intersects the primitive, and if so, the various properties of that intersection (hit point, normal, texture coordinates and so forth).</p> <p>Ray-quad intersection will be determined in three steps:</p> <ol> <li>finding the plane that contains that quad,</li> <li>solving for the intersection of a ray and the quad-containing plane,</li> <li>determining if the hit point lies inside the quad.</li> </ol> <p>We'll first tackle the middle step, solving for general ray-plane intersection.</p> <p>Spheres are generally the first ray tracing primitive taught because their implicit formula makes it so easy to solve for ray intersection. Like spheres, planes also have an implicit formula, and we can use their implicit formula to produce an algorithm that solves for ray-plane intersection. Indeed, ray-plane intersection is even easier to solve than ray-sphere intersection.</p> <p>You may already know this implicit formula for a plane:</p> <p>$$ Ax + By + Cz + D = 0 $$</p> <p>where $A,B,C,D$ are just constants, and $x,y,z$ are the values of any point $(x,y,z)$ that lies on the plane. A plane is thus the set of all points $(x,y,z)$ that satisfy the formula above. It makes things slightly easier to use the alternate formulation:</p> <p>$$ Ax + By + Cz = D $$</p> <p>(We didn't flip the sign of D because it's just some constant that we'll figure out later.)</p> <p>Here's an intuitive way to think of this formula: given the plane perpendicular to the normal vector $\\mathbf{n} = (A,B,C)$, and the position vector $\\mathbf{v} = (x,y,z)$ (that is, the vector from the origin to any point on the plane), then we can use the dot product to solve for $D$:</p> <p>$$ \\mathbf{n} \\cdot \\mathbf{v} = D $$</p> <p>for any position on the plane. This is an equivalent formulation of the $Ax + By + Cz = D$ formula given above, only now in terms of vectors.</p> <p>Now to find the intersection with some ray $\\mathbf{R}(t) = \\mathbf{P} + t\\mathbf{d}$. Plugging in the ray equation, we get</p> <p>$$ \\mathbf{n} \\cdot ( \\mathbf{P} + t \\mathbf{d} ) = D $$</p> <p>Solving for $t$:</p> <p>$$ \\mathbf{n} \\cdot \\mathbf{P} + \\mathbf{n} \\cdot t \\mathbf{d}  = D $$</p> <p>$$ \\mathbf{n} \\cdot \\mathbf{P} + t(\\mathbf{n} \\cdot \\mathbf{d}) = D $$</p> <p>$$ t = \\frac{D - \\mathbf{n} \\cdot \\mathbf{P}}{\\mathbf{n} \\cdot \\mathbf{d}} $$</p> <p>This gives us $t$, which we can plug into the ray equation to find the point of intersection. Note that the denominator $\\mathbf{n} \\cdot \\mathbf{d}$ will be zero if the ray is parallel to the plane. In this case, we can immediately record a miss between the ray and the plane. As for other primitives, if the ray $t$ parameter is less than the minimum acceptable value, we also record a miss.</p> <p>All right, we can find the point of intersection between a ray and the plane that contains a given quadrilateral. In fact, we can use this approach to test any planar primitive, like triangles and disks (more on that later).</p>"},{"location":"the_next_week/quadrilaterals/#finding-the-plane-that-contains-a-given-quadrilateral","title":"Finding the Plane That Contains a Given Quadrilateral","text":"<p>We've solved step two above: solving the ray-plane intersection, assuming we have the plane equation. To do this, we need to tackle step one above: finding the equation for the plane that contains the quad. We have quadrilateral parameters $\\mathbf{Q}$, $\\mathbf{u}$, and $\\mathbf{v}$, and want the corresponding equation of the plane containing the quad defined by these three values.</p> <p>Fortunately, this is very simple. Recall that in the equation $Ax + By + Cz = D$, $(A,B,C)$ represents the normal vector. To get this, we just use the cross product of the two side vectors $\\mathbf{u}$ and $\\mathbf{v}$:</p> <p>$$ \\mathbf{n} = \\operatorname{unit_vector}(\\mathbf{u} \\times \\mathbf{v}) $$</p> <p>The plane is defined as all points $(x,y,z)$ that satisfy the equation $Ax + By + Cz = D$. Well, we know that $\\mathbf{Q}$ lies on the plane, so that's enough to solve for $D$:</p> <p>$$ \\begin{align}      D &amp;= n_x Q_x + n_y Q_y + n_z Q_z \\        &amp;= \\mathbf{n} \\cdot \\mathbf{Q} \\      \\end{align}   $$</p> <p>Add the planar values to the <code>quad</code> class:</p> Caching thel planar values<pre><code>class quad : public hittable {\npublic:\nquad(const point3&amp; _Q, const vec3&amp; _u, const vec3&amp; _v, shared_ptr&lt;material&gt; m)\n: Q(_Q), u(_u), v(_v), mat(m)\n{\nauto n = cross(u, v);\nnormal = unit_vector(n);\nD = dot(normal, Q);\nset_bounding_box();\n}\n...\nprivate:\npoint3 Q;\nvec3 u, v;\nshared_ptr&lt;material&gt; mat;\naabb bbox;\nvec3 normal;\ndouble D;\n};\n</code></pre> <p>We will use the two values <code>normal</code> and <code>D</code> to find the point of intersection between a given ray and the plane containing the quadrilateral.</p> <p>As an incremental step, let's implement the <code>hit()</code> method to handle the infinite plane containing our quadrilateral.</p> hit() method for the infinite plane<pre><code>class quad : public hittable {\n...\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\nauto denom = dot(normal, r.direction());\n// No hit if the ray is parallel to the plane.\nif (fabs(denom) &lt; 1e-8)\nreturn false;\n// Return false if the hit point parameter t is outside the ray interval.\nauto t = (D - dot(normal, r.origin())) / denom;\nif (!ray_t.contains(t))\nreturn false;\nauto intersection = r.at(t);\nrec.t = t;\nrec.p = intersection;\nrec.mat = mat;\nrec.set_face_normal(r, normal);\nreturn true;\n}\n...\n</code></pre>"},{"location":"the_next_week/quadrilaterals/#orienting-points-on-the-plane","title":"Orienting Points on The Plane","text":"<p>At this stage, the intersection point is on the plane that contains the quadrilateral, but it could be anywhere on the plane: the ray-plane intersection point will lie inside or outside the quadrilateral. We need to test for intersection points that lie inside the quadrilateral (hit), and reject points that lie outside (miss). To determine where a point lies relative to the quad, and to assign texture coordinates to the point of intersection, we need to orient the intersection point on the plane.</p> <p>To do this, we'll construct a coordinate frame for the plane -- a way of orienting any point located on the plane. We've already been using a coordinate frame for our 3D space -- this is defined by an origin point $\\mathbf{O}$ and three basis vectors $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$.</p> <p>Since a plane is a 2D construct, we just need a plane origin point $\\mathbf{Q}$ and two basis vectors: $\\mathbf{u}$ and $\\mathbf{v}$. Normally, axes are perpendicular to each other. However, this doesn't need to be the case in order to span the entire space -- you just need two axes that are not parallel to each other.</p> <p></p> <p>Consider figure [ray-plane] as an example. Ray $\\mathbf{R}$ intersects the plane, yielding intersection point $\\mathbf{P}$ (not to be confused with the ray origin point $\\mathbf{P}$ above). Measuring against plane vectors $\\mathbf{u}$ and $\\mathbf{v}$, the intersection point $\\mathbf{P}$ in the example above is at $\\mathbf{Q} + (1)\\mathbf{u} + (\\frac{1}{2})\\mathbf{v}$. In other words, the $\\mathbf{UV}$ (plane) coordinates of intersection point $\\mathbf{P}$ are $(1,\\frac{1}{2})$.</p> <p>Generally, given some arbitrary point $\\mathbf{P}$, we seek two scalar values $\\alpha$ and $\\beta$, so that</p> <p>$$ \\mathbf{P} = \\mathbf{Q} + \\alpha \\mathbf{u} + \\beta \\mathbf{v} $$</p> <p>If $\\mathbf{u}$ and $\\mathbf{v}$ were guaranteed to be orthogonal to each other (forming a 90\u00b0 angle between them), then this would be a simple matter of using the dot product to project $\\mathbf{P}$ onto each of the basis vectors $\\mathbf{u}$ and $\\mathbf{v}$. However, since we are not restricting $\\mathbf{u}$ and $\\mathbf{v}$ to be orthogonal, the math's a little bit trickier.</p> <p>$$ \\mathbf{P} = \\mathbf{Q} + \\alpha \\mathbf{u} + \\beta \\mathbf{v}$$</p> <p>$$ \\mathbf{p} = \\mathbf{P} - \\mathbf{Q} = \\alpha \\mathbf{u} + \\beta \\mathbf{v} $$</p> <p>Here, $\\mathbf{P}$ is the point of intersection, and $\\mathbf{p}$ is the vector from $\\mathbf{Q}$ to $\\mathbf{P}$.</p> <p>Cross the above equation with $\\mathbf{u}$ and $\\mathbf{v}$, respectively:</p> <p>$$ \\begin{align}      \\mathbf{u} \\times \\mathbf{p} &amp;= \\mathbf{u} \\times (\\alpha \\mathbf{u} + \\beta \\mathbf{v}) \\      &amp;= \\mathbf{u} \\times \\alpha \\mathbf{u} + \\mathbf{u} \\times \\beta \\mathbf{v} \\      &amp;= \\alpha(\\mathbf{u} \\times \\mathbf{u}) + \\beta(\\mathbf{u} \\times \\mathbf{v})      \\end{align} $$</p> <p>$$ \\begin{align}      \\mathbf{v} \\times \\mathbf{p} &amp;= \\mathbf{v} \\times (\\alpha \\mathbf{u} + \\beta \\mathbf{v}) \\      &amp;= \\mathbf{v} \\times \\alpha \\mathbf{u} + \\mathbf{v} \\times \\beta \\mathbf{v} \\      &amp;= \\alpha(\\mathbf{v} \\times \\mathbf{u}) + \\beta(\\mathbf{v} \\times \\mathbf{v})      \\end{align} $$</p> <p>Since any vector crossed with itself yields zero, these equations simplify to</p> <p>$$ \\mathbf{v} \\times \\mathbf{p} = \\alpha(\\mathbf{v} \\times \\mathbf{u}) $$   $$ \\mathbf{u} \\times \\mathbf{p} = \\beta(\\mathbf{u} \\times \\mathbf{v}) $$</p> <p>Now to solve for the coefficients $\\alpha$ and $\\beta$. If you're new to vector math, you might try to divide by $\\mathbf{u} \\times \\mathbf{v}$ and $\\mathbf{v} \\times \\mathbf{u}$, but you can't divide by vectors. Instead, we can take the dot product of both sides of the above equations with the plane normal $\\mathbf{n} = \\mathbf{u} \\times \\mathbf{v}$, reducing both sides to scalars, which we can divide by.</p> <p>$$ \\mathbf{n} \\cdot (\\mathbf{v} \\times \\mathbf{p})      = \\mathbf{n} \\cdot \\alpha(\\mathbf{v} \\times \\mathbf{u}) $$</p> <p>$$ \\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{p})      = \\mathbf{n} \\cdot \\beta(\\mathbf{u} \\times \\mathbf{v}) $$</p> <p>Now isolating the coefficients is a simple matter of division:</p> <p>$$ \\alpha = \\frac{\\mathbf{n} \\cdot (\\mathbf{v} \\times \\mathbf{p})}                    {\\mathbf{n} \\cdot (\\mathbf{v} \\times \\mathbf{u})} $$</p> <p>$$ \\beta  = \\frac{\\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{p})}                    {\\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{v})} $$</p> <p>Reversing the cross products for both the numerator and denominator of $\\alpha$ (recall that $\\mathbf{a} \\times \\mathbf{b} = - \\mathbf{b} \\times \\mathbf{a}$) gives us a common denominator for both coefficients:</p> <p>$$ \\alpha = \\frac{\\mathbf{n} \\cdot (\\mathbf{p} \\times \\mathbf{v})}                    {\\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{v})} $$</p> <p>$$ \\beta  = \\frac{\\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{p})}                    {\\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{v})} $$</p> <p>Now we can perform one final simplification, computing a vector $\\mathbf{w}$ that will be constant for the plane's basis frame, for any planar point $\\mathbf{P}$:</p> <p>$$ \\mathbf{w} = \\frac{\\mathbf{n}}{\\mathbf{n} \\cdot (\\mathbf{u} \\times \\mathbf{v})}                 = \\frac{\\mathbf{n}}{\\mathbf{n} \\cdot \\mathbf{n}}$$</p> <p>$$ \\alpha = \\mathbf{w} \\cdot (\\mathbf{p} \\times \\mathbf{v}) $$   $$ \\beta  = \\mathbf{w} \\cdot (\\mathbf{u} \\times \\mathbf{p}) $$</p> <p>The vector $\\mathbf{w}$ is constant for a given quadrilateral, so we'll cache that value.</p> Caching the quadrilateral's w value<pre><code>class quad : public hittable {\npublic:\nquad(const point3&amp; _Q, const vec3&amp; _u, const vec3&amp; _v, shared_ptr&lt;material&gt; m)\n: Q(_Q), u(_u), v(_v), mat(m)\n{\nauto n = cross(u, v);\nnormal = unit_vector(n);\nD = dot(normal, Q);\nw = n / dot(n,n);\nset_bounding_box();\n}\n...\nprivate:\npoint3 Q;\nvec3 u, v;\nshared_ptr&lt;material&gt; mat;\naabb bbox;\nvec3 normal;\ndouble D;\nvec3 w;\n};\n</code></pre>"},{"location":"the_next_week/quadrilaterals/#interior-testing-of-the-intersection-using-uv-coordinates","title":"Interior Testing of The Intersection Using UV Coordinates","text":"<p>Now that we have the intersection point's planar coordinates $\\alpha$ and $\\beta$, we can easily use these to determine if the intersection point is inside the quadrilateral -- that is, if the ray actually hit the quadrilateral.</p> <p>The plane is divided into coordinate regions like so:</p> <p></p> <p>Thus, to see if a point with planar coordinates $(\\alpha,\\beta)$ lies inside the quadrilateral, it just needs to meet the following criteria:</p> <ol> <li>$ 0 \\leq \\alpha \\leq 1 $</li> <li>$ 0 \\leq \\beta \\leq 1 $</li> </ol> <p>That's the last piece needed to implement quadrilateral primitives.</p> <p>Pause a bit here and consider that if you use the $(\\alpha,\\beta)$ coordinates to determine if a point lies inside a quadrilateral (parallelogram), it's not too hard to imagine using these same 2D coordinates to determine if the intersection point lies inside any other 2D (planar) primitive!</p> <p>We'll leave these additional 2D shape possibilities as an exercise to the reader, depending on your desire to explore. Consider triangles, disks, and rings (all of these are surprisingly easy). You could even create cut-out stencils based on the pixels of a texture map, or a Mandelbrot shape!</p> <p>In order to make such experimentation a bit easier, we'll factor out the $(\\alpha,\\beta)$ interior test method from the hit method.</p> Final quad class<pre><code>...\n#include &lt;cmath&gt;\nclass quad : public hittable {\npublic:\n...\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\nauto denom = dot(normal, r.direction());\n// No hit if the ray is parallel to the plane.\nif (fabs(denom) &lt; 1e-8)\nreturn false;\n// Return false if the hit point parameter t is outside the ray interval.\nauto t = (D - dot(normal, r.origin())) / denom;\nif (!ray_t.contains(t))\nreturn false;\n// Determine the hit point lies within the planar shape using its plane coordinates.\nauto intersection = r.at(t);\nvec3 planar_hitpt_vector = intersection - Q;\nauto alpha = dot(w, cross(planar_hitpt_vector, v));\nauto beta = dot(w, cross(u, planar_hitpt_vector));\nif (!is_interior(alpha, beta, rec))\nreturn false;\n// Ray hits the 2D shape; set the rest of the hit record and return true.\nrec.t = t;\nrec.p = intersection;\nrec.mat = mat;\nrec.set_face_normal(r, normal);\nreturn true;\n}\nvirtual bool is_interior(double a, double b, hit_record&amp; rec) const {\n// Given the hit point in plane coordinates, return false if it is outside the\n// primitive, otherwise set the hit record UV coordinates and return true.\nif ((a &lt; 0) || (1 &lt; a) || (b &lt; 0) || (1 &lt; b))\nreturn false;\nrec.u = a;\nrec.v = b;\nreturn true;\n}\n...\n};\n#endif\n</code></pre> <p>And now we add a new scene to demonstrate our new <code>quad</code> primitive:</p> A new scene with quads<pre><code>...\n#include \"quad.h\"\n...\nvoid quads() {\nhittable_list world;\n// Materials\nauto left_red     = make_shared&lt;lambertian&gt;(color(1.0, 0.2, 0.2));\nauto back_green   = make_shared&lt;lambertian&gt;(color(0.2, 1.0, 0.2));\nauto right_blue   = make_shared&lt;lambertian&gt;(color(0.2, 0.2, 1.0));\nauto upper_orange = make_shared&lt;lambertian&gt;(color(1.0, 0.5, 0.0));\nauto lower_teal   = make_shared&lt;lambertian&gt;(color(0.2, 0.8, 0.8));\n// Quads\nworld.add(make_shared&lt;quad&gt;(point3(-3,-2, 5), vec3(0, 0,-4), vec3(0, 4, 0), left_red));\nworld.add(make_shared&lt;quad&gt;(point3(-2,-2, 0), vec3(4, 0, 0), vec3(0, 4, 0), back_green));\nworld.add(make_shared&lt;quad&gt;(point3( 3,-2, 1), vec3(0, 0, 4), vec3(0, 4, 0), right_blue));\nworld.add(make_shared&lt;quad&gt;(point3(-2, 3, 1), vec3(4, 0, 0), vec3(0, 0, 4), upper_orange));\nworld.add(make_shared&lt;quad&gt;(point3(-2,-3, 5), vec3(4, 0, 0), vec3(0, 0,-4), lower_teal));\ncamera cam;\ncam.aspect_ratio      = 1.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 80;\ncam.lookfrom = point3(0,0,9);\ncam.lookat   = point3(0,0,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (5) {\ncase 1:  random_spheres();     break;\ncase 2:  two_spheres();        break;\ncase 3:  earth();              break;\ncase 4:  two_perlin_spheres(); break;\ncase 5:  quads();              break;\n}\n}\n</code></pre> <p></p>"},{"location":"the_next_week/texture_mapping/","title":"Texture Mapping","text":"<p>Texture mapping in computer graphics is the process of applying a material effect to an object in the scene. The \"texture\" part is the effect, and the \"mapping\" part is in the mathematical sense of mapping one space onto another. This effect could be any material property: color, shininess, bump geometry (called Bump Mapping), or even material existence (to create cut-out regions of the surface).</p> <p>The most common type of texture mapping maps an image onto the surface of an object, defining the color at each point on the object\u2019s surface. In practice, we implement the process in reverse: given some point on the object, we\u2019ll look up the color defined by the texture map.</p> <p>To begin with, we'll make the texture colors procedural, and will create a texture map of constant color. Most programs keep constant RGB colors and textures in different classes, so feel free to do something different, but I am a big believer in this architecture because it's great being able to make any color a texture.</p> <p>In order to perform the texture lookup, we need a texture coordinate. This coordinate can be defined in many ways, and we'll develop this idea as we progress. For now, we'll pass in two dimensional texture coordinates. By convention, texture coordinates are named $u$ and $v$. For a constant texture, every $(u,v)$ pair yields a constant color, so we can actually ignore the coordinates completely. However, other texture types will need these coordinates, so we keep these in the method interface.</p> <p>The primary method of our texture classes is the <code>color value(...)</code> method, which returns the texture color given the input coordinates. In addition to taking the point's texture coordinates $u$ and $v$, we also provide the position of the point in question, for reasons that will become apparent later.</p>"},{"location":"the_next_week/texture_mapping/#constant-color-texture","title":"Co|nstant Color Texture","text":"A texture class<pre><code>#ifndef TEXTURE_H\n#define TEXTURE_H\n#include \"rtweekend.h\"\nclass texture {\npublic:\nvirtual ~texture() = default;\nvirtual color value(double u, double v, const point3&amp; p) const = 0;\n};\nclass solid_color : public texture {\npublic:\nsolid_color(color c) : color_value(c) {}\nsolid_color(double red, double green, double blue) : solid_color(color(red,green,blue)) {}\ncolor value(double u, double v, const point3&amp; p) const override {\nreturn color_value;\n}\nprivate:\ncolor color_value;\n};\n#endif\n</code></pre> <p>We'll need to update the <code>hit_record</code> structure to store the $u,v$ surface coordinates of the ray-object hit point.</p> Adding u,v coordinates to the hit_record<pre><code>class hit_record {\npublic:\nvec3 p;\nvec3 normal;\nshared_ptr&lt;material&gt; mat;\ndouble t;\ndouble u;\ndouble v;\nbool front_face;\n...\n</code></pre> <p>We will also need to compute $(u,v)$ texture coordinates for a given point on each type of <code>hittable</code>.</p>"},{"location":"the_next_week/texture_mapping/#solid-textures-a-checker-texture","title":"Solid Textures: A Checker Texture","text":"<p>A solid (or spatial) texture depends only on the position of each point in 3D space. You can think of a solid texture as if it's coloring all of the points in space itself, instead of coloring a given object in that space. For this reason, the object can move through the colors of the texture as it changes position, though usually you would to fix the relationship between the object and the solid texture.</p> <p>To explore spatial textures, we'll implement a spatial <code>checker_texture</code> class, which implements a three-dimensional checker pattern. Since a spatial texture function is driven by a given position in space, the texture <code>value()</code> function ignores the <code>u</code> and <code>v</code> parameters, and uses only the <code>p</code> parameter.</p> <p>To accomplish the checkered pattern, we'll first compute the floor of each component of the input point. We could truncate the coordinates, but that would pull values toward zero, which would give us the same color on both sides of zero. The floor function will always shift values to the integer value on the left (toward negative infinity). Given these three integer results ($\\lfloor x \\rfloor, \\lfloor y \\rfloor, \\lfloor z \\rfloor$) we take their sum and compute the result modulo two, which gives us either 0 or 1. Zero maps to the even color, and one to the odd color.</p> <p>Finally, we add a scaling factor to the texture, to allow us to control the size of the checker pattern in the scene.</p> Checkered texture<pre><code>class checker_texture : public texture {\npublic:\nchecker_texture(double _scale, shared_ptr&lt;texture&gt; _even, shared_ptr&lt;texture&gt; _odd)\n: inv_scale(1.0 / _scale), even(_even), odd(_odd) {}\nchecker_texture(double _scale, color c1, color c2)\n: inv_scale(1.0 / _scale),\neven(make_shared&lt;solid_color&gt;(c1)),\nodd(make_shared&lt;solid_color&gt;(c2))\n{}\ncolor value(double u, double v, const point3&amp; p) const override {\nauto xInteger = static_cast&lt;int&gt;(std::floor(inv_scale * p.x()));\nauto yInteger = static_cast&lt;int&gt;(std::floor(inv_scale * p.y()));\nauto zInteger = static_cast&lt;int&gt;(std::floor(inv_scale * p.z()));\nbool isEven = (xInteger + yInteger + zInteger) % 2 == 0;\nreturn isEven ? even-&gt;value(u, v, p) : odd-&gt;value(u, v, p);\n}\nprivate:\ndouble inv_scale;\nshared_ptr&lt;texture&gt; even;\nshared_ptr&lt;texture&gt; odd;\n};\n</code></pre> <p>Those checker odd/even parameters can point to a constant texture or to some other procedural texture. This is in the spirit of shader networks introduced by Pat Hanrahan back in the 1980s.</p> <p>If we add this to our <code>random_scene()</code> function\u2019s base sphere:</p> Checkered texture in use<pre><code>...\n#include \"texture.h\"\nvoid random_spheres() {\nhittable_list world;\nauto checker = make_shared&lt;checker_texture&gt;(0.32, color(.2, .3, .1), color(.9, .9, .9));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, make_shared&lt;lambertian&gt;(checker)));\nfor (int a = -11; a &lt; 11; a++) {\n...\n}\n...\n</code></pre> <p>We get:</p> <p></p>"},{"location":"the_next_week/texture_mapping/#rendering-the-solid-checker-texture","title":"Rendering The Solid Checker Texture","text":"<p>We're going to add a second scene to our program, and will add more scenes after that as we progress through this book. To help with this, we'll set up a switch statement to select the desired scene for a given run. It's a crude approach, but we're trying to keep things dead simple and focus on the raytracing. You may want to use a different approach in your own raytracer, such as supporting command-line arguments.</p> <p>Here's what our main.cc looks like after refactoring for our single random spheres scene. Rename <code>main()</code> to <code>random_spheres()</code>, and add a new <code>main()</code> function to call it:</p> Main dispatching to selected scene<pre><code>#include \"rtweekend.h\"\n#include \"camera.h\"\n#include \"color.h\"\n#include \"hittable_list.h\"\n#include \"material.h\"\n#include \"sphere.h\"\nvoid random_spheres() {\nhittable_list world;\nauto ground_material = make_shared&lt;lambertian&gt;(color(0.5, 0.5, 0.5));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-1000,0), 1000, ground_material));\n...\ncam.render(world);\n}\nint main() {\nrandom_spheres();\n}\n</code></pre> <p>Now add a scene with two checkered spheres, one atop the other.</p> Two textured spheres<pre><code>#include \"rtweekend.h\"\n#include \"camera.h\"\n#include \"color.h\"\n#include \"hittable_list.h\"\n#include \"material.h\"\n#include \"sphere.h\"\nvoid random_spheres() {\n...\n}\nvoid two_spheres() {\nhittable_list world;\nauto checker = make_shared&lt;checker_texture&gt;(0.8, color(.2, .3, .1), color(.9, .9, .9));\nworld.add(make_shared&lt;sphere&gt;(point3(0,-10, 0), 10, make_shared&lt;lambertian&gt;(checker)));\nworld.add(make_shared&lt;sphere&gt;(point3(0, 10, 0), 10, make_shared&lt;lambertian&gt;(checker)));\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 20;\ncam.lookfrom = point3(13,2,3);\ncam.lookat   = point3(0,0,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (2) {\ncase 1: random_spheres(); break;\ncase 2: two_spheres();    break;\n}\n}\n</code></pre> <p>We get this result:</p> <p></p> <p>You may think the result looks a bit odd. Since <code>checker_texture</code> is a spatial texture, we're really looking at the surface of the sphere cutting through the three-dimensional checker space. There are many situations where this is perfect, or at least sufficient. In many other situations, we really want to get a consistent effect on the surface of our objects. That approach is covered next.</p>"},{"location":"the_next_week/texture_mapping/#texture-coordinates-for-spheres","title":"Texture Coordinates for Spheres","text":"<p>Constant-color textures use no coordinates. Solid (or spatial) textures use the coordinates of a point in space. Now it's time to make use of the $u,v$ texture coordinates. These coordinates specify the location on 2D source image (or in some 2D parameterized space). To get this, we need a way to find the $u,v$ coordinates of any point on the surface of a 3D object. This mapping is completely arbitrary, but generally you'd like to cover the entire surface, and be able to scale, orient and stretch the 2D image in a way that makes some sense. We'll start with deriving a scheme to get the $u,v$ coordinates of a sphere.</p> <p>For spheres, texture coordinates are usually based on some form of longitude and latitude, i.e., spherical coordinates. So we compute $(\\theta,\\phi)$ in spherical coordinates, where $\\theta$ is the angle up from the bottom pole (that is, up from -Y), and $\\phi$ is the angle around the Y-axis (from -X to +Z to +X to -Z back to -X).</p> <p>We want to map $\\theta$ and $\\phi$ to texture coordinates $u$ and $v$ each in $[0,1]$, where $(u=0,v=0)$ maps to the bottom-left corner of the texture. Thus the normalization from $(\\theta,\\phi)$ to $(u,v)$ would be:</p> <p>$$ u = \\frac{\\phi}{2\\pi} $$   $$ v = \\frac{\\theta}{\\pi} $$</p> <p>To compute $\\theta$ and $\\phi$ for a given point on the unit sphere centered at the origin, we start with the equations for the corresponding Cartesian coordinates:</p> <p>$$ \\begin{align}       y &amp;= -\\cos(\\theta)            \\       x &amp;= -\\cos(\\phi) \\sin(\\theta) \\       z &amp;= \\quad\\sin(\\phi) \\sin(\\theta)      \\end{align}   $$</p> <p>We need to invert these equations to solve for $\\theta$ and $\\phi$. Because of the lovely <code>&lt;cmath&gt;</code> function <code>atan2()</code>, which takes any pair of numbers proportional to sine and cosine and returns the angle, we can pass in $x$ and $z$ (the $\\sin(\\theta)$ cancel) to solve for $\\phi$:</p> <p>$$ \\phi = \\operatorname{atan2}(z, -x) $$</p> <p><code>atan2()</code> returns values in the range $-\\pi$ to $\\pi$, but they go from 0 to $\\pi$, then flip to $-\\pi$ and proceed back to zero. While this is mathematically correct, we want $u$ to range from $0$ to $1$, not from $0$ to $1/2$ and then from $-1/2$ to $0$. Fortunately,</p> <p>$$ \\operatorname{atan2}(a,b) = \\operatorname{atan2}(-a,-b) + \\pi, $$</p> <p>and the second formulation yields values from $0$ continuously to $2\\pi$. Thus, we can compute $\\phi$ as</p> <p>$$ \\phi = \\operatorname{atan2}(-z, x) + \\pi $$</p> <p>The derivation for $\\theta$ is more straightforward:</p> <p>$$ \\theta = \\arccos(-y) $$</p> <p>So for a sphere, the $(u,v)$ coord computation is accomplished by a utility function that takes points on the unit sphere centered at the origin, and computes $u$ and $v$:</p> get_sphere_uv function<pre><code>class sphere : public hittable {\n...\nprivate:\n...\nstatic void get_sphere_uv(const point3&amp; p, double&amp; u, double&amp; v) {\n// p: a given point on the sphere of radius one, centered at the origin.\n// u: returned value [0,1] of angle around the Y axis from X=-1.\n// v: returned value [0,1] of angle from Y=-1 to Y=+1.\n//     &lt;1 0 0&gt; yields &lt;0.50 0.50&gt;       &lt;-1  0  0&gt; yields &lt;0.00 0.50&gt;\n//     &lt;0 1 0&gt; yields &lt;0.50 1.00&gt;       &lt; 0 -1  0&gt; yields &lt;0.50 0.00&gt;\n//     &lt;0 0 1&gt; yields &lt;0.25 0.50&gt;       &lt; 0  0 -1&gt; yields &lt;0.75 0.50&gt;\nauto theta = acos(-p.y());\nauto phi = atan2(-p.z(), p.x()) + pi;\nu = phi / (2*pi);\nv = theta / pi;\n}\n};\n</code></pre> <p>Update the <code>sphere::hit()</code> function to use this function to update the hit record UV coordinates.</p> Sphere UV coordinates from hit<pre><code>class sphere : public hittable {\npublic:\n...\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n...\nrec.t = root;\nrec.p = r.at(rec.t);\nvec3 outward_normal = (rec.p - center) / radius;\nrec.set_face_normal(r, outward_normal);\nget_sphere_uv(outward_normal, rec.u, rec.v);\nrec.mat = mat;\nreturn true;\n}\n...\n};\n</code></pre> <p>Now we can make textured materials by replacing the <code>const color&amp; a</code> with a texture pointer:</p> Lambertian material with texture<pre><code>#include \"texture.h\"\n...\nclass lambertian : public material {\npublic:\nlambertian(const color&amp; a) : albedo(make_shared&lt;solid_color&gt;(a)) {}\nlambertian(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nauto scatter_direction = rec.normal + random_unit_vector();\n// Catch degenerate scatter direction\nif (scatter_direction.near_zero())\nscatter_direction = rec.normal;\nscattered = ray(rec.p, scatter_direction, r_in.time());\nattenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\nreturn true;\n}\nprivate:\nshared_ptr&lt;texture&gt; albedo;\n};\n</code></pre> <p>From the hitpoint $\\mathbf{P}$, we compute the surface coordinates $(u,v)$. We then use these to index into our procedural solid texture (like marble). We can also read in an image and use the 2D $(u,v)$ texture coordinate to index into the image.</p> <p>A direct way to use scaled $(u,v)$ in an image is to round the $u$ and $v$ to integers, and use that as $(i,j)$ pixels. This is awkward, because we don\u2019t want to have to change the code when we change image resolution. So instead, one of the the most universal unofficial standards in graphics is to use texture coordinates instead of image pixel coordinates. These are just some form of fractional position in the image. For example, for pixel $(i,j)$ in an $N_x$ by $N_y$ image, the image texture position is:</p> <p>$$ u = \\frac{i}{N_x-1} $$   $$ v = \\frac{j}{N_y-1} $$</p> <p>This is just a fractional position.</p>"},{"location":"the_next_week/texture_mapping/#accessing-texture-image-data","title":"Accessing Texture Image Data","text":"<p>Now it's time to create a texture class that holds an image. I am going to use my favorite image utility, [stb_image][]. It reads image data into a big array of unsigned chars. These are just packed RGBs with each component in the range [0,255] (black to full white). To help make loading our image files even easier, we provide a helper class to manage all this -- <code>rtw_image</code>. The following listing assumes that you have copied the <code>stb_image.h</code> header into a folder called <code>external</code>. Adjust according to your directory structure.</p> The rtw_image helper class<pre><code>#ifndef RTW_STB_IMAGE_H\n#define RTW_STB_IMAGE_H\n// Disable strict warnings for this header from the Microsoft Visual C++ compiler.\n#ifdef _MSC_VER\n#pragma warning (push, 0)\n#endif\n#define STB_IMAGE_IMPLEMENTATION\n#define STBI_FAILURE_USERMSG\n#include \"external/stb_image.h\"\n#include &lt;cstdlib&gt;\n#include &lt;iostream&gt;\nclass rtw_image {\npublic:\nrtw_image() : data(nullptr) {}\nrtw_image(const char* image_filename) {\n// Loads image data from the specified file. If the RTW_IMAGES environment variable is\n// defined, looks only in that directory for the image file. If the image was not found,\n// searches for the specified image file first from the current directory, then in the\n// images/ subdirectory, then the _parent's_ images/ subdirectory, and then _that_\n// parent, on so on, for six levels up. If the image was not loaded successfully,\n// width() and height() will return 0.\nauto filename = std::string(image_filename);\nauto imagedir = getenv(\"RTW_IMAGES\");\n// Hunt for the image file in some likely locations.\nif (imagedir &amp;&amp; load(std::string(imagedir) + \"/\" + image_filename)) return;\nif (load(filename)) return;\nif (load(\"images/\" + filename)) return;\nif (load(\"../images/\" + filename)) return;\nif (load(\"../../images/\" + filename)) return;\nif (load(\"../../../images/\" + filename)) return;\nif (load(\"../../../../images/\" + filename)) return;\nif (load(\"../../../../../images/\" + filename)) return;\nif (load(\"../../../../../../images/\" + filename)) return;\nstd::cerr &lt;&lt; \"ERROR: Could not load image file '\" &lt;&lt; image_filename &lt;&lt; \"'.\\n\";\n}\n~rtw_image() { STBI_FREE(data); }\nbool load(const std::string filename) {\n// Loads image data from the given file name. Returns true if the load succeeded.\nauto n = bytes_per_pixel; // Dummy out parameter: original components per pixel\ndata = stbi_load(filename.c_str(), &amp;image_width, &amp;image_height, &amp;n, bytes_per_pixel);\nbytes_per_scanline = image_width * bytes_per_pixel;\nreturn data != nullptr;\n}\nint width()  const { return (data == nullptr) ? 0 : image_width; }\nint height() const { return (data == nullptr) ? 0 : image_height; }\nconst unsigned char* pixel_data(int x, int y) const {\n// Return the address of the three bytes of the pixel at x,y (or magenta if no data).\nstatic unsigned char magenta[] = { 255, 0, 255 };\nif (data == nullptr) return magenta;\nx = clamp(x, 0, image_width);\ny = clamp(y, 0, image_height);\nreturn data + y*bytes_per_scanline + x*bytes_per_pixel;\n}\nprivate:\nconst int bytes_per_pixel = 3;\nunsigned char *data;\nint image_width, image_height;\nint bytes_per_scanline;\nstatic int clamp(int x, int low, int high) {\n// Return the value clamped to the range [low, high).\nif (x &lt; low) return low;\nif (x &lt; high) return x;\nreturn high - 1;\n}\n};\n// Restore MSVC compiler warnings\n#ifdef _MSC_VER\n#pragma warning (pop)\n#endif\n#endif\n</code></pre> <p>If you are writing your implementation in a language other than C or C++, you'll need to locate (or write) an image loading library that provides similar functionality.</p> <p>The <code>image_texture</code> class uses the <code>rtw_image</code> class:</p> Image texture class<pre><code>#include \"rtweekend.h\"\n#include \"rtw_stb_image.h\"\n#include \"perlin.h\"\n...\nclass image_texture : public texture {\npublic:\nimage_texture(const char* filename) : image(filename) {}\ncolor value(double u, double v, const point3&amp; p) const override {\n// If we have no texture data, then return solid cyan as a debugging aid.\nif (image.height() &lt;= 0) return color(0,1,1);\n// Clamp input texture coordinates to [0,1] x [1,0]\nu = interval(0,1).clamp(u);\nv = 1.0 - interval(0,1).clamp(v);  // Flip V to image coordinates\nauto i = static_cast&lt;int&gt;(u * image.width());\nauto j = static_cast&lt;int&gt;(v * image.height());\nauto pixel = image.pixel_data(i,j);\nauto color_scale = 1.0 / 255.0;\nreturn color(color_scale*pixel[0], color_scale*pixel[1], color_scale*pixel[2]);\n}\nprivate:\nrtw_image image;\n};\n</code></pre>"},{"location":"the_next_week/texture_mapping/#rendering-the-image-texture","title":"Rendering The Image Texture","text":"<p>I just grabbed a random earth map from the web -- any standard projection will do for our purposes.</p> <p></p> <p>Here's the code to read an image from a file and then assign it to a diffuse material:</p> Using stbi_load() to load an image<pre><code>void earth() {\nauto earth_texture = make_shared&lt;image_texture&gt;(\"earthmap.jpg\");\nauto earth_surface = make_shared&lt;lambertian&gt;(earth_texture);\nauto globe = make_shared&lt;sphere&gt;(point3(0,0,0), 2, earth_surface);\ncamera cam;\ncam.aspect_ratio      = 16.0 / 9.0;\ncam.image_width       = 400;\ncam.samples_per_pixel = 100;\ncam.max_depth         = 50;\ncam.vfov     = 20;\ncam.lookfrom = point3(0,0,12);\ncam.lookat   = point3(0,0,0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(hittable_list(globe));\n}\nint main() {\nswitch (3) {\ncase 1:  random_spheres(); break;\ncase 2:  two_spheres();    break;\ncase 3:  earth();          break;\n}\n}\n</code></pre> <p>We start to see some of the power of all colors being textures -- we can assign any kind of texture to the lambertian material, and lambertian doesn\u2019t need to be aware of it.</p> <p>If the photo comes back with a large cyan sphere in the middle, then <code>stb_image</code> failed to find your Earth map photo. The program will look for the file in the same directory as the executable. Make sure to copy the Earth into your build directory, or rewrite <code>earth()</code> to point somewhere else.</p> <p></p>"},{"location":"the_next_week/volumes/","title":"Volumes","text":"<p>One thing it\u2019s nice to add to a ray tracer is smoke/fog/mist. These are sometimes called volumes or participating media. Another feature that is nice to add is subsurface scattering, which is sort of like dense fog inside an object. This usually adds software architectural mayhem because volumes are a different animal than surfaces, but a cute technique is to make a volume a random surface. A bunch of smoke can be replaced with a surface that probabilistically might or might not be there at every point in the volume. This will make more sense when you see the code.</p>"},{"location":"the_next_week/volumes/#constant-density-mediums","title":"Constant Density Mediums","text":"<p>First, let\u2019s start with a volume of constant density. A ray going through there can either scatter inside the volume, or it can make it all the way through like the middle ray in the figure. More thin transparent volumes, like a light fog, are more likely to have rays like the middle one. How far the ray has to travel through the volume also determines how likely it is for the ray to make it through.</p> <p></p> <p>As the ray passes through the volume, it may scatter at any point. The denser the volume, the more likely that is. The probability that the ray scatters in any small distance $\\Delta L$ is:</p> <p>$$ \\mathit{probability} = C \\cdot \\Delta L $$</p> <p>where $C$ is proportional to the optical density of the volume. If you go through all the differential equations, for a random number you get a distance where the scattering occurs. If that distance is outside the volume, then there is no \u201chit\u201d. For a constant volume we just need the density $C$ and the boundary. I\u2019ll use another hittable for the boundary.</p> <p>The resulting class is:</p> Constant medium class<pre><code>#ifndef CONSTANT_MEDIUM_H\n#define CONSTANT_MEDIUM_H\n#include \"rtweekend.h\"\n#include \"hittable.h\"\n#include \"material.h\"\n#include \"texture.h\"\nclass constant_medium : public hittable {\npublic:\nconstant_medium(shared_ptr&lt;hittable&gt; b, double d, shared_ptr&lt;texture&gt; a)\n: boundary(b), neg_inv_density(-1/d), phase_function(make_shared&lt;isotropic&gt;(a))\n{}\nconstant_medium(shared_ptr&lt;hittable&gt; b, double d, color c)\n: boundary(b), neg_inv_density(-1/d), phase_function(make_shared&lt;isotropic&gt;(c))\n{}\nbool hit(const ray&amp; r, interval ray_t, hit_record&amp; rec) const override {\n// Print occasional samples when debugging. To enable, set enableDebug true.\nconst bool enableDebug = false;\nconst bool debugging = enableDebug &amp;&amp; random_double() &lt; 0.00001;\nhit_record rec1, rec2;\nif (!boundary-&gt;hit(r, interval::universe, rec1))\nreturn false;\nif (!boundary-&gt;hit(r, interval(rec1.t+0.0001, infinity), rec2))\nreturn false;\nif (debugging) std::clog &lt;&lt; \"\\nray_tmin=\" &lt;&lt; rec1.t &lt;&lt; \", ray_tmax=\" &lt;&lt; rec2.t &lt;&lt; '\\n';\nif (rec1.t &lt; ray_t.min) rec1.t = ray_t.min;\nif (rec2.t &gt; ray_t.max) rec2.t = ray_t.max;\nif (rec1.t &gt;= rec2.t)\nreturn false;\nif (rec1.t &lt; 0)\nrec1.t = 0;\nauto ray_length = r.direction().length();\nauto distance_inside_boundary = (rec2.t - rec1.t) * ray_length;\nauto hit_distance = neg_inv_density * log(random_double());\nif (hit_distance &gt; distance_inside_boundary)\nreturn false;\nrec.t = rec1.t + hit_distance / ray_length;\nrec.p = r.at(rec.t);\nif (debugging) {\nstd::clog &lt;&lt; \"hit_distance = \" &lt;&lt;  hit_distance &lt;&lt; '\\n'\n&lt;&lt; \"rec.t = \" &lt;&lt;  rec.t &lt;&lt; '\\n'\n&lt;&lt; \"rec.p = \" &lt;&lt;  rec.p &lt;&lt; '\\n';\n}\nrec.normal = vec3(1,0,0);  // arbitrary\nrec.front_face = true;     // also arbitrary\nrec.mat = phase_function;\nreturn true;\n}\naabb bounding_box() const override { return boundary-&gt;bounding_box(); }\nprivate:\nshared_ptr&lt;hittable&gt; boundary;\ndouble neg_inv_density;\nshared_ptr&lt;material&gt; phase_function;\n};\n#endif\n</code></pre> <p>The scattering function of isotropic picks a uniform random direction:</p> The isotropic class<pre><code>class isotropic : public material {\npublic:\nisotropic(color c) : albedo(make_shared&lt;solid_color&gt;(c)) {}\nisotropic(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nscattered = ray(rec.p, random_unit_vector(), r_in.time());\nattenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\nreturn true;\n}\nprivate:\nshared_ptr&lt;texture&gt; albedo;\n};\n</code></pre> <p>The reason we have to be so careful about the logic around the boundary is we need to make sure this works for ray origins inside the volume. In clouds, things bounce around a lot so that is a common case.</p> <p>In addition, the above code assumes that once a ray exits the constant medium boundary, it will continue forever outside the boundary. Put another way, it assumes that the boundary shape is convex. So this particular implementation will work for boundaries like boxes or spheres, but will not work with toruses or shapes that contain voids. It's possible to write an implementation that handles arbitrary shapes, but we'll leave that as an exercise for the reader.</p>"},{"location":"the_next_week/volumes/#rendering-a-cornell-box-with-smoke-and-fog-boxes","title":"Rendering a Cornell Box with Smoke and Fog Boxes","text":"<p>If we replace the two blocks with smoke and fog (dark and light particles), and make the light bigger (and dimmer so it doesn\u2019t blow out the scene) for faster convergence:</p> Cornell box, with smoke<pre><code>#include \"constant_medium.h\"\n...\nvoid cornell_smoke() {\nhittable_list world;\nauto red   = make_shared&lt;lambertian&gt;(color(.65, .05, .05));\nauto white = make_shared&lt;lambertian&gt;(color(.73, .73, .73));\nauto green = make_shared&lt;lambertian&gt;(color(.12, .45, .15));\nauto light = make_shared&lt;diffuse_light&gt;(color(7, 7, 7));\nworld.add(make_shared&lt;quad&gt;(point3(555,0,0), vec3(0,555,0), vec3(0,0,555), green));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,0), vec3(0,555,0), vec3(0,0,555), red));\nworld.add(make_shared&lt;quad&gt;(point3(113,554,127), vec3(330,0,0), vec3(0,0,305), light));\nworld.add(make_shared&lt;quad&gt;(point3(0,555,0), vec3(555,0,0), vec3(0,0,555), white));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,0), vec3(555,0,0), vec3(0,0,555), white));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,555), vec3(555,0,0), vec3(0,555,0), white));\nshared_ptr&lt;hittable&gt; box1 = box(point3(0,0,0), point3(165,330,165), white);\nbox1 = make_shared&lt;rotate_y&gt;(box1, 15);\nbox1 = make_shared&lt;translate&gt;(box1, vec3(265,0,295));\nshared_ptr&lt;hittable&gt; box2 = box(point3(0,0,0), point3(165,165,165), white);\nbox2 = make_shared&lt;rotate_y&gt;(box2, -18);\nbox2 = make_shared&lt;translate&gt;(box2, vec3(130,0,65));\nworld.add(make_shared&lt;constant_medium&gt;(box1, 0.01, color(0,0,0)));\nworld.add(make_shared&lt;constant_medium&gt;(box2, 0.01, color(1,1,1)));\ncamera cam;\ncam.aspect_ratio      = 1.0;\ncam.image_width       = 600;\ncam.samples_per_pixel = 200;\ncam.max_depth         = 50;\ncam.background        = color(0,0,0);\ncam.vfov     = 40;\ncam.lookfrom = point3(278, 278, -800);\ncam.lookat   = point3(278, 278, 0);\ncam.vup      = vec3(0,1,0);\ncam.defocus_angle = 0;\ncam.render(world);\n}\nint main() {\nswitch (8) {\ncase 1:  random_spheres();     break;\ncase 2:  two_spheres();        break;\ncase 3:  earth();              break;\ncase 4:  two_perlin_spheres(); break;\ncase 5:  quads();              break;\ncase 6:  simple_light();       break;\ncase 7:  cornell_box();        break;\ncase 8:  cornell_smoke();      break;\n}\n}\n</code></pre> <p>We get:</p> <p></p>"},{"location":"the_rest_of_your_life/a_simple_monte_carlo_program/","title":"A Simple Monte Carlo Program","text":"<p>Let\u2019s start with one of the simplest Monte Carlo programs. If you're not familiar with Monte Carlo programs, then it'll be good to pause and catch you up. There are two kinds of randomized algorithms: Monte Carlo and Las Vegas. Randomized algorithms can be found everywhere in computer graphics, so getting a decent foundation isn't a bad idea. A randomized algorithm uses some amount of randomness in its computation. A Las Vegas (LV) random algorithm always produces the correct result, whereas a Monte Carlo (MC) algorithm may produce a correct result--and frequently gets it wrong! But for especially complicated problems such as ray tracing, we may not place as huge a priority on being perfectly exact as on getting an answer in a reasonable amount of time. LV algorithms will eventually arrive at the correct result, but we can't make too many guarantees on how long it will take to get there. The classic example of an LV algorithm is the quicksort sorting algorithm. The quicksort algorithm will always complete with a fully sorted list, but, the time it takes to complete is random. Another good example of an LV algorithm is the code that we use to pick a random point in a unit sphere:</p> A Las Vegas algorithm<pre><code>inline vec3 random_in_unit_sphere() {\nwhile (true) {\nauto p = vec3::random(-1,1);\nif (p.length_squared() &lt; 1)\nreturn p;\n}\n}\n</code></pre> <p>This code will always eventually arrive at a random point in the unit sphere, but we can't say beforehand how long it'll take. It may take only 1 iteration, it may take 2, 3, 4, or even longer. Whereas, an MC program will give a statistical estimate of an answer, and this estimate will get more and more accurate the longer you run it. Which means that at a certain point, we can just decide that the answer is accurate enough and call it quits. This basic characteristic of simple programs producing noisy but ever-better answers is what MC is all about, and is especially good for applications like graphics where great accuracy is not needed.</p>"},{"location":"the_rest_of_your_life/a_simple_monte_carlo_program/#estimating-pi","title":"Estimating Pi","text":"<p>The canonical example of a Monte Carlo algorithm is estimating $\\pi$, so let's do that. There are many ways to estimate $\\pi$, with the Buffon Needle problem being a classic case study. We\u2019ll do a variation inspired by this method. Suppose you have a circle inscribed inside a square:</p> <p></p> <p>Now, suppose you pick random points inside the square. The fraction of those random points that end up inside the circle should be proportional to the area of the circle. The exact fraction should in fact be the ratio of the circle area to the square area:</p> <p>$$ \\frac{\\pi r^2}{(2r)^2} = \\frac{\\pi}{4} $$</p> <p>Since the $r$ cancels out, we can pick whatever is computationally convenient. Let\u2019s go with $r=1$, centered at the origin:</p> Estimating \u03c0<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\nint main() {\nint N = 100000;\nint inside_circle = 0;\nfor (int i = 0; i &lt; N; i++) {\nauto x = random_double(-1,1);\nauto y = random_double(-1,1);\nif (x*x + y*y &lt; 1)\ninside_circle++;\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"Estimate of Pi = \" &lt;&lt; (4.0 * inside_circle) / N &lt;&lt; '\\n';\n}\n</code></pre> <p>The answer of $\\pi$ found will vary from computer to computer based on the initial random seed. On my computer, this gives me the answer <code>Estimate of Pi = 3.143760000000</code>.</p>"},{"location":"the_rest_of_your_life/a_simple_monte_carlo_program/#showing-convergence","title":"Showing Convergence","text":"<p>If we change the program to run forever and just print out a running estimate:</p> Estimating \u03c0, v2<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\nint main() {\nint inside_circle = 0;\nint runs = 0;\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nwhile (true) {\nruns++;\nauto x = random_double(-1,1);\nauto y = random_double(-1,1);\nif (x*x + y*y &lt; 1)\ninside_circle++;\nif (runs % 100000 == 0)\nstd::cout &lt;&lt; \"Estimate of Pi = \"\n&lt;&lt; (4.0 * inside_circle) / runs\n&lt;&lt; '\\n';\n}\n}\n</code></pre>"},{"location":"the_rest_of_your_life/a_simple_monte_carlo_program/#stratified-samples-jittering","title":"Stratified Samples (Jittering)","text":"<p>We get very quickly near $\\pi$, and then more slowly zero in on it. This is an example of the Law of Diminishing Returns, where each sample helps less than the last. This is the worst part of Monte Carlo. We can mitigate this diminishing return by stratifying the samples (often called jittering), where instead of taking random samples, we take a grid and take one sample within each:</p> <p></p> <p>This changes the sample generation, but we need to know how many samples we are taking in advance because we need to know the grid. Let\u2019s take a million and try it both ways:</p> Estimating \u03c0, v3<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\nint main() {\nint inside_circle = 0;\nint inside_circle_stratified = 0;\nint sqrt_N = 1000;\nfor (int i = 0; i &lt; sqrt_N; i++) {\nfor (int j = 0; j &lt; sqrt_N; j++) {\nauto x = random_double(-1,1);\nauto y = random_double(-1,1);\nif (x*x + y*y &lt; 1)\ninside_circle++;\nx = 2*((i + random_double()) / sqrt_N) - 1;\ny = 2*((j + random_double()) / sqrt_N) - 1;\nif (x*x + y*y &lt; 1)\ninside_circle_stratified++;\n}\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout\n&lt;&lt; \"Regular    Estimate of Pi = \"\n&lt;&lt; (4.0 * inside_circle) / (sqrt_N*sqrt_N) &lt;&lt; '\\n'\n&lt;&lt; \"Stratified Estimate of Pi = \"\n&lt;&lt; (4.0 * inside_circle_stratified) / (sqrt_N*sqrt_N) &lt;&lt; '\\n';\n}\n</code></pre> <p>On my computer, I get:</p> <pre><code>Regular    Estimate of Pi = 3.141184000000\nStratified Estimate of Pi = 3.141460000000\n</code></pre> <p>Where the first 12 decimal places of pi are:</p> <pre><code>3.141592653589\n</code></pre> <p>Interestingly, the stratified method is not only better, it converges with a better asymptotic rate! Unfortunately, this advantage decreases with the dimension of the problem (so for example, with the 3D sphere volume version the gap would be less). This is called the Curse of Dimensionality. Ray tracing is a very high-dimensional algorithm, where each reflection adds two new dimensions: $\\phi_o$ and $\\theta_o$. We won't be stratifying the output reflection angle in this book, simply because it is a little bit too complicated to cover, but there is a lot of interesting research currently happening in this space.</p> <p>As an intermediary, we'll be stratifying the locations of the sampling positions around each pixel location.</p> Stratifying the samples inside pixels<pre><code>#include \"rtweekend.h\"\n#include \"camera.h\"\n#include \"color.h\"\n#include \"hittable_list.h\"\n#include \"material.h\"\n#include \"quad.h\"\n#include \"sphere.h\"\nint main() {\nhittable_list world;\nauto red   = make_shared&lt;lambertian&gt;(color(.65, .05, .05));\nauto white = make_shared&lt;lambertian&gt;(color(.73, .73, .73));\nauto green = make_shared&lt;lambertian&gt;(color(.12, .45, .15));\nauto light = make_shared&lt;diffuse_light&gt;(color(15, 15, 15));\n// Cornell box sides\nworld.add(make_shared&lt;quad&gt;(point3(555,0,0), vec3(0,0,555), vec3(0,555,0), green));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,555), vec3(0,0,-555), vec3(0,555,0), red));\nworld.add(make_shared&lt;quad&gt;(point3(0,555,0), vec3(555,0,0), vec3(0,0,555), white));\nworld.add(make_shared&lt;quad&gt;(point3(0,0,555), vec3(555,0,0), vec3(0,0,-555), white));\nworld.add(make_shared&lt;quad&gt;(point3(555,0,555), vec3(-555,0,0), vec3(0,555,0), white));\n// Light\nworld.add(make_shared&lt;quad&gt;(point3(213,554,227), vec3(130,0,0), vec3(0,0,105), light));\n// Box 1\nshared_ptr&lt;hittable&gt; box1 = box(point3(0,0,0), point3(165,330,165), white);\nbox1 = make_shared&lt;rotate_y&gt;(box1, 15);\nbox1 = make_shared&lt;translate&gt;(box1, vec3(265,0,295));\nworld.add(box1);\n// Box 2\nshared_ptr&lt;hittable&gt; box2 = box(point3(0,0,0), point3(165,165,165), white);\nbox2 = make_shared&lt;rotate_y&gt;(box2, -18);\nbox2 = make_shared&lt;translate&gt;(box2, vec3(130,0,65));\nworld.add(box2);\ncamera cam;\ncam.aspect_ratio      = 1.0;\ncam.image_width       = 600;\ncam.samples_per_pixel = 64;\ncam.max_depth         = 50;\ncam.background        = color(0,0,0);\ncam.vfov     = 40;\ncam.lookfrom = point3(278, 278, -800);\ncam.lookat   = point3(278, 278, 0);\ncam.vup      = vec3(0, 1, 0);\ncam.defocus_angle = 0;\ncam.render(world, lights);\n}\n</code></pre> Stratifying the samples inside pixels (render)<pre><code>class camera {\npublic:\n...\nvoid render(const hittable&amp; world) {\ninitialize();\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\ncolor pixel_color(0,0,0);\nfor (int s_j = 0; s_j &lt; sqrt_spp; ++s_j) {\nfor (int s_i = 0; s_i &lt; sqrt_spp; ++s_i) {\nray r = get_ray(i, j, s_i, s_j);\npixel_color += ray_color(r, max_depth, world);\n}\n}\nwrite_color(std::cout, pixel_color, samples_per_pixel);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n...\nprivate:\n...\nray get_ray(int i, int j, int s_i, int s_j) const {\n// Get a randomly-sampled camera ray for the pixel at location i,j, originating from\n// the camera defocus disk, and randomly sampled around the pixel location.\nauto pixel_center = pixel00_loc + (i * pixel_delta_u) + (j * pixel_delta_v);\nauto pixel_sample = pixel_center + pixel_sample_square(s_i, s_j);\nauto ray_origin = (defocus_angle &lt;= 0) ? center : defocus_disk_sample();\nauto ray_direction = pixel_sample - ray_origin;\nauto ray_time = random_double();\nreturn ray(ray_origin, ray_direction, ray_time);\n}\nvec3 pixel_sample_square(int s_i, int s_j) const {\n// Returns a random point in the square surrounding a pixel at the origin, given\n// the two subpixel indices.\nauto px = -0.5 + recip_sqrt_spp * (s_i + random_double());\nauto py = -0.5 + recip_sqrt_spp * (s_j + random_double());\nreturn (px * pixel_delta_u) + (py * pixel_delta_v);\n}\n...\n};\n</code></pre> <p>If we compare the results from without stratification:</p> <p></p> <p>To after, with stratification:</p> <p></p> <p>You should, if you squint, be able to see sharper contrast at the edges of planes and at the edges of boxes. The effect will be more pronounced at locations that have a higher frequency of change. High frequency change can also be thought of as high information density. For our cornell box scene, all of our materials are matte, with a soft area light overhead, so the only locations of high information density are at the edges of objects. The effect will be more obvious with textures and reflective materials.</p> <p>If you are ever doing single-reflection or shadowing or some strictly 2D problem, you definitely want to stratify.</p>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/","title":"Cleaning Up PDF Management","text":"<p>So far I have the <code>ray_color()</code> function create two hard-coded PDFs:</p> <ol> <li><code>p0()</code> related to the shape of the light</li> <li><code>p1()</code> related to the normal vector and type of surface</li> </ol> <p>We can pass information about the light (or whatever <code>hittable</code> we want to sample) into the <code>ray_color()</code> function, and we can ask the <code>material</code> function for a PDF (we would have to add instrumentation to do that). We also need to know if the scattered ray is specular, and we can do this either by asking the <code>hit()</code> function or the <code>material</code> class.</p>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/#diffuse-versus-specular","title":"Diffuse Versus Specular","text":"<p>One thing we would like to allow for is a material -- like varnished wood -- that is partially ideal specular (the polish) and partially diffuse (the wood). Some renderers have the material generate two rays: one specular and one diffuse. I am not fond of branching, so I would rather have the material randomly decide whether it is diffuse or specular. The catch with that approach is that we need to be careful when we ask for the PDF value, and <code>ray_color()</code> needs to be aware of whether this ray is diffuse or specular. Fortunately, we have decided that we should only call the <code>pdf_value()</code> if it is diffuse, so we can handle that implicitly.</p> <p>We can redesign <code>material</code> and stuff all the new arguments into a class like we did for <code>hittable</code>:</p> Refactoring the material class<pre><code>class scatter_record {\npublic:\ncolor attenuation;\nshared_ptr&lt;pdf&gt; pdf_ptr;\nbool skip_pdf;\nray skip_pdf_ray;\n};\nclass material {\npublic:\n...\nvirtual bool scatter(\nconst ray&amp; r_in, const hit_record&amp; rec, scatter_record&amp; srec\n) const {\nreturn false;\n}\n...\n};\n</code></pre> <p>The <code>lambertian</code> material becomes simpler:</p> New lambertian scatter() method<pre><code>class lambertian : public material {\npublic:\nlambertian(const color&amp; a) : albedo(make_shared&lt;solid_color&gt;(a)) {}\nlambertian(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, scatter_record&amp; srec) const override {\nsrec.attenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\nsrec.pdf_ptr = make_shared&lt;cosine_pdf&gt;(rec.normal);\nsrec.skip_pdf = false;\nreturn true;\n}\ndouble scattering_pdf(const ray&amp; r_in, const hit_record&amp; rec, const ray&amp; scattered) const {\nauto cosine = dot(rec.normal, unit_vector(scattered.direction()));\nreturn cosine &lt; 0 ? 0 : cosine/pi;\n}\nprivate:\nshared_ptr&lt;texture&gt; albedo;\n};\n</code></pre> <p>As does the <code>isotropic</code> material:</p> New isotropic scatter() method<pre><code>class isotropic : public material {\npublic:\nisotropic(color c) : albedo(make_shared&lt;solid_color&gt;(c)) {}\nisotropic(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, scatter_record&amp; srec) const override {\nsrec.attenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\nsrec.pdf_ptr = make_shared&lt;sphere_pdf&gt;();\nsrec.skip_pdf = false;\nreturn true;\n}\ndouble scattering_pdf(const ray&amp; r_in, const hit_record&amp; rec, const ray&amp; scattered)\nconst override {\nreturn 1 / (4 * pi);\n}\nprivate:\nshared_ptr&lt;texture&gt; albedo;\n};\n</code></pre> <p>And <code>ray_color()</code> changes are small:</p> The ray_color function, using mixture PDF<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world, const hittable&amp; lights)\nconst {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\n// If the ray hits nothing, return the background color.\nif (!world.hit(r, interval(0.001, infinity), rec))\nreturn background;\nscatter_record srec;\ncolor color_from_emission = rec.mat-&gt;emitted(r, rec, rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, srec))\nreturn color_from_emission;\nauto light_ptr = make_shared&lt;hittable_pdf&gt;(lights, rec.p);\nmixture_pdf p(light_ptr, srec.pdf_ptr);\nray scattered = ray(rec.p, p.generate(), r.time());\nauto pdf_val = p.value(scattered.direction());\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ncolor sample_color = ray_color(scattered, depth-1, world, lights);\ncolor color_from_scatter = (srec.attenuation * scattering_pdf * sample_color) / pdf_val;\nreturn color_from_emission + color_from_scatter;\n}\n};\n</code></pre>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/#handling-specular","title":"Handling Specular","text":"<p>We have not yet dealt with specular surfaces, nor instances that mess with the surface normal. But this design is clean overall, and those are all fixable. For now, I will just fix <code>specular</code>. Metal and dielectric materials are easy to fix.</p> The metal and dielectric scatter methods<pre><code>class metal : public material {\npublic:\nmetal(const color&amp; a, double f) : albedo(a), fuzz(f &lt; 1 ? f : 1) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, scatter_record&amp; srec) const override {\nsrec.attenuation = albedo;\nsrec.pdf_ptr = nullptr;\nsrec.skip_pdf = true;\nvec3 reflected = reflect(unit_vector(r_in.direction()), rec.normal);\nsrec.skip_pdf_ray =\nray(rec.p, reflected + fuzz*random_in_unit_sphere(), r_in.time());\nreturn true;\n}\nprivate:\ncolor albedo;\ndouble fuzz;\n};\n...\nclass dielectric : public material {\npublic:\ndielectric(double index_of_refraction) : ir(index_of_refraction) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, scatter_record&amp; srec) const override {\nsrec.attenuation = color(1.0, 1.0, 1.0);\nsrec.pdf_ptr = nullptr;\nsrec.skip_pdf = true;\ndouble refraction_ratio = rec.front_face ? (1.0/ir) : ir;\nvec3 unit_direction = unit_vector(r_in.direction());\ndouble cos_theta = fmin(dot(-unit_direction, rec.normal), 1.0);\ndouble sin_theta = sqrt(1.0 - cos_theta*cos_theta);\nbool cannot_refract = refraction_ratio * sin_theta &gt; 1.0;\nvec3 direction;\nif (cannot_refract || reflectance(cos_theta, refraction_ratio) &gt; random_double())\ndirection = reflect(unit_direction, rec.normal);\nelse\ndirection = refract(unit_direction, rec.normal, refraction_ratio);\nsrec.skip_pdf_ray = ray(rec.p, direction, r_in.time());\nreturn true;\n}\n...\n};\n</code></pre> <p>Note that if the fuzziness is nonzero, this surface isn\u2019t really ideally specular, but the implicit sampling works just like it did before. We're effectively skipping all of our PDF work for the materials that we're treating specularly.</p> <p><code>ray_color()</code> just needs a new case to generate an implicitly sampled ray:</p> Ray color function with implicitly-sampled rays<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world, const hittable&amp; lights)\nconst {\n...\nif (!rec.mat-&gt;scatter(r, rec, srec))\nreturn color_from_emission;\nif (srec.skip_pdf) {\nreturn srec.attenuation * ray_color(srec.skip_pdf_ray, depth-1, world, lights);\n}\nauto light_ptr = make_shared&lt;hittable_pdf&gt;(lights, rec.p);\nmixture_pdf p(light_ptr, srec.pdf_ptr);\n...\n}\n};\n</code></pre> <p>We'll check our work by changing a block to metal. We'd also like to swap out one of the blocks for a glass object, but we'll push that off for the next section. Glass objects are difficult to render well, so we'd like to make a PDF for them, but we have some more work to do before we're able to do that.</p> Cornell box scene with aluminum material<pre><code>int main() {\n...\n// Light\nworld.add(make_shared&lt;quad&gt;(point3(213,554,227), vec3(130,0,0), vec3(0,0,105), light));\n// Box 1\nshared_ptr&lt;material&gt; aluminum = make_shared&lt;metal&gt;(color(0.8, 0.85, 0.88), 0.0);\nshared_ptr&lt;hittable&gt; box1 = box(point3(0,0,0), point3(165,330,165), aluminum);\nbox1 = make_shared&lt;rotate_y&gt;(box1, 15);\nbox1 = make_shared&lt;translate&gt;(box1, vec3(265,0,295));\nworld.add(box1);\n// Box 2\nshared_ptr&lt;hittable&gt; box2 = box(point3(0,0,0), point3(165,165,165), white);\nbox2 = make_shared&lt;rotate_y&gt;(box2, -18);\nbox2 = make_shared&lt;translate&gt;(box2, vec3(130,0,65));\nworld.add(box2);\n// Light Sources\nhittable_list lights;\nauto m = shared_ptr&lt;material&gt;();\nlights.add(make_shared&lt;quad&gt;(point3(343,554,332), vec3(-130,0,0), vec3(0,0,-105), m));\n...\n}\n</code></pre> <p>The resulting image has a noisy reflection on the ceiling because the directions toward the box are not sampled with more density.</p> <p></p>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/#sampling-a-sphere-object","title":"Sampling a Sphere Object","text":"<p>The noisiness on the ceiling could be reduced by making a PDF of the metal block. We would also want a PDF for the block if we made it glass. But making a PDF for a block is quite a bit of work and isn't terribly interesting, so let\u2019s create a PDF for a glass sphere instead. It's quicker and makes for a more interesting render. We need to figure out how to sample a sphere to determine an appropriate PDF distribution. If we want to sample a sphere from a point outside of the sphere, we can't just pick a random point on its surface and be done. If we did that, we would frequently pick a point on the far side of the sphere, which would be occluded by the front side of the sphere. We need a way to uniformly sample the side of the sphere that is visible from an arbitrary point. When we sample a sphere\u2019s solid angle uniformly from a point outside the sphere, we are really just sampling a cone uniformly. The cone axis goes from the ray origin through the sphere center, with the sides of the cone tangent to the sphere -- see illustration below. Let\u2019s say the code has <code>theta_max</code>. Recall from the Generating Random Directions chapter that to sample $\\theta$ we have:</p> <p>$$ r_2 = \\int_{0}^{\\theta} 2 \\pi f(\\theta') \\sin(\\theta') d\\theta' $$</p> <p>Here $f(\\theta')$ is an as-of-yet uncalculated constant $C$, so:</p> <p>$$ r_2 = \\int_{0}^{\\theta} 2 \\pi C \\sin(\\theta') d\\theta' $$</p> <p>If we solve through the calculus:</p> <p>$$ r_2 = 2\\pi \\cdot C \\cdot (1-\\cos(\\theta)) $$</p> <p>So</p> <p>$$ cos(\\theta) = 1 - \\frac{r_2}{2 \\pi \\cdot C} $$</p> <p>We are constraining our distribution so that the random direction must be less than $\\theta_{max}$. This means that the integral from 0 to $\\theta_{max}$ must be one, and therefore $r_2 = 1$. We can use this to solve for $C$:</p> <p>$$ r_2 = 2\\pi \\cdot C \\cdot (1-\\cos(\\theta)) $$   $$ 1 = 2\\pi \\cdot C \\cdot (1-\\cos(\\theta_{max})) $$   $$ C = \\frac{1}{2\\pi \\cdot (1-\\cos(\\theta_{max})} $$</p> <p>Which gives us an equality between $\\theta$, $\\theta_{max}$, and $r_2$:</p> <p>$$ \\cos(\\theta) = 1 + r_2 \\cdot (\\cos(\\theta_{max})-1) $$</p> <p>We sample $\\phi$ like before, so:</p> <p>$$ z = \\cos(\\theta) = 1 + r_2 \\cdot (\\cos(\\theta_{max}) - 1) $$   $$ x = \\cos(\\phi) \\cdot \\sin(\\theta) = \\cos(2\\pi \\cdot r_1) \\cdot \\sqrt{1-z^2} $$   $$ y = \\sin(\\phi) \\cdot \\sin(\\theta) = \\sin(2\\pi \\cdot r_1) \\cdot \\sqrt{1-z^2} $$</p> <p>Now what is $\\theta_{max}$?</p> <p></p> <p>We can see from the figure that $\\sin(\\theta_{max}) = R / length(\\mathbf{c} - \\mathbf{p})$. So:</p> <p>$$ \\cos(\\theta_{max}) = \\sqrt{1 - \\frac{R^2}{length^2(\\mathbf{c} - \\mathbf{p})}} $$</p> <p>We also need to evaluate the PDF of directions. For a uniform distribution toward the sphere the PDF is $1/\\mathit{solid_angle}$. What is the solid angle of the sphere? It has something to do with the $C$ above. It is -- by definition -- the area on the unit sphere, so the integral is</p> <p>$$ \\mathit{solid angle} = \\int_{0}^{2\\pi} \\int_{0}^{\\theta_{max}} \\sin(\\theta)        = 2 \\pi \\cdot (1-\\cos(\\theta_{max})) $$</p> <p>It\u2019s good to check the math on all such calculations. I usually plug in the extreme cases (thank you for that concept, Mr. Horton -- my high school physics teacher). For a zero radius sphere $\\cos(\\theta_{max}) = 0$, and that works. For a sphere tangent at $\\mathbf{p}$, $\\cos(\\theta_{max}) = 0$, and $2\\pi$ is the area of a hemisphere, so that works too.</p>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/#updating-the-sphere-code","title":"Updating the Sphere Code","text":"<p>The sphere class needs the two PDF-related functions:</p> Sphere with PDF<pre><code>class sphere : public hittable {\npublic:\n...\ndouble pdf_value(const point3&amp; o, const vec3&amp; v) const override {\n// This method only works for stationary spheres.\nhit_record rec;\nif (!this-&gt;hit(ray(o, v), interval(0.001, infinity), rec))\nreturn 0;\nauto cos_theta_max = sqrt(1 - radius*radius/(center1 - o).length_squared());\nauto solid_angle = 2*pi*(1-cos_theta_max);\nreturn  1 / solid_angle;\n}\nvec3 random(const point3&amp; o) const override {\nvec3 direction = center1 - o;\nauto distance_squared = direction.length_squared();\nonb uvw;\nuvw.build_from_w(direction);\nreturn uvw.local(random_to_sphere(radius, distance_squared));\n}\nprivate:\n...\nstatic vec3 random_to_sphere(double radius, double distance_squared) {\nauto r1 = random_double();\nauto r2 = random_double();\nauto z = 1 + r2*(sqrt(1-radius*radius/distance_squared) - 1);\nauto phi = 2*pi*r1;\nauto x = cos(phi)*sqrt(1-z*z);\nauto y = sin(phi)*sqrt(1-z*z);\nreturn vec3(x, y, z);\n}\n};\n</code></pre> <p>We can first try just sampling the sphere rather than the light:</p> Sampling just the sphere<pre><code>int main() {\n...\n// Light\nworld.add(make_shared&lt;quad&gt;(point3(213,554,227), vec3(130,0,0), vec3(0,0,105), light));\n// Box\nshared_ptr&lt;hittable&gt; box1 = box(point3(0,0,0), point3(165,330,165), white);\nbox1 = make_shared&lt;rotate_y&gt;(box1, 15);\nbox1 = make_shared&lt;translate&gt;(box1, vec3(265,0,295));\nworld.add(box1);\n// Glass Sphere\nauto glass = make_shared&lt;dielectric&gt;(1.5);\nworld.add(make_shared&lt;sphere&gt;(point3(190,90,190), 90, glass));\n// Light Sources\nhittable_list lights;\nauto m = shared_ptr&lt;material&gt;();\nlights.add(make_shared&lt;quad&gt;(point3(343,554,332), vec3(-130,0,0), vec3(0,0,-105), m));\n...\n}\n</code></pre> <p>This yields a noisy room, but the caustic under the sphere is good. It took five times as long as sampling the light did for my code. This is probably because those rays that hit the glass are expensive!</p> <p></p>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/#adding-pdf-functions-to-hittable-lists","title":"Adding PDF Functions to Hittable Lists","text":"<p>We should probably just sample both the sphere and the light. We can do that by creating a mixture density of their two distributions. We could do that in the <code>ray_color()</code> function by passing a list of hittables in and building a mixture PDF, or we could add PDF functions to <code>hittable_list</code>. I think both tactics would work fine, but I will go with instrumenting <code>hittable_list</code>.</p> Creating a mixture of densities<pre><code>class hittable_list : public hittable {\npublic:\n...\ndouble pdf_value(const point3&amp; o, const vec3&amp; v) const override {\nauto weight = 1.0/objects.size();\nauto sum = 0.0;\nfor (const auto&amp; object : objects)\nsum += weight * object-&gt;pdf_value(o, v);\nreturn sum;\n}\nvec3 random(const vec3&amp; o) const override {\nauto int_size = static_cast&lt;int&gt;(objects.size());\nreturn objects[random_int(0, int_size-1)]-&gt;random(o);\n}\n...\n};\n</code></pre> <p>We assemble a list to pass to <code>render()</code> from <code>main()</code>:</p> Updating the scene<pre><code>int main() {\n...\n// Light Sources\nhittable_list lights;\nauto m = shared_ptr&lt;material&gt;();\nlights.add(make_shared&lt;quad&gt;(point3(343,554,332), vec3(-130,0,0), vec3(0,0,-105), m));\nlights.add(make_shared&lt;sphere&gt;(point3(190, 90, 190), 90, m));\n...\n}\n</code></pre> <p>And we get a decent image with 1000 samples as before:</p> <p></p>"},{"location":"the_rest_of_your_life/cleaning_up_PDF_management/#handling-surface-acne","title":"Handling Surface Acne","text":"<p>An astute reader pointed out there are some black specks in the image above. All Monte Carlo Ray Tracers have this as a main loop:</p> <pre><code>pixel_color = average(many many samples)\n</code></pre> <p>If you find yourself getting some form of acne in your renders, and this acne is white or black -- where one \"bad\" sample seems to kill the whole pixel -- then that sample is probably a huge number or a <code>NaN</code> (Not A Number). This particular acne is probably a <code>NaN</code>. Mine seems to come up once in every 10\u2013100 million rays or so.</p> <p>So big decision: sweep this bug under the rug and check for <code>NaN</code>s, or just kill <code>NaN</code>s and hope this doesn't come back to bite us later. I will always opt for the lazy strategy, especially when I know that working with floating point is hard. First, how do we check for a <code>NaN</code>? The one thing I always remember for <code>NaN</code>s is that a <code>NaN</code> does not equal itself. Using this trick, we update the <code>write_color()</code> function to replace any <code>NaN</code> components with zero:</p> NaN-tolerant write_color function<pre><code>void write_color(std::ostream &amp;out, color pixel_color, int samples_per_pixel) {\nauto r = pixel_color.x();\nauto g = pixel_color.y();\nauto b = pixel_color.z();\n// Replace NaN components with zero.\nif (r != r) r = 0.0;\nif (g != g) g = 0.0;\nif (b != b) b = 0.0;\n// Divide the color by the number of samples and gamma-correct for gamma=2.0.\nauto scale = 1.0 / samples_per_pixel;\nr = sqrt(scale * r);\ng = sqrt(scale * g);\nb = sqrt(scale * b);\n// Write the translated [0,255] value of each color component.\nstatic const interval intensity(0.000, 0.999);\nout &lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(r)) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(g)) &lt;&lt; ' '\n&lt;&lt; static_cast&lt;int&gt;(256 * intensity.clamp(b)) &lt;&lt; '\\n';\n}\n</code></pre> <p>Happily, the black specks are gone:</p> <p></p>"},{"location":"the_rest_of_your_life/generating_random_directions/","title":"Generating Random Directions","text":"<p>In this and the next two chapters, we'll harden our understanding and our tools.</p>"},{"location":"the_rest_of_your_life/generating_random_directions/#random-directions-relative-to-the-z-axis","title":"Random Directions Relative to the Z Axis","text":"<p>Let\u2019s first figure out how to generate random directions. We already have a method to generate random directions using the rejection method, so let's create one using the inversion method. To simplify things, assume the $z$ axis is the surface normal, and $\\theta$ is the angle from the normal. We'll set everything up in terms of the $z$ axis this chapter. Next chapter we\u2019ll get them oriented to the surface normal vector. We will only deal with distributions that are rotationally symmetric about $z$. So $p(\\omega) = f(\\theta)$.</p> <p>Given a directional PDF on the sphere (where $p(\\omega) = f(\\theta)$), the one dimensional PDFs on $\\theta$ and $\\phi$ are:</p> <pre><code>$$ a(\\phi) = \\frac{1}{2\\pi} $$\n$$ b(\\theta) = 2\\pi f(\\theta)\\sin(\\theta) $$\n</code></pre> <p>For uniform random numbers $r_1$ and $r_2$, we solve for the CDF of $\\theta$ and $\\phi$ so that we can invert the CDF to derive the random number generator.</p> <pre><code>$$ r_1 = \\int_{0}^{\\phi} a(\\phi') d\\phi' $$\n$$ = \\int_{0}^{\\phi} \\frac{1}{2\\pi} d\\phi' $$\n$$ = \\frac{\\phi}{2\\pi} $$\n</code></pre> <p>Invert to solve for $\\phi$:</p> <pre><code>$$ \\phi = 2 \\pi \\cdot r_1 $$\n</code></pre> <p>This should match with your intuition. To solve for a random $\\phi$ you can take a uniform random number in the interval [0,1] and multiply by $2\\pi$ to cover the full range of all possible $\\phi$ values, which is just [0,$2\\pi$]. You may not have a fully formed intuition for how to solve for a random value of $\\theta$, so let's walk through the math to help you get set up. We rewrite $\\phi$ as $\\phi'$ and $\\theta$ as $\\theta'$ just like before, as a formality. For $\\theta$ we have:</p> <pre><code>$$ r_2 = \\int_{0}^{\\theta} b(\\theta') d\\theta' $$\n$$ = \\int_{0}^{\\theta} 2 \\pi f(\\theta') \\sin(\\theta') d\\theta' $$\n</code></pre> <p>Let\u2019s try some different functions for $f()$. Let\u2019s first try a uniform density on the sphere. The area of the unit sphere is $4\\pi$, so a uniform $p(\\omega) = \\frac{1}{4\\pi}$ on the unit sphere.</p> <pre><code>$$ r_2 = \\int_{0}^{\\theta} 2 \\pi \\frac{1}{4\\pi} \\sin(\\theta') d\\theta' $$\n$$ = \\int_{0}^{\\theta} \\frac{1}{2} \\sin(\\theta') d\\theta' $$\n$$ = \\frac{-\\cos(\\theta)}{2} - \\frac{-\\cos(0)}{2} $$\n$$ = \\frac{1 - \\cos(\\theta)}{2} $$\n</code></pre> <p>Solving for $\\cos(\\theta)$ gives:</p> <pre><code>$$ \\cos(\\theta) = 1 - 2 r_2 $$\n</code></pre> <p>We don\u2019t solve for theta because we probably only need to know $\\cos(\\theta)$ anyway, and don\u2019t want needless $\\arccos()$ calls running around.</p> <p>To generate a unit vector direction toward $(\\theta,\\phi)$ we convert to Cartesian coordinates:</p> <pre><code>$$ x = \\cos(\\phi) \\cdot \\sin(\\theta) $$\n$$ y = \\sin(\\phi) \\cdot \\sin(\\theta) $$\n$$ z = \\cos(\\theta) $$\n</code></pre> <p>And using the identity $\\cos^2 + \\sin^2 = 1$, we get the following in terms of random $(r_1,r_2)$:</p> <pre><code>$$ x = \\cos(2\\pi \\cdot r_1)\\sqrt{1 - (1-2 r_2)^2} $$\n$$ y = \\sin(2\\pi \\cdot r_1)\\sqrt{1 - (1-2 r_2)^2} $$\n$$ z = 1 - 2  r_2 $$\n</code></pre> <p>Simplifying a little, $(1 - 2 r_2)^2 = 1 - 4r_2 + 4r_2^2$, so:</p> <pre><code>$$ x = \\cos(2 \\pi r_1) \\cdot 2 \\sqrt{r_2(1 - r_2)} $$\n$$ y = \\sin(2 \\pi r_1) \\cdot 2 \\sqrt{r_2(1 - r_2)} $$\n$$ z = 1 - 2 r_2 $$\n</code></pre> <p>We can output some of these:</p> Random points on the unit sphere<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;math.h&gt;\nint main() {\nfor (int i = 0; i &lt; 200; i++) {\nauto r1 = random_double();\nauto r2 = random_double();\nauto x = cos(2*pi*r1)*2*sqrt(r2*(1-r2));\nauto y = sin(2*pi*r1)*2*sqrt(r2*(1-r2));\nauto z = 1 - 2*r2;\nstd::cout &lt;&lt; x &lt;&lt; \" \" &lt;&lt; y &lt;&lt; \" \" &lt;&lt; z &lt;&lt; '\\n';\n}\n}\n</code></pre> <p>And plot them for free on plot.ly (a great site with 3D scatterplot support):</p> <p></p> <p>On the plot.ly website you can rotate that around and see that it appears uniform.</p>"},{"location":"the_rest_of_your_life/generating_random_directions/#uniform-sampling-a-hemisphere","title":"Uniform Sampling a Hemisphere","text":"<p>Now let\u2019s derive uniform on the hemisphere. The density being uniform on the hemisphere means $p(\\omega) = f(\\theta) = \\frac{1}{2\\pi}$. Just changing the constant in the theta equations yields:</p> <pre><code>$$ r_2 = \\int_{0}^{\\theta} b(\\theta') d\\theta' $$\n$$ = \\int_{0}^{\\theta} 2 \\pi f(\\theta') \\sin(\\theta') d\\theta' $$\n$$ = \\int_{0}^{\\theta} 2 \\pi \\frac{1}{2\\pi} \\sin(\\theta') d\\theta' $$\n$$ \\ldots $$\n$$ \\cos(\\theta) = 1 - r_2 $$\n</code></pre> <p>This means that $\\cos(\\theta)$ will vary from 1 to 0, so $\\theta$ will vary from 0 to $\\pi/2$, which means that nothing will go below the horizon. Rather than plot it, we'll solve for a 2D integral with a known solution. Let\u2019s integrate cosine cubed over the hemisphere (just picking something arbitrary with a known solution). First we'll solve the integral by hand:</p> <pre><code>$$ \\int_\\omega \\cos^3(\\theta) dA $$\n$$ = \\int_{0}^{2 \\pi} \\int_{0}^{\\pi /2} \\cos^3(\\theta) \\sin(\\theta) d\\theta d\\phi $$\n$$ = 2 \\pi \\int_{0}^{\\pi/2} \\cos^3(\\theta) \\sin(\\theta) d\\theta = \\frac{\\pi}{2} $$\n</code></pre> <p>Now for integration with importance sampling. $p(\\omega) = \\frac{1}{2\\pi}$, so we average $f()/p() = \\cos^3(\\theta) / \\frac{1}{2\\pi}$, and we can test this:</p> Integration using $cos^3(x)$<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\ndouble f(double r1, double r2) {\n// auto x = cos(2*pi*r1)*2*sqrt(r2*(1-r2));\n// auto y = sin(2*pi*r1)*2*sqrt(r2*(1-r2));\nauto z = 1 - r2;\ndouble cos_theta = z;\nreturn cos_theta*cos_theta*cos_theta;\n}\ndouble pdf(double r1, double r2) {\nreturn 1.0 / (2.0*pi);\n}\nint main() {\nint N = 1000000;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nauto r1 = random_double();\nauto r2 = random_double();\nsum += f(r1, r2) / pdf(r1, r2);\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"PI/2 = \" &lt;&lt; pi / 2.0 &lt;&lt; '\\n';\nstd::cout &lt;&lt; \"Estimate = \" &lt;&lt; sum / N &lt;&lt; '\\n';\n}\n</code></pre>"},{"location":"the_rest_of_your_life/generating_random_directions/#cosine-sampling-a-hemisphere","title":"Cosine Sampling a Hemisphere","text":"<p>We'll now continue trying to solve for cosine cubed over the horizon, but we'll change our PDF to generate directions with $p(\\omega) =  f(\\theta) = \\cos(\\theta) / \\pi$.</p> <pre><code>$$ r_2 = \\int_{0}^{\\theta} b(\\theta') d\\theta' $$\n$$ = \\int_{0}^{\\theta} 2 \\pi f(\\theta') \\sin(\\theta') d\\theta' $$\n$$ = \\int_{0}^{\\theta} 2 \\pi \\frac{\\cos(\\theta')}{\\pi} \\sin(\\theta') d\\theta' $$\n$$ = 1 - \\cos^2(\\theta) $$\n</code></pre> <p>So,</p> <pre><code>$$ \\cos(\\theta) = \\sqrt{1 - r_2} $$\n</code></pre> <p>We can save a little algebra on specific cases by noting</p> <pre><code>$$ z = \\cos(\\theta) = \\sqrt{1 - r_2} $$\n$$ x = \\cos(\\phi) \\sin(\\theta) = \\cos(2 \\pi r_1) \\sqrt{1 - z^2} = \\cos(2 \\pi r_1) \\sqrt{r_2} $$\n$$ y = \\sin(\\phi) \\sin(\\theta) = \\sin(2 \\pi r_1) \\sqrt{1 - z^2} = \\sin(2 \\pi r_1) \\sqrt{r_2} $$\n</code></pre> <p>Here's a function that generates random vectors weighted by this PDF:</p> Random cosine direction utility function<pre><code>inline vec3 random_cosine_direction() {\nauto r1 = random_double();\nauto r2 = random_double();\nauto phi = 2*pi*r1;\nauto x = cos(phi)*sqrt(r2);\nauto y = sin(phi)*sqrt(r2);\nauto z = sqrt(1-r2);\nreturn vec3(x, y, z);\n}\n</code></pre> Integration with cosine density function<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\ndouble f(const vec3&amp; d) {\nauto cos_theta = d.z();\nreturn cos_theta*cos_theta*cos_theta;\n}\ndouble pdf(const vec3&amp; d) {\nreturn d.z() / pi;\n}\nint main() {\nint N = 1000000;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nvec3 d = random_cosine_direction();\nsum += f(d) / pdf(d);\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"PI/2 = \" &lt;&lt; pi / 2.0 &lt;&lt; '\\n';\nstd::cout &lt;&lt; \"Estimate = \" &lt;&lt; sum / N &lt;&lt; '\\n';\n}\n</code></pre> <p>We can generate other densities later as we need them. This <code>random_cosine_direction()</code> function produces a random direction weighted by $\\cos(\\theta)$ where $\\theta$ is the angle from the $z$ axis.</p>"},{"location":"the_rest_of_your_life/light_scattering/","title":"Light Scattering","text":"<p>In this chapter we won't actually program anything. We'll just be setting up for a big lighting change in the next chapter. Our ray tracing program from the first two books scatters a ray when it interacts with a surface or a volume. Ray scattering is the most commonly used model for simulating light propagation through a scene. This can naturally be modeled probabilistically. There are many things to consider when modeling the probabilistic scattering of rays.</p>"},{"location":"the_rest_of_your_life/light_scattering/#albedo","title":"Albedo","text":"<p>First, is the light absorbed?</p> <p>Probability of light being scattered: $A$</p> <p>Probability of light being absorbed: $1-A$</p> <p>Where here $A$ stands for albedo, which is latin for whiteness. Albedo is a precise technical term in some disciplines, but in all cases it is used to define some form of fractional reflectance. This fractional reflectance (or albedo) will vary with color and (as we implemented for our glass material) can also vary with incident direction (the direction of the incoming ray). It can help to stop and remember that when we simulate light propagation, all we're doing is simulating the movement of photons through a space. If you remember your high school Physics then you should recall that every photon has a unique energy and wavelength associated by the Planck constant:</p> <pre><code>$$ E = \\frac{hc}{\\lambda} $$\n</code></pre> <p>Each individual photon has a tiny amount of energy, but when you add enough of them up you get all of the illumination in your rendering. The absorption or scattering of a photon with a surface or a volume (or really anything that a photon can interact with) is probabilistically determined by the albedo of the object. Albedo can depend on color because some objects are more likely to absorb some wavelengths.</p> <p>In most physically based renderers, we would use a predefined set of specific wavelengths for the light color rather than RGB. As an example, we would replace our tristimulus RGB renderer with something that specifically samples at 300nm, 350nm, 400nm, ..., 700nm. We can extend our intuition by thinking of R, G, and B as specific algebraic mixtures of wavelengths where R is mostly red wavelengths, G is mostly green wavelengths, and B is mostly blue wavelengths. This is an approximation of the human visual system which has 3 unique sets of color receptors, called cones, that are each sensitive to different algebraic mixtures of wavelengths, roughly RGB, but are referred to as long, medium, and short cones (the names are in reference to the wavelengths that each cone is sensitive to, not the length of the cone). Just as colors can be represented by their strength in the RGB color space, colors can also be represented by how excited each set of cones is in the LMS color space (long, medium, short).</p>"},{"location":"the_rest_of_your_life/light_scattering/#scattering","title":"Scattering","text":"<p>If the light does scatter, it will have a directional distribution that we can describe as a PDF over solid angle. I will refer to this as its scattering PDF: $\\operatorname{pScatter}()$. The scattering PDF will vary with outgoing direction: $\\operatorname{pScatter}(\\omega_o)$. The scattering PDF can also vary with incident direction:   $\\operatorname{pScatter}(\\omega_i, \\omega_o)$. You can see this varying with incident direction when you look at reflections off a road -- they   become mirror-like as your viewing angle (incident angle) approaches grazing. The scattering PDF can vary with the wavelength of the light:   $\\operatorname{pScatter}(\\omega_i, \\omega_o, \\lambda)$. A good example of this is a prism refracting white light into a rainbow. Lastly, the scattering PDF can also depend on the scattering position:   $\\operatorname{pScatter}(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda)$. The $\\mathbf{x}$ is just math notation for the scattering position:   $\\mathbf{x} = (x, y, z)$. The albedo of an object can also depend on these quantities:   $A(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda)$.</p> <p>The color of a surface is found by integrating these terms over the unit hemisphere by the incident direction:</p> <pre><code>$$ \\operatorname{Color}_o(\\mathbf{x}, \\omega_o, \\lambda) = \\int_{\\omega_i}\n    A(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda) \\cdot\n    \\operatorname{pScatter}(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda) \\cdot\n    \\operatorname{Color}_i(\\mathbf{x}, \\omega_i, \\lambda) $$\n</code></pre> <p>We've added a $\\operatorname{Color}_i$ term. The scattering PDF and the albedo at the surface of an object are acting as filters to the light   that is shining on that point. So we need to solve for the light that is shining on that point. This is a recursive algorithm, and is the reason our <code>ray_color</code> function returns the color of the   current object multiplied by the color of the next ray.</p>"},{"location":"the_rest_of_your_life/light_scattering/#the-scattering-pdf","title":"The Scattering PDF","text":"<p>If we apply the Monte Carlo basic formula we get the following statistical estimate:</p> <pre><code>$$ \\operatorname{Color}_o(\\mathbf{x}, \\omega_o, \\lambda) \\approx \\sum\n    \\frac{A(\\, \\ldots \\,) \\cdot\n    \\operatorname{pScatter}(\\, \\ldots \\,) \\cdot\n    \\operatorname{Color}_i(\\, \\ldots \\,)}\n    {p(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda)} $$\n</code></pre> <p>where $p(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda)$ is the PDF of whatever outgoing direction we randomly generate.</p> <p>For a Lambertian surface we already implicitly implemented this formula for the special case where   $pScatter(\\, \\ldots \\,)$ is a cosine density. The $\\operatorname{pScatter}(\\, \\ldots \\,)$ of a Lambertian surface is proportional to   $\\cos(\\theta_o)$, where $\\theta_o$ is the angle relative to the surface normal. Let's solve for $C$ once more:</p> <pre><code>$$ \\operatorname{pScatter}(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda) = C \\cdot \\cos(\\theta_o) $$\n</code></pre> <p>All two dimensional PDFs need to integrate to one over the whole surface (remember that   $\\operatorname{pScatter}$ is a PDF). We set $\\operatorname{pScatter}(\\theta_o &lt; 0) = 0$ so that we don't scatter below the horizon.</p> <pre><code>$$ 1 = \\int_{0}^{2 \\pi} \\int_{0}^{\\pi / 2} C \\cdot cos(\\theta) dA $$\n</code></pre> <p>To integrate over the hemisphere, remember that in spherical coordinates:</p> <pre><code>$$ dA = \\sin(\\theta) d\\theta d\\phi $$\n</code></pre> <p>So:</p> <pre><code>$$ 1 = C \\cdot \\int_{0}^{2 \\pi} \\int_{0}^{\\pi / 2} cos(\\theta) sin(\\theta) d\\theta d\\phi $$\n$$ 1 = C \\cdot 2 \\pi \\frac{1}{2} $$\n$$ 1 = C \\cdot \\pi $$\n$$ C = \\frac{1}{\\pi} $$\n</code></pre> <p>The integral of $\\cos(\\theta_o)$ over the hemisphere is $\\pi$, so we need to we need to normalize   by $\\frac{1}{\\pi}$. The PDF $\\operatorname{pScatter}$ is only dependent on outgoing direction ($\\omega_o$), so we'll   simplify its representation to just $\\operatorname{pScatter}(\\omega_o)$. Put all of this together and you get the scattering PDF for a Lambertian surface:</p> <pre><code>$$ \\operatorname{pScatter}(\\omega_o) = \\frac{\\cos(\\theta_o)}{\\pi} $$\n</code></pre> <p>We'll assume that the $p(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda)$ is equal to the scattering PDF:</p> <pre><code>$$ p(\\omega_o) = \\operatorname{pScatter}(\\omega_o) = \\frac{\\cos(\\theta_o)}{\\pi} $$\n</code></pre> <p>The numerator and denominator cancel out, and we get:</p> <pre><code>$$ \\operatorname{Color}_o(\\mathbf{x}, \\omega_o, \\lambda) \\approx \\sum\n    A(\\, \\ldots \\,) \\cdot\n    \\operatorname{Color}_i(\\, \\ldots \\,) $$\n</code></pre> <p>This is exactly what we had in our original <code>ray_color()</code> function!</p> <pre><code>return attenuation * ray_color(scattered, depth-1, world);\n</code></pre> <p>The treatment above is slightly non-standard because I want the same math to work for surfaces and volumes. If you read the literature, you\u2019ll see reflection defined by the Bidirectional Reflectance Distribution Function (BRDF). It relates pretty simply to our terms:</p> <pre><code>$$ BRDF(\\omega_i, \\omega_o, \\lambda) = \\frac{A(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda) \\cdot\n    \\operatorname{pScatter}(\\mathbf{x}, \\omega_i, \\omega_o, \\lambda)}{\\cos(\\theta_o)} $$\n</code></pre> <p>So for a Lambertian surface for example, $BRDF = A / \\pi$. Translation between our terms and BRDF is easy. For participating media (volumes), our albedo is usually called the scattering albedo, and our scattering PDF is usually called the phase function.</p> <p>All that we've done here is outline the PDF for the Lambertian scattering of a material. However, we'll need to generalize so that we can send extra rays in important directions, such as toward the lights.</p>"},{"location":"the_rest_of_your_life/mixture_densities/","title":"Mixture Densities","text":"<p>We have used a PDF related to $\\cos(\\theta)$, and a PDF related to sampling the light. We would like a PDF that combines these.</p>"},{"location":"the_rest_of_your_life/mixture_densities/#the-pdf-class","title":"The PDF Class","text":"<p>We've worked with PDFs in quite a lot of code already. I think that now is a good time to figure out how we want to standardize our usage of PDFs. We already know that we are going to have a PDF for the surface and a PDF for the light, so let's create a <code>pdf</code> base class. So far, we've had a <code>pdf()</code> function that took a direction and returned the PDF's distribution value for that direction. This value has so far been one of $1/4\\pi$, $1/2\\pi$, and $\\cos(\\theta)/\\pi$. In a couple of our examples we generated the random direction using a different distribution than the distribution of the PDF. We covered this quite a lot in the chapter Playing with Importance Sampling. In general, if we know the distribution of our random directions, we should use a PDF with the same distribution. This will lead to the fastest convergence. With that in mind, we'll create a <code>pdf</code> class that is responsible for generating random directions and determining the value of the PDF.</p> <p>From all of this, any <code>pdf</code> class should be responsible for</p> <ol> <li>returning a random direction weighted by the internal PDF distribution, and</li> <li>returning the corresponding PDF distribution value in that direction.</li> </ol> <p>The details of how this is done under the hood varies for $\\operatorname{pSurface}$ and   $\\operatorname{pLight}$, but that is exactly what class hierarchies were invented for! It\u2019s never obvious what goes in an abstract class, so my approach is to be greedy and hope a minimal   interface works, and for <code>pdf</code> this implies:</p> The abstract pdf class<pre><code>#ifndef PDF_H\n#define PDF_H\n#include \"rtweekend.h\"\n#include \"onb.h\"\nclass pdf {\npublic:\nvirtual ~pdf() {}\nvirtual double value(const vec3&amp; direction) const = 0;\nvirtual vec3 generate() const = 0;\n};\n#endif\n</code></pre> <p>We\u2019ll see if we need to add anything else to <code>pdf</code> by fleshing out the subclasses. First, we'll create a uniform density over the unit sphere:</p> The uniform_pdf class<pre><code>class sphere_pdf : public pdf {\npublic:\nsphere_pdf() { }\ndouble value(const vec3&amp; direction) const override {\nreturn 1/ (4 * pi);\n}\nvec3 generate() const override {\nreturn random_unit_vector();\n}\n};\n</code></pre> <p>Next, let\u2019s try a cosine density:</p> The cosine_pdf class<pre><code>class cosine_pdf : public pdf {\npublic:\ncosine_pdf(const vec3&amp; w) { uvw.build_from_w(w); }\ndouble value(const vec3&amp; direction) const override {\nauto cosine_theta = dot(unit_vector(direction), uvw.w());\nreturn fmax(0, cosine_theta/pi);\n}\nvec3 generate() const override {\nreturn uvw.local(random_cosine_direction());\n}\nprivate:\nonb uvw;\n};\n</code></pre> <p>We can try this cosine PDF in the <code>ray_color()</code> function:</p> The ray_color function, using cosine pdf<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\n// If the ray hits nothing, return the background color.\nif (!world.hit(r, interval(0.001, infinity), rec))\nreturn background;\nray scattered;\ncolor attenuation;\ndouble pdf_val;\ncolor color_from_emission = rec.mat-&gt;emitted(r, rec, rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered, pdf_val))\nreturn color_from_emission;\ncosine_pdf surface_pdf(rec.normal);\nscattered = ray(rec.p, surface_pdf.generate(), r.time());\npdf_val = surface_pdf.value(scattered.direction());\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ncolor color_from_scatter =\n(attenuation * scattering_pdf * ray_color(scattered, depth-1, world)) / pdf_val;\nreturn color_from_emission + color_from_scatter;\n}\n};\n</code></pre> <p>This yields an exactly matching result so all we\u2019ve done so far is move some computation up into the <code>cosine_pdf</code> class:</p> <p></p>"},{"location":"the_rest_of_your_life/mixture_densities/#sampling-directions-towards-a-hittable","title":"Sampling Directions towards a Hittable","text":"<p>Now we can try sampling directions toward a <code>hittable</code>, like the light.</p> The hittable_pdf class<pre><code>...\n#include \"hittable_list.h\"\n...\nclass hittable_pdf : public pdf {\npublic:\nhittable_pdf(const hittable&amp; _objects, const point3&amp; _origin)\n: objects(_objects), origin(_origin)\n{}\ndouble value(const vec3&amp; direction) const override {\nreturn objects.pdf_value(origin, direction);\n}\nvec3 generate() const override {\nreturn objects.random(origin);\n}\nprivate:\nconst hittable&amp; objects;\npoint3 origin;\n};\n</code></pre> <p>If we want to sample the light, we will need <code>hittable</code> to answer some queries that it doesn\u2019t yet have an interface for. The above code assumes the existence of two as-of-yet unimplemented functions in the <code>hittable</code> class: <code>pdf_value()</code> and <code>random()</code>. We need to add these functions for the program to compile. We could go through all of the <code>hittable</code> subclasses and add these functions, but that would be a hassle, so we\u2019ll just add two trivial functions to the <code>hittable</code> base class. This breaks our previously pure abstract implementation, but it saves work. Feel free to write these functions through to subclasses if you want a purely abstract <code>hittable</code> interface class.</p> The hittable class, with two new methods<pre><code>class hittable {\npublic:\n...\nvirtual double pdf_value(const point3&amp; o, const vec3&amp; v) const {\nreturn 0.0;\n}\nvirtual vec3 random(const vec3&amp; o) const {\nreturn vec3(1, 0, 0);\n}\n};\n</code></pre> <p>And then we change <code>quad</code> to implement those functions:</p> quad with pdf<pre><code>class quad : public hittable {\npublic:\nquad(const point3&amp; _Q, const vec3&amp; _u, const vec3&amp; _v, shared_ptr&lt;material&gt; m)\n: Q(_Q), u(_u), v(_v), mat(m)\n{\nauto n = cross(u, v);\nnormal = unit_vector(n);\nD = dot(normal, Q);\nw = n / dot(n,n);\narea = n.length();\nset_bounding_box();\n}\n...\ndouble pdf_value(const point3&amp; origin, const vec3&amp; v) const override {\nhit_record rec;\nif (!this-&gt;hit(ray(origin, v), interval(0.001, infinity), rec))\nreturn 0;\nauto distance_squared = rec.t * rec.t * v.length_squared();\nauto cosine = fabs(dot(v, rec.normal) / v.length());\nreturn distance_squared / (cosine * area);\n}\nvec3 random(const point3&amp; origin) const override {\nauto p = plane_origin + (random_double() * axis_A) + (random_double() * axis_B);\nreturn p - origin;\n}\nprivate:\npoint3 Q;\nvec3 u, v;\nshared_ptr&lt;material&gt; mat;\naabb bbox;\nvec3 normal;\ndouble D;\nvec3 w;\ndouble area;\n};\n</code></pre> <p>We only need to add <code>pdf_value()</code> and <code>random()</code> to <code>quad</code> because we're using this to importance sample the light, and the only light we have in our scene is a <code>quad</code>. if you want other light geometries, or want to use a PDF with other objects, you'll need to implement the above functions for the corresponding classes.</p> <p>Add a <code>lights</code> parameter to the camera <code>render()</code> function:</p> ray_color function with light PDF<pre><code>class camera {\npublic:\n...\nvoid render(const hittable&amp; world, const hittable&amp; lights) {\ninitialize();\nstd::cout &lt;&lt; \"P3\\n\" &lt;&lt; image_width &lt;&lt; ' ' &lt;&lt; image_height &lt;&lt; \"\\n255\\n\";\nint sqrt_spp = int(sqrt(samples_per_pixel));\nfor (int j = 0; j &lt; image_height; ++j) {\nstd::clog &lt;&lt; \"\\rScanlines remaining: \" &lt;&lt; (image_height - j) &lt;&lt; ' ' &lt;&lt; std::flush;\nfor (int i = 0; i &lt; image_width; ++i) {\ncolor pixel_color(0,0,0);\nfor (int s_j = 0; s_j &lt; sqrt_spp; ++s_j) {\nfor (int s_i = 0; s_i &lt; sqrt_spp; ++s_i) {\nray r = get_ray(i, j, s_i, s_j);\npixel_color += ray_color(r, max_depth, world, lights);\n}\n}\nwrite_color(std::cout, pixel_color, samples_per_pixel);\n}\n}\nstd::clog &lt;&lt; \"\\rDone.                 \\n\";\n}\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world, const hittable&amp; lights)\nconst {\n...\nray scattered;\ncolor attenuation;\ndouble pdf_val;\ncolor color_from_emission = rec.mat-&gt;emitted(r, rec, rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered, pdf_val))\nreturn color_from_emission;\nhittable_pdf light_pdf(light_ptr, rec.p);\nscattered = ray(rec.p, light_pdf.generate(), r.time());\npdf_val = light_pdf.value(scattered.direction());\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ncolor sample_color = ray_color(scattered, depth-1, world, lights);\ncolor color_from_scatter = (attenuation * scattering_pdf * sample_color) / pdf_val;\nreturn color_from_emission + color_from_scatter;\n}\n};\n</code></pre> <p>Create a light in the middle of the ceiling:</p> Adding a light to the Cornell box<pre><code>int main() {\n...\n// Box 2\nshared_ptr&lt;hittable&gt; box2 = box(point3(0,0,0), point3(165,165,165), white);\nbox2 = make_shared&lt;rotate_y&gt;(box2, -18);\nbox2 = make_shared&lt;translate&gt;(box2, vec3(130,0,65));\nworld.add(box2);\n// Light Sources\nhittable_list lights;\nauto m = shared_ptr&lt;material&gt;();\nlights.add(make_shared&lt;quad&gt;(point3(343,554,332), vec3(-130,0,0), vec3(0,0,-105), m));\ncamera cam;\n...\ncam.render(world, lights);\n}\n</code></pre> <p>At 10 samples per pixel we get:</p> <p></p>"},{"location":"the_rest_of_your_life/mixture_densities/#the-mixture-pdf-class","title":"The Mixture PDF Class","text":"<p>As was briefly mentioned in the chapter Playing with Importance Sampling, we can create linear mixtures of any PDFs to form mixture densities that are also PDFs. Any weighted average of PDFs is also a PDF. As long as the weights are positive and add up to any one, we have a new PDF.</p> <p>$$ \\operatorname{pMixture}() = w_0 p_0() + w_1 p_1() + w_2 p_2() + \\ldots + w_{n-1} p_{n-1}() $$</p> <p>$$ 1 = w_0 + w_1 + w_2 + \\ldots + w_{n-1} $$</p> <p>For example, we could just average the two densities:</p> <p>$$ \\operatorname{pMixture}(\\omega_o)      = \\frac{1}{2} \\operatorname{pSurface}(\\omega_o) + \\frac{1}{2} \\operatorname{pLight}(\\omega_o)   $$</p> <p>How would we instrument our code to do that? There is a very important detail that makes this not quite as easy as one might expect. Generating the random direction for a mixture PDF is simple:</p> <pre><code>if (random_double() &lt; 0.5)\npick direction according to pSurface\nelse\npick direction according to pLight\n</code></pre> <p>But solving for the PDF value of $\\operatorname{pMixture}$ is slightly more subtle. We can't just</p> <pre><code>if (direction is from pSurface)\nget PDF value of pSurface\nelse\nget PDF value of pLight\n</code></pre> <p>For one, figuring out which PDF the random direction came from is probably not trivial. We don't have any plumbing for <code>generate()</code> to tell <code>value()</code> what the original <code>random_double()</code> was, so we can't trivially say which PDF the random direction comes from. If we thought that the above was correct, we would have to solve backwards to figure which PDF the direction could come from. Which honestly sounds like a nightmare, but fortunately we don't need to do that. There are some directions that both PDFs could have generated. For example, a direction toward the light could have been generated by either   $\\operatorname{pLight}$ or $\\operatorname{pSurface}$. It is sufficient for us to solve for the pdf value of $\\operatorname{pSurface}$ and of   $\\operatorname{pLight}$ for a random direction and then take the PDF mixture weights to solve for   the total PDF value for that direction. The mixture density class is actually pretty straightforward:</p> The mixture_pdf class<pre><code>class mixture_pdf : public pdf {\npublic:\nmixture_pdf(shared_ptr&lt;pdf&gt; p0, shared_ptr&lt;pdf&gt; p1) {\np[0] = p0;\np[1] = p1;\n}\ndouble value(const vec3&amp; direction) const override {\nreturn 0.5 * p[0]-&gt;value(direction) + 0.5 *p[1]-&gt;value(direction);\n}\nvec3 generate() const override {\nif (random_double() &lt; 0.5)\nreturn p[0]-&gt;generate();\nelse\nreturn p[1]-&gt;generate();\n}\nprivate:\nshared_ptr&lt;pdf&gt; p[2];\n};\n</code></pre> <p>Now we would like to do a mixture density of the cosine sampling and of the light sampling. We can plug it into <code>ray_color()</code>:</p> The ray_color function, using mixture PDF<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world, const hittable&amp; lights)\nconst {\n...\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered, pdf_val))\nreturn color_from_emission;\nauto p0 = make_shared&lt;hittable_pdf&gt;(light_ptr, rec.p);\nauto p1 = make_shared&lt;cosine_pdf&gt;(rec.normal);\nmixture_pdf mixed_pdf(p0, p1);\nscattered = ray(rec.p, mixed_pdf.generate(), r.time());\npdf_val = mixed_pdf.value(scattered.direction());\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ncolor sample_color = ray_color(scattered, depth-1, world, lights);\ncolor color_from_scatter = (attenuation * scattering_pdf * sample_color) / pdf_val;\nreturn color_from_emission + color_from_scatter;\n}\n</code></pre> <p>1000 samples per pixel yields:</p> <p></p>"},{"location":"the_rest_of_your_life/monte_carlo_integration_on_the_sphere_of_directions/","title":"Monte Carlo Integration on the Sphere of Directions","text":"<p>In chapter One Dimensional Monte Carlo Integration we started with uniform random numbers and slowly, over the course of a chapter, built up more and more complicated ways of producing random numbers, before ultimately arriving at the intuition of PDFs, and how to use them to generate random numbers of arbitrary distribution.</p> <p>All of the concepts covered in that chapter continue to work as we extend beyond a single dimension. Moving forward, we might need to be able to select a point from a two, three, or even higher dimensional space and then weight that selection by an arbitrary PDF. An important case of this--at least for ray tracing--is producing a random direction. In the first two books we generated a random direction by creating a random vector and rejecting it if it fell outside of the unit sphere. We repeated this process until we found a random vector that fell inside the unit sphere. Normalizing this vector produced points that lay exactly on the unit sphere and thereby represent a random direction. This process of generating samples and rejecting them if they are not inside a desired space is called the rejection method, and is found all over the literature. The method covered last chapter is referred to as the inversion method because we invert a PDF.</p> <p>Every direction in 3D space has an associated point on the unit sphere and can be generated by solving for the vector that travels from the origin to that associated point. You can think of choosing a random direction as choosing a random point in a constrained two dimensional plane: the plane created by mapping the unit sphere to Cartesian coordinates. The same methodology as before applies, but now we might have a PDF defined over two dimensions. Suppose we want to integrate this function over the surface of the unit sphere:</p> <p>$$ f(\\theta, \\phi) = cos^2(\\theta) $$</p> <p>Using Monte Carlo integration, we should just be able to sample $\\cos^2(\\theta) / p(r)$, where the $p(r)$ is now just $p(direction)$. But what is direction in that context? We could make it based on polar coordinates, so $p$ would be in terms of $\\theta$ and $\\phi$ for $p(\\theta, \\phi)$. It doesn't matter which coordinate system you choose to use. Although, however you choose to do it, remember that a PDF must integrate to one over the whole surface and that the PDF represents the relative probability of that direction being sampled. Recall that we have a <code>vec3</code> function to generate uniform random samples on the unit sphere (<code>random_unit_vector()</code>). What is the PDF of these uniform samples? As a uniform density on the unit sphere, it is $1/\\mathit{area}$ of the sphere, which is $1/(4\\pi)$. If the integrand is $\\cos^2(\\theta)$, and $\\theta$ is the angle with the $z$ axis:</p> Generating importance-sampled points on the unit sphere<pre><code>double f(const vec3&amp; d) {\nauto cosine_squared = d.z()*d.z();\nreturn cosine_squared;\n}\ndouble pdf(const vec3&amp; d) {\nreturn 1 / (4*pi);\n}\nint main() {\nint N = 1000000;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nvec3 d = random_unit_vector();\nauto f_d = f(d);\nsum += f_d / pdf(d);\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"I = \" &lt;&lt; sum / N &lt;&lt; '\\n';\n}\n</code></pre> <p>The analytic answer is $\\frac{4}{3} \\pi$ -- if you remember enough advanced calc, check me! And the code above produces that. The key point here is that all of the integrals and the probability and everything else is over the unit sphere. The way to represent a single direction in 3D is its associated point on the unit sphere. The way to represent a range of directions in 3D is the amount of area on the unit sphere that those directions travel through. Call it direction, area, or solid angle -- it\u2019s all the same thing. Solid angle is the term that you'll usually find in the literature. You have radians (r) in $\\theta$ over one dimension, and you have steradians (sr) in $\\theta$ and $\\phi$ over two dimensions (the unit sphere is a three dimensional object, but its surface is only two dimensional). Solid Angle is just the two dimensional extension of angles. If you are comfortable with a two dimensional angle, great! If not, do what I do and imagine the area on the unit sphere that a set of directions goes through. The solid angle $\\omega$ and the projected area $A$ on the unit sphere are the same thing.</p> <p></p> <p>Now let\u2019s go on to the light transport equation we are solving.</p>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/","title":"One Dimensional Monte Carlo Integration","text":"<p>Our Buffon Needle example is a way of calculating $\\pi$ by solving for the ratio of the area of the circle and the area of the inscribing square:</p> <pre><code>$$ \\frac{\\operatorname{area}(\\mathit{circle})}{\\operatorname{area}(\\mathit{square})}\n   = \\frac{\\pi}{4}\n$$\n</code></pre> <p>We picked a bunch of random points in the inscribing square and counted the fraction of them that were also in the unit circle. This fraction was an estimate that tended toward $\\frac{\\pi}{4}$ as more points were added. If we didn't know the area of a circle, we could still solve for it using the above ratio. We know that the ratio of areas of the unit circle and the inscribing square is $\\frac{\\pi}{4}$, and we know that the area of a inscribing square is $4r^2$, so we could then use those two quantities to get the area of a circle:</p> <pre><code>$$ \\frac{\\operatorname{area}(\\mathit{circle})}{\\operatorname{area}(\\mathit{square})}\n   = \\frac{\\pi}{4}\n$$\n\n$$ \\frac{\\operatorname{area}(\\mathit{circle})}{(2r)^2} = \\frac{\\pi}{4} $$\n\n$$ \\operatorname{area}(\\mathit{circle}) = \\frac{\\pi}{4} 4r^2 $$\n\n$$ \\operatorname{area}(\\mathit{circle}) = \\pi r^2 $$\n</code></pre> <p>We choose a circle with radius $r = 1$ and get:</p> <pre><code>$$ \\operatorname{area}(\\mathit{circle}) = \\pi $$\n</code></pre> <p>Our work above is equally valid as a means to solve for $pi$ as it is a means to solve for the area of a circle:</p> Estimating area of unit circle<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\nint main() {\nint N = 100000;\nint inside_circle = 0;\nfor (int i = 0; i &lt; N; i++) {\nauto x = random_double(-1,1);\nauto y = random_double(-1,1);\nif (x*x + y*y &lt; 1)\ninside_circle++;\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"Estimated area of unit circle = \" &lt;&lt; (4.0 * inside_circle) / N &lt;&lt; '\\n';\n}\n</code></pre>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#expected-value","title":"Expected Value","text":"<p>Let's take a step back and think about our Monte Carlo algorithm a little bit more generally.</p> <p>If we assume that we have all of the following:</p> <ol> <li> <p>A list of values $X$ that contains members $x_i$:</p> <p>$$ X = (x_0, x_1, ..., x_{N-1})  $$</p> </li> <li> <p>A continuous function $f(x)$ that takes members from the list:</p> <p>$$ y_i = f(x_i) $$</p> </li> <li> <p>A function $F(X)$ that takes the list $X$ as input and produces the list $Y$ as output:</p> <p>$$ Y = F(X) $$</p> </li> <li> <p>Where output list $Y$ has members $y_i$:</p> <p>$$ Y = (y_0, y_1, ..., y_{N-1}) = (f(x_0), f(x_1), ..., f(x_{N-1})) $$</p> </li> </ol> <p>If we assume all of the above, then we could solve for the arithmetic mean--the average--of the list $Y$ with the following:</p> <pre><code>$$ \\operatorname{average}(Y) = E[Y] = \\frac{1}{N} \\sum_{i=0}^{N-1} y_i $$\n$$ = \\frac{1}{N} \\sum_{i=0}^{N-1} f(x_i) $$\n$$ = E[F(X)] $$\n</code></pre> <p>Where $E[Y]$ is referred to as the expected value of $Y$. If the values of $x_i$ are chosen randomly from a continuous interval $[a,b]$ such that $ a \\leq x_i \\leq b $ for all values of $i$, then $E[F(X)]$ will approximate the average of the continuous function $f(x')$ over the the same interval $ a \\leq x' \\leq b $.</p> <pre><code>$$ E[f(x') | a \\leq x' \\leq b] \\approx E[F(X) | X =\n    \\{\\small x_i | a \\leq x_i \\leq b \\normalsize \\} ] $$\n$$ \\approx E[Y = \\{\\small y_i = f(x_i) | a \\leq x_i \\leq b \\normalsize \\} ] $$\n\n$$ \\approx \\frac{1}{N} \\sum_{i=0}^{N-1} f(x_i) $$\n</code></pre> <p>If we take the number of samples $N$ and take the limit as $N$ goes to $\\infty$, then we get the following:</p> <pre><code>$$ E[f(x') | a \\leq x' \\leq b]  = \\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=0}^{N-1} f(x_i) $$\n</code></pre> <p>Within the continuous interval $[a,b]$, the expected value of continuous function $f(x')$ can be perfectly represented by summing an infinite number of random points within the interval. As this number of points approaches $\\infty$ the average of the outputs tends to the exact answer. This is a Monte Carlo algorithm.</p> <p>Sampling random points isn't our only way to solve for the expected value over an interval. We can also choose where we place our sampling points. If we had $N$ samples over an interval $[a,b]$ then we could choose to equally space points throughout:</p> <pre><code>$$ x_i = a + i \\Delta x $$\n$$ \\Delta x = \\frac{b - a}{N} $$\n</code></pre> <p>Then solving for their expected value:</p> <pre><code>$$ E[f(x') | a \\leq x' \\leq b] \\approx \\frac{1}{N} \\sum_{i=0}^{N-1} f(x_i)\n    \\Big|_{x_i = a + i \\Delta x} $$\n$$ E[f(x') | a \\leq x' \\leq b] \\approx \\frac{\\Delta x}{b - a} \\sum_{i=0}^{N-1} f(x_i)\n    \\Big|_{x_i = a + i \\Delta x} $$\n$$ E[f(x') | a \\leq x' \\leq b] \\approx \\frac{1}{b - a} \\sum_{i=0}^{N-1} f(x_i) \\Delta x\n    \\Big|_{x_i = a + i \\Delta x} $$\n</code></pre> <p>Take the limit as $N$ approaches $\\infty$</p> <pre><code>$$ E[f(x') | a \\leq x' \\leq b] = \\lim_{N \\to \\infty} \\frac{1}{b - a} \\sum_{i=0}^{N-1}\n    f(x_i) \\Delta x \\Big|_{x_i = a + i \\Delta x} $$\n</code></pre> <p>This is, of course, just a regular integral:</p> <pre><code>$$ E[f(x') | a \\leq x' \\leq b] = \\frac{1}{b - a} \\int_{a}^{b} f(x) dx $$\n</code></pre> <p>If you recall your introductory calculus class, the integral of a function is the area under the curve over that interval:</p> <pre><code>$$ \\operatorname{area}(f(x), a, b) = \\int_{a}^{b} f(x) dx$$\n</code></pre> <p>Therefore, the average over an interval is intrinsically linked with the area under the curve in that interval.</p> <pre><code>$$  E[f(x) | a \\leq x \\leq b] = \\frac{1}{b - a} \\cdot \\operatorname{area}(f(x), a, b) $$\n</code></pre> <p>Both the integral of a function and a Monte Carlo sampling of that function can be used to solve for the average over a specific interval. While integration solves for the average with the sum of infinitely many infinitesimally small slices of the interval, a Monte Carlo algorithm will approximate the same average by solving the sum of ever increasing random sample points within the interval. Counting the number of points that fall inside of an object isn't the only way to measure its average or area. Integration is also a common mathematical tool for this purpose. If a closed form exists for a problem, integration is frequently the most natural and clean way to formulate things.</p> <p>I think a couple of examples will help.</p>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#integrating-x2","title":"Integrating x\u00b2","text":"<p>Let\u2019s look at a classic integral:</p> <pre><code>$$ I = \\int_{0}^{2} x^2 dx $$\n</code></pre> <p>We could solve this using integration:</p> <pre><code>$$ I = \\frac{1}{3} x^3 \\Big|_{0}^{2} $$\n$$ I = \\frac{1}{3} (2^3 - 0^3) $$\n$$ I = \\frac{8}{3} $$\n</code></pre> <p>Or, we could solve the integral using a Monte Carlo approach. In computer sciency notation, we might write this as:</p> <pre><code>$$ I = \\operatorname{area}( x^2, 0, 2 ) $$\n</code></pre> <p>We could also write it as:</p> <pre><code>$$  E[f(x) | a \\leq x \\leq b] = \\frac{1}{b - a} \\cdot \\operatorname{area}(f(x), a, b) $$\n$$ \\operatorname{average}(x^2, 0, 2) = \\frac{1}{2 - 0} \\cdot \\operatorname{area}( x^2, 0, 2 ) $$\n$$ \\operatorname{average}(x^2, 0, 2) = \\frac{1}{2 - 0} \\cdot I $$\n$$ I = 2 \\cdot \\operatorname{average}(x^2, 0, 2) $$\n</code></pre> <p>The Monte Carlo approach:</p> Integrating x2<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\nint main() {\nint a = 0;\nint b = 2;\nint N = 1000000;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nauto x = random_double(a, b);\nsum += x*x;\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"I = \" &lt;&lt; (b - a) * (sum / N) &lt;&lt; '\\n';\n}\n</code></pre> <p>This, as expected, produces approximately the exact answer we get with integration, i.e. $I = 8/3$. You could rightly point to this example and say that the integration is actually a lot less work than the Monte Carlo. That might be true in the case where the function is $f(x) = x^2$, but there exist many functions where it might be simpler to solve for the Monte Carlo than for the integration, like $f(x) = sin^5(x)$.</p> Integrating sin^5<pre><code>for (int i = 0; i &lt; N; i++) {\nauto x = random_double(a, b);\nsum += pow(sin(x), 5.0);\n}\n</code></pre> <p>We could also use the Monte Carlo algorithm for functions where an analytical integration does not exist, like $f(x) = \\ln(\\sin(x))$.</p> Integrating ln(sin)<pre><code>for (int i = 0; i &lt; N; i++) {\nauto x = random_double(a, b);\nsum += log(sin(x));\n}\n</code></pre> <p>In graphics, we often have functions that we can write down explicitly but that have a complicated analytic integration, or, just as often, we have functions that can be evaluated but that can't be written down explicitly, and we will frequently find ourselves with a function that can only be evaluated probabilistically. The function <code>ray_color</code> from the first two books is an example of a function that can only be determined probabilistically. We can\u2019t know what color can be seen from any given place in all directions, but we can statistically estimate which color can be seen from one particular place, for a single particular direction.</p>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#density-functions","title":"Density Functions","text":"<p>The <code>ray_color</code> function that we wrote in the first two books, while elegant in its simplicity, has a fairly major problem. Small light sources create too much noise. This is because our uniform sampling doesn\u2019t sample these light sources often enough. Light sources are only sampled if a ray scatters toward them, but this can be unlikely for a small light, or a light that is far away. If the background color is black, then the only real sources of light in the scene are from the lights that are actually placed about the scene. There might be two rays that intersect at nearby points on a surface, one that is randomly reflected toward the light and one that is not. The ray that is reflected toward the light will appear a very bright color. The ray that is reflected to somewhere else will appear a very dark color. The two intensities should really be somewhere in the middle. We could lessen this problem if we steered both of these rays toward the light, but this would cause the scene to be inaccurately bright.</p> <p>For any given ray, we usually trace from the camera, through the scene, and terminate at a light. But imagine if we traced this same ray from the light source, through the scene, and terminated at the camera. This ray would start with a bright intensity and would lose energy with each successive bounce around the scene. It would ultimately arrive at the camera, having been dimmed and colored by its reflections off various surfaces. Now, imagine if this ray was forced to bounce toward the camera as soon as it could. It would appear inaccurately bright because it hadn't been dimmed by successive bounces. This is analogous to sending more random samples toward the light. It would go a long way toward solving our problem of having a bright pixel next to a dark pixel, but it would then just make all of our pixels bright.</p> <p>We can remove this inaccuracy by downweighting those samples to adjust for the over-sampling. How do we do this adjustment? Well, we'll first need to understand the concept of a probability density function. But to understand the concept of a probability density function, we'll first need to know what a density function is.</p> <p>A density function is just the continuous version of a histogram. Here\u2019s an example of a histogram from the histogram Wikipedia page:</p> <p></p> <p>If we had more items in our data source, the number of bins would stay the same, but each bin would have a higher frequency of each item. If we divided the data into more bins, we'd have more bins, but each bin would have a lower frequency of each item. If we took the number of bins and raised it to infinity, we'd have an infinite number of zero-frequency bins. To solve for this, we'll replace our histogram, which is a discrete function, with a discrete density function. A discrete density function differs from a discrete function in that it normalizes the y-axis to a fraction or percentage of the total, i.e its density, instead of a total count for each bin. Converting from a discrete function to a discrete density function is trivial:</p> <pre><code>$$ \\text{Density of Bin i} = \\frac{\\text{Number of items in Bin i}}\n                                  {\\text{Number of items total}} $$\n</code></pre> <p>Once we have a discrete density function, we can then convert it into a density function by changing our discrete values into continuous values.</p> <pre><code>$$ \\text{Bin Density} = \\frac{(\\text{Fraction of trees between height }H\\text{ and }H\u2019)}\n                        {(H-H\u2019)} $$\n</code></pre> <p>So a density function is a continuous histogram where all of the values are normalized against a total. If we had a specific tree we wanted to know the height of, we could create a probability function that would tell us how likely it is for our tree to fall within a specific bin.</p> <pre><code>$$ \\text{Probability of Bin i} = \\frac{\\text{Number of items in Bin i}}\n                                      {\\text{Number of items total}} $$\n</code></pre> <p>If we combined our probability function and our (continuous) density function, we could interpret that as a statistical predictor of a tree\u2019s height:</p> <pre><code>$$ \\text{Probability a random tree is between } H \\text{ and } H\u2019 =\n    \\text{Bin Density}\\cdot(H-H\u2019)$$\n</code></pre> <p>Indeed, with this continuous probability function, we can now say the likelihood that any given tree has a height that places it within any arbitrary span of multiple bins. This is a probability density function (henceforth PDF). In short, a PDF is a continuous function that can be  integrated over to determine how likely a result is over an integral.</p>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#constructing-a-pdf","title":"Constructing a PDF","text":"<p>Let\u2019s make a PDF and play around with it to build up an intuition. We'll use the following function:</p> <p></p> <p>What does this function do? Well, we know that a PDF is just a continuous function that defines the likelihood of an arbitrary range of values. This function $p(r)$ is constrained between $0$ and $2$ and linearly increases along that interval. So, if we used this function as a PDF to generate a random number then the probability of getting a number near zero would be less than the probability of getting a number near two.</p> <p>The PDF $p(r)$ is a linear function that starts with $0$ at $r=0$ and monotonically increases to its highest point at $p(2)$ for $r=2$. What is the value of $p(2)$? What is the value of $p(r)$? Maybe $p(2)$ is 2? The PDF increases linearly from 0 to 2, so guessing that the value of $p(2)$ is 2 seems reasonable. At least it looks like it can't be 0.</p> <p>Remember that the PDF is a probability function. We are constraining the PDF so that it lies in the range [0,2]. The PDF represents the continuous density function for a probabilistic list. If we know that everything in that list is contained within 0 and 2, we can say that the probability of getting a value between 0 and 2 is 100%. Therefore, the area under the curve must sum to 1:</p> <pre><code>$$ \\operatorname{area}(p(r), 0, 2) = 1 $$\n</code></pre> <p>All linear functions can be represented as a constant term multiplied by a variable.</p> <pre><code>$$ p(r) = C \\cdot r $$\n</code></pre> <p>We need to solve for the value of $C$. We can use integration to work backwards.</p> <pre><code>$$ 1 = \\operatorname{area}(p(r), 0, 2) $$\n$$ = \\int_{0}^{2} C \\cdot r dr $$\n$$ = C \\cdot \\int_{0}^{2} r dr $$\n$$ = C \\cdot \\frac{r^2}{2} \\Big|_{0}^{2} $$\n$$ = C ( \\frac{2^2}{2} - \\frac{0}{2} ) $$\n$$ C = \\frac{1}{2} $$\n</code></pre> <p>That gives us the PDF of $p(r) = r/2$. Just as with histograms we can sum up (integrate) the region to figure out the probability that $r$ is in some interval $[x_0,x_1]$:</p> <pre><code>$$ \\operatorname{Probability} (r | x_0 \\leq r \\leq x_1 )\n   = \\operatorname{area}(p(r), x_0, x_1)\n$$\n\n$$ \\operatorname{Probability} (r | x_0 \\leq r \\leq x_1 ) = \\int_{x_0}^{x_1}  \\frac{r}{2} dr $$\n</code></pre> <p>To confirm your understanding, you should integrate over the region $r=0$ to $r=2$, you should get a probability of 1.</p> <p>After spending enough time with PDFs you might start referring to a PDF as the probability that a variable $r$ is value $x$, i.e. $p(r=x)$. Don't do this. For a continuous function, the probability that a variable is a specific value is always zero. A PDF can only tell you the probability that a variable will fall within a given interval. If the interval you're checking against is a single value, then the PDF will always return a zero probability because its \"bin\" is infinitely thin (has zero width). Here's a simple mathematical proof of this fact:</p> <pre><code>$$ \\operatorname{Probability} (r = x) = \\int_{x}^{x}  p(r) dr $$\n$$ = P(r) \\Big|_{x}^{x} $$\n$$ = P(x) - P(x) $$\n$$ = 0 $$\n</code></pre> <p>Finding the probability of a region surrounding x may not be zero:</p> <pre><code>$$ \\operatorname{Probability} (r | x - \\Delta x &lt; r &lt; x + \\Delta x ) =\n     \\operatorname{area}(p(r), x - \\Delta x, x + \\Delta x) $$\n$$ = P(x + \\Delta x) - P(x - \\Delta x) $$\n</code></pre>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#choosing-our-samples","title":"Choosing our Samples","text":"<p>If we have a PDF for the function that we care about, then we have the probability that the function will return a value within an arbitrary interval. We can use this to determine where we should sample. Remember that this started as a quest to determine the best way to sample a scene so that we wouldn't get very bright pixels next to very dark pixels. If we have a PDF for the scene, then we can probabilistically steer our samples toward the light without making the image inaccurately bright. We already said that if we steer our samples toward the light then we will make the image inaccurately bright. We need to figure out how to steer our samples without introducing this inaccuracy, this will be explained a little bit later, but for now we'll focus on generating samples if we have a PDF. How do we generate a random number with a PDF? For that we will need some more machinery. Don\u2019t worry -- this doesn\u2019t go on forever!</p> <p>Our random number generator <code>random_double()</code> produces a random double between 0 and 1. The number generator is uniform between 0 and 1, so any number between 0 and 1 has equal likelihood. If our PDF is uniform over a domain, say $[0,10]$, then we can trivially produce perfect samples for this uniform PDF with</p> <pre><code>10.0 * random_double()\n</code></pre> <p>That's an easy case, but the vast majority of cases that we're going to care about are nonuniform. We need to figure out a way to convert a uniform random number generator into a nonuniform random number generator, where the distribution is defined by the PDF. We'll just assume that there exists a function $f(d)$ that takes uniform input and produces a nonuniform distribution weighted by PDF. We just need to figure out a way to solve for $f(d)$.</p> <p>For the PDF given above, where $p(r) = \\frac{r}{2}$, the probability of a random sample is higher toward 2 than it is toward 0. There is a greater probability of getting a number between 1.8 and 2.0 than between 0.0 and 0.2. If we put aside our mathematics hat for a second and put on our computer science hat, maybe we can figure out a smart way of partitioning the PDF. We know that there is a higher probability near 2 than near 0, but what is the value that splits the probability in half? What is the value that a random number has a 50% chance of being higher than and a 50% chance of being lower than? What is the $x$ that solves:</p> <pre><code>$$ 50\\% = \\int_{0}^{x}  \\frac{r}{2} dr  = \\int_{x}^{2}  \\frac{r}{2} dr $$\n</code></pre> <p>Solving gives us:</p> <pre><code>$$ 0.5 = \\frac{r^2}{4} \\Big|_{0}^{x} $$\n$$ 0.5 = \\frac{x^2}{4} $$\n$$ x^2 = 2$$\n$$ x = \\sqrt{2}$$\n</code></pre> <p>As a crude approximation we could create a function <code>f(d)</code> that takes as input <code>double d = random_double()</code>. If <code>d</code> is less than (or equal to) 0.5, it produces a uniform number in $[0,\\sqrt{2}]$, if <code>d</code> is greater than 0.5, it produces a uniform number in $[\\sqrt{2}, 2]$.</p> A crude, first-order approximation to nonuniform PDF<pre><code>double f(double d)\n{\nif (d &lt;= 0.5)\nreturn sqrt(2.0) * random_double();\nelse\nreturn sqrt(2.0) + (2 - sqrt(2.0)) * random_double();\n}\n</code></pre> <p>While our initial random number generator was uniform from 0 to 1:</p> <p></p> <p>Our, new, crude approximation for $\\frac{r}{2}$ is nonuniform (but only just):</p> <p></p> <p>We had the analytical solution to the integration above, so we could very easily solve for the 50% value. But we could also solve for this 50% value experimentally. There will be functions that we either can't or don't want to solve for the integration. In these cases, we can get an experimental result close to the real value. Let's take the function:</p> <pre><code>$$ p(x) = e^{\\frac{-x}{2 \\pi}} sin^2(x) $$\n</code></pre> <p>Which looks a little something like this:</p> <p></p> <p>At this point you should be familiar with how to experimentally solve for the area under a curve. We'll take our existing code and modify it slightly to get an estimate for the 50% value. We want to solve for the $x$ value that gives us half of the total area under the curve. As we go along and solve for the rolling sum over N samples, we're also going to store each individual sample alongside its <code>p(x)</code> value. After we solve for the total sum, we'll sort our samples and add them up until we have an area that is half of the total. From $0$ to $2\\pi$ for example:</p> Estimating the 50% point of a function<pre><code>#include \"rtweekend.h\"\n#include &lt;algorithm&gt;\n#include &lt;vector&gt;\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\n#include &lt;cmath&gt;\n#include &lt;stdlib.h&gt;\nstruct sample {\ndouble x;\ndouble p_x;\n};\nbool compare_by_x(const sample&amp; a, const sample&amp; b) {\nreturn a.x &lt; b.x;\n}\nint main() {\nint N = 10000;\ndouble sum = 0.0;\n// iterate through all of our samples\nstd::vector&lt;sample&gt; samples;\nfor (int i = 0; i &lt; N; i++) {\n// Get the area under the curve\nauto x = random_double(0, 2*pi);\nauto sin_x = sin(x);\nauto p_x = exp(-x / (2*pi)) * sin_x * sin_x;\nsum += p_x;\n// store this sample\nsample this_sample = {x, p_x};\nsamples.push_back(this_sample);\n}\n// Sort the samples by x\nstd::sort(samples.begin(), samples.end(), compare_by_x);\n// Find out the sample at which we have half of our area\ndouble half_sum = sum / 2.0;\ndouble halfway_point = 0.0;\ndouble accum = 0.0;\nfor (int i = 0; i &lt; N; i++){\naccum += samples[i].p_x;\nif (accum &gt;= half_sum){\nhalfway_point = samples[i].x;\nbreak;\n}\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"Average = \" &lt;&lt; sum / N &lt;&lt; '\\n';\nstd::cout &lt;&lt; \"Area under curve = \" &lt;&lt; 2 * pi * sum / N &lt;&lt; '\\n';\nstd::cout &lt;&lt; \"Halfway = \" &lt;&lt; halfway_point &lt;&lt; '\\n';\n}\n</code></pre> <p>This code snippet isn't too different from what we had before. We're still solving for the sum over an interval (0 to $2\\pi$). Only this time, we're also storing and sorting all of our samples by their input and output. We use this to determine the point at which they subtotal half of the sum across the entire interval. Once we know that our first $j$ samples sum up to half of the total sum, we know that the $j\\text{th}$ $x$ roughly corresponds to our halfway point:</p> <pre><code>Average = 0.314686555791\nArea under curve = 1.977233943713\nHalfway = 2.016002314977\n</code></pre> <p>If you solve for the integral from $0$ to $2.016$ and from $2.016$ to $2\\pi$ you should get almost exactly the same result for both.</p> <p>We have a method of solving for the halfway point that splits a PDF in half. If we wanted to, we could use this to create a nested binary partition of the PDF:</p> <ol> <li>Solve for halfway point of a PDF</li> <li>Recurse into lower half, repeat step 1</li> <li>Recurse into upper half, repeat step 1</li> </ol> <p>Stopping at a reasonable depth, say 6\u201310. As you can imagine, this could be quite computationally expensive. The computational bottleneck for the code above is probably sorting the samples. A naive sorting algorithm can have an algorithmic complexity of $\\mathcal{O}(\\mathbf{n^2})$ time, which is tremendously expensive. Fortunately, the sorting algorithm included in the standard library is usually much closer to $\\mathcal{O}(\\mathbf{n\\log{}n})$ time, but this can still be quite expensive, especially for millions or billions of samples. But this will produce decent nonuniform distributions of nonuniform numbers. This divide and conquer method of producing nonuniform distributions is used somewhat commonly in practice, although there are much more efficient means of doing so than a simple binary partition. If you have an arbitrary function that you wish to use as the PDF for a distribution, you'll want to research the Metropolis-Hastings Algorithm.</p>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#approximating-distributions","title":"Approximating Distributions","text":"<p>This was a lot of math and work to build up a couple of notions. Let's return to our initial PDF. For the intervals without an explicit probability, we assume the PDF to be zero. So for our example from the beginning of the chapter, $p(r) = 0$, for $r \\notin [0,2]$. We can rewrite our $p(r)$ in piecewise fashion:</p> <pre><code>$$ p(r)=\\begin{cases}\n        0           &amp; r &lt; 0           \\\\\n        \\frac{r}{2} &amp; 0 \\leq r \\leq 2 \\\\\n        0           &amp; 2 &lt; r           \\\\\n   \\end{cases}\n$$\n</code></pre> <p>If you consider what we were trying to do in the previous section, a lot of math revolved around the accumulated area (or accumulated probability) from zero. In the case of the function</p> <pre><code>$$ f(x) = e^{\\frac{-x}{2 \\pi}} sin^2(x)  $$\n</code></pre> <p>we cared about the accumulated probability from $0$ to $2\\pi$ (100%) and the accumulated probability from $0$ to $2.016$ (50%). We can generalize this to an important term, the Cumulative Distribution Function $P(x)$ is defined as:</p> <pre><code>$$ P(x) =  \\int_{-\\infty}^{x}  p(x') dx' $$\n</code></pre> <p>Or,</p> <pre><code>$$ P(x) = \\operatorname{area}(p(x'), -\\infty, x) $$\n</code></pre> <p>Which is the amount of cumulative probability from $-\\infty$. We rewrote the integral in terms of $x'$ instead of $x$ because of calculus rules, if you're not sure what it means, don't worry about it, you can just treat it like it's the same. If we take the integration outlined above, we get the piecewise $P(r)$:</p> <pre><code>$$ P(r)=\\begin{cases}\n       0             &amp; r &lt; 0           \\\\\n       \\frac{r^2}{4} &amp; 0 \\leq r \\leq 2 \\\\\n       1             &amp; 2 &lt; r           \\\\\n   \\end{cases}\n$$\n</code></pre> <p>The Probability Density Function (PDF) is the probability function that explains how likely an interval of numbers is to be chosen. The Cumulative Distribution Function (CDF) is the distribution function that explains how likely all numbers smaller than its input is to be chosen. To go from the PDF to the CDF, you need to integrate from $-\\infty$ to $x$, but to go from the CDF to the PDF, all you need to do is take the derivative:</p> <pre><code>$$ p(x) = \\frac{d}{dx}P(x) $$\n</code></pre> <p>If we evaluate the CDF, $P(r)$, at $r = 1.0$, we get:</p> <pre><code>$$ P(1.0) = \\frac{1}{4} $$\n</code></pre> <p>This says a random variable plucked from our PDF has a 25% chance of being 1 or lower. We want a function $f(d)$ that takes a uniform distribution between 0 and 1 (i.e <code>f(random_double())</code>), and returns a random value according to a distribution that has the CDF $P(x) = \\frac{x^2}{4}$. We don\u2019t know yet know what the function $f(d)$ is analytically, but we do know that 25% of what it returns should be less than 1.0, and 75% should be above 1.0. Likewise, we know that 50% of what it returns should be less than $\\sqrt{2}$, and 50% should be above $\\sqrt{2}$. If $f(d)$ monotonically increases, then we would expect $f(0.25) = 1.0$ and $f(0.5) = \\sqrt{2}$. This can be generalized to figure out $f(d)$ for every possible input:</p> <pre><code>$$ f(P(x)) = x $$\n</code></pre> <p>Let's take some more samples:</p> <pre><code>$$ P(0.0) = 0 $$\n$$ P(0.5) = \\frac{1}{16} $$\n$$ P(1.0) = \\frac{1}{4} $$\n$$ P(1.5) = \\frac{9}{16} $$\n$$ P(2.0) = 1 $$\n</code></pre> <p>so, the function $f()$ has values</p> <pre><code>$$ f(P(0.0)) = f(0) = 0 $$\n$$ f(P(0.5)) = f(\\frac{1}{16}) = 0.5 $$\n$$ f(P(1.0)) = f(\\frac{1}{4}) = 1.0 $$\n$$ f(P(1.5)) = f(\\frac{9}{16}) = 1.5 $$\n$$ f(P(2.0)) = f(1) = 2.0 $$\n</code></pre> <p>We could use these intermediate values and interpolate between them to approximate $f(d)$:</p> <p></p> <p>If you can't solve for the PDF analytically, then you can't solve for the CDF analytically. After all, the CDF is just the integral of the PDF. However, you can still create a distribution that approximates the PDF. If you take a bunch of samples from the random function you want the PDF from, you can approximate the PDF by getting a histogram of the samples and then converting to a PDF. Alternatively, you can do as we did above and sort all of your samples.</p> <p>Looking closer at the equality:</p> <pre><code>$$ f(P(x)) = x $$\n</code></pre> <p>That just means that $f()$ just undoes whatever $P()$ does. So, $f()$ is the inverse function:</p> <pre><code>$$ f(d) = P^{-1}(x) $$\n</code></pre> <p>For our purposes, if we have PDF $p()$ and cumulative distribution function $P()$, we can use this \"inverse function\" with a random number to get what we want:</p> <pre><code>$$ f(d) = P^{-1} (\\operatorname{random_double}()) $$\n</code></pre> <p>For our PDF $p(r) = r/2$, and corresponding $P(r)$, we need to compute the inverse of $P(r)$. If we have</p> <pre><code>$$ y = \\frac{r^2}{4} $$\n</code></pre> <p>we get the inverse by solving for $r$ in terms of $y$:</p> <pre><code>$$ r = \\sqrt{4y} $$\n</code></pre> <p>Which means the inverse of our CDF is defined as</p> <pre><code>$$ P^{-1}(r) = \\sqrt{4y} $$\n</code></pre> <p>Thus our random number generator with density $p(r)$ can be created with:</p> <pre><code>$$ f(d) = \\sqrt{4\\cdot\\operatorname{random_double}()} $$\n</code></pre> <p>Note that this ranges from 0 to 2 as we hoped, and if we check our work, we replace <code>random_double()</code> with $1/4$ to get 1, and also replace with $1/2$ to get $\\sqrt{2}$, just as expected.</p>"},{"location":"the_rest_of_your_life/one_dimensional_monte_carlo_integration/#importance-sampling","title":"Importance Sampling","text":"<p>You should now have a decent understanding of how to take an analytical PDF and generate a function that produces random numbers with that distribution. We return to our original integral and try it with a few different PDFs to get a better understanding:</p> <pre><code>$$ I = \\int_{0}^{2} x^2 dx $$\n</code></pre> <p>The last time that we tried to solve for the integral we used a Monte Carlo approach, uniformly sampling from the interval $[0, 2]$. We didn't know it at the time, but we were implicitly using a uniform PDF between 0 and 2. This means that we're using a PDF = $1/2$ over the range $[0,2]$, which means the CDF is $P(x) = x/2$, so $f(d) = 2d$. Knowing this, we can make this uniform PDF explicit:</p> Explicit uniform PDF for $x^2$<pre><code>#include \"rtweekend.h\"\n#include &lt;iostream&gt;\n#include &lt;iomanip&gt;\n#include &lt;math.h&gt;\n#include &lt;stdlib.h&gt;\ndouble f(double d) {\nreturn 2.0 * d;\n}\ndouble pdf(double x) {\nreturn 0.5;\n}\nint main() {\nint N = 1000000;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nauto x = f(random_double());\nsum += x*x / pdf(x);\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"I = \" &lt;&lt; sum / N &lt;&lt; '\\n';\n}\n</code></pre> <p>There are a couple of important things to emphasize. Every value of $x$ represents one sample of the function $x^2$ within the distribution $[0, 2]$. We use a function $f$ to randomly select samples from within this distribution. We were previously multiplying the average over the interval (<code>sum / N</code>) times the length of the interval (<code>b - a</code>) to arrive at the final answer. Here, we don't need to multiply by the interval length--that is, we no longer need to multiply the average by $2$.</p> <p>We need to account for the nonuniformity of the PDF of $x$. Failing to account for this nonuniformity will introduce bias in our scene. Indeed, this bias is the source of our inaccurately bright image--if we account for nonuniformity, we will get accurate results. The PDF will \"steer\" samples toward specific parts of the distribution, which will cause us to converge faster, but at the cost of introducing bias. To remove this bias, we need to down-weight where we sample more frequently, and to up-weight where we sample less frequently. For our new nonuniform random number generator, the PDF defines how much or how little we sample a specific portion. So the weighting function should be proportional to $1/\\mathit{pdf}$. In fact it is exactly $1/\\mathit{pdf}$. This is why we divide <code>x*x</code> by <code>pdf(x)</code>.</p> <p>We can try to solve for the integral using the linear PDF $p(r) = \\frac{r}{2}$, for which we were able to solve for the CDF and its inverse. To do that, all we need to do is replace the functions $f = \\sqrt{4d}$ and $pdf = x/2$.</p> Integrating $x^2$ with linear PDF<pre><code>double f(double d) {\nreturn sqrt(4.0 * d);\n}\ndouble pdf(double x) {\nreturn x / 2.0;\n}\nint main() {\nint N = 1000000;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nauto x = f(random_double());\nsum += x*x / pdf(x);\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"I = \" &lt;&lt; sum / N &lt;&lt; '\\n';\n}\n</code></pre> <p>If you compared the runs from the uniform PDF and the linear PDF, you would have probably found that the linear PDF converged faster. If you think about it, a linear PDF is probably a better approximation for a quadratic function than a uniform PDF, so you would expect it to converge faster. If that's the case, then we should just try to make the PDF match the integrand by turning the PDF into a quadratic function:</p> <pre><code>$$ p(r)=\\begin{cases}\n        0           &amp; r &lt; 0           \\\\\n        C \\cdot r^2 &amp; 0 \\leq r \\leq 2 \\\\\n        0           &amp; 2 &lt; r           \\\\\n   \\end{cases}\n$$\n</code></pre> <p>Like the linear PDF, we'll solve for the constant $C$ by integrating to 1 over the interval:</p> <pre><code>$$ 1 = \\int_{0}^{2} C \\cdot r^2 dr $$\n$$ = C \\cdot \\int_{0}^{2} r^2 dr $$\n$$ = C \\cdot \\frac{r^3}{3} \\Big|_{0}^{2} $$\n$$ = C ( \\frac{2^3}{3} - \\frac{0}{3} ) $$\n$$ C = \\frac{3}{8} $$\n</code></pre> <p>Which gives us:</p> <pre><code>$$ p(r)=\\begin{cases}\n        0           &amp; r &lt; 0           \\\\\n        \\frac{3}{8} r^2 &amp; 0 \\leq r \\leq 2 \\\\\n        0           &amp; 2 &lt; r           \\\\\n   \\end{cases}\n$$\n</code></pre> <p>And we get the corresponding CDF:</p> <pre><code>$$ P(r) = \\frac{r^3}{8} $$\n</code></pre> <p>and</p> <pre><code>$$ P^{-1}(x) = f(d) = 8d^\\frac{1}{3} $$\n</code></pre> <p>For just one sample we get:</p> Integrating $x^2$, final version<pre><code>double f(double d) {\nreturn 8.0 * pow(d, 1.0/3.0);\n}\ndouble pdf(double x) {\nreturn (3.0/8.0) * x*x;\n}\nint main() {\nint N = 1;\nauto sum = 0.0;\nfor (int i = 0; i &lt; N; i++) {\nauto x = f(random_double()));\nsum += x*x / pdf(x);\n}\nstd::cout &lt;&lt; std::fixed &lt;&lt; std::setprecision(12);\nstd::cout &lt;&lt; \"I = \" &lt;&lt; sum / N &lt;&lt; '\\n';\n}\n</code></pre> <p>This always returns the exact answer. Which, honestly, feels a bit like magic.</p> <p>A nonuniform PDF \"steers\" more samples to where the PDF is big, and fewer samples to where the PDF is small. By this sampling, we would expect less noise in the places where the PDF is big and more noise where the PDF is small. If we choose a PDF that is higher in the parts of the scene that have higher noise, and is smaller in the parts of the scene that have lower noise, we'll be able to reduce the total noise of the scene with fewer samples. This means that we will be able to converge to the correct scene faster than with a uniform PDF. In effect, we are steering our samples toward the parts of the distribution that are more important. This is why using a carefully chosen nonuniform PDF is usually called importance sampling.</p> <p>In all of the examples given, we always converged to the correct answer of $8/3$. We got the same answer when we used both a uniform PDF and the \"correct\" PDF ($i.e. f(d)=8d^{\\frac{1}{3}}$). While they both converged to the same answer, the uniform PDF took much longer. After all, we only needed a single sample from the PDF that perfectly matched the integral. This should make sense, as we were choosing to sample the important parts of the distribution more often, whereas the uniform PDF just sampled the whole distribution equally, without taking importance into account.</p> <p>Indeed, this is the case for any PDF that you create--they will all converge eventually. This is just another part of the power of the Monte Carlo algorithm. Even the naive PDF where we solved for the 50% value and split the distribution into two halves: $[0, \\sqrt{2}]$ and $[\\sqrt{2}, 2]$. That PDF will converge. Hopefully you should have an intuition as to why that PDF will converge faster than a pure uniform PDF, but slower than the linear PDF ($i.e. f(d) = \\sqrt{4d}$).</p> <p>The perfect importance sampling is only possible when we already know the answer (we got $P$ by integrating $p$ analytically), but it\u2019s a good exercise to make sure our code works.</p> <p>Let's review the main concepts that underlie Monte Carlo ray tracers:</p> <ol> <li>You have an integral of $f(x)$ over some domain $[a,b]$</li> <li>You pick a PDF $p$ that is non-zero and non-negative over $[a,b]$</li> <li>You average a whole ton of $\\frac{f(r)}{p(r)}$ where $r$ is a random number with PDF $p$.</li> </ol> <p>Any choice of PDF $p$ will always converge to the right answer, but the closer that $p$ approximates $f$, the faster that it will converge.</p>"},{"location":"the_rest_of_your_life/orthonormal_bases/","title":"Orthonormal Bases","text":"<p>In the last chapter we developed methods to generate random directions relative to the $z$ axis. If we want to be able to produce reflections off of any surface, we are going to need to make this more general: Not all normals are going to be perfectly aligned with the $z$ axis. So in this chapter we are going to generalize our methods so that they support arbitrary surface normal vectors.</p>"},{"location":"the_rest_of_your_life/orthonormal_bases/#relative-coordinates","title":"Relative Coordinates","text":"<p>An orthonormal basis (ONB) is a collection of three mutually orthogonal unit vectors. It is a strict subtype of coordinate system. The Cartesian $xyz$ axes are one example of an orthonormal basis. All of our renders are the result of the relative positions and orientations of the objects in a scene projected onto the image plane of the camera. The camera and objects must be described in the same coordinate system, so that the projection onto the image plane is logically defined, otherwise the camera has no definitive means of correctly rendering the objects. Either the camera must be redefined in the objects' coordinate system, or the objects must be redefined in the camera's coordinate system. It's best to start with both in the same coordinate system, so no redefinition is necessary. So long as the camera and scene are described in the same coordinate system, all is well. The orthonormal basis defines how distances and orientations are represented in the space, but an orthonormal basis alone is not enough. The objects and the camera need to described by their displacement from a mutually defined location. This is just the origin $\\mathbf{O}$ of the scene; it represents the center of the universe for everything to displace from.</p> <p>Suppose we have an origin $\\mathbf{O}$ and Cartesian unit vectors $\\mathbf{x}$, $\\mathbf{y}$, and $\\mathbf{z}$. When we say a location is (3,-2,7), we really are saying:</p> <pre><code>$$ \\text{Location is } \\mathbf{O} + 3\\mathbf{x} - 2\\mathbf{y} + 7\\mathbf{z} $$\n</code></pre> <p>If we want to measure coordinates in another coordinate system with origin $\\mathbf{O}'$ and basis vectors $\\mathbf{u}$, $\\mathbf{v}$, and $\\mathbf{w}$, we can just find the numbers $(u,v,w)$ such that:</p> <pre><code>$$ \\text{Location is } \\mathbf{O}' + u\\mathbf{u} + v\\mathbf{v} + w\\mathbf{w} $$\n</code></pre>"},{"location":"the_rest_of_your_life/orthonormal_bases/#generating-an-orthonormal-basis","title":"Generating an Orthonormal Basis","text":"<p>If you take an intro to graphics course, there will be a lot of time spent on coordinate systems and 4\u00d74 coordinate transformation matrices. Pay attention, it\u2019s really important stuff! But we won\u2019t be needing it for this book and we'll make do without it. What we do need is to generate random directions with a set distribution relative to the surface normal vector $\\mathbf{n}$. We won\u2019t be needing an origin for this because a direction is relative and has no specific origin. To start off with, we need two cotangent vectors that are each perpendicular to $\\mathbf{n}$ and that are also perpendicular to each other.</p> <p>Some 3D object models will come with one or more cotangent vectors for each vertex. If our model has only one cotangent vector, then the process of making an ONB is a nontrivial one. Suppose we have any vector $\\mathbf{a}$ that is of nonzero length and nonparallel with $\\mathbf{n}$. We can get vectors $\\mathbf{s}$ and $\\mathbf{t}$ perpendicular to $\\mathbf{n}$ by using the property of the cross product that $\\mathbf{n} \\times \\mathbf{a}$ is perpendicular to both $\\mathbf{n}$ and $\\mathbf{a}$:</p> <pre><code>$$ \\mathbf{s} = \\operatorname{unit_vector}(\\mathbf{n} \\times \\mathbf{a}) $$\n\n$$ \\mathbf{t} = \\mathbf{n} \\times \\mathbf{s} $$\n</code></pre> <p>This is all well and good, but the catch is that we may not be given an $\\mathbf{a}$ when we load a model, and our current program doesn't have a way to generate one. If we went ahead and picked an arbitrary $\\mathbf{a}$ to use as an initial vector we may get an $\\mathbf{a}$ that is parallel to $\\mathbf{n}$. So a common method is to pick an arbitrary axis and check to see if it's parallel to $\\mathbf{n}$ (which we assume to be of unit length), if it is, just use another axis:</p> <pre><code>if (fabs(n.x()) &gt; 0.9)\na = vec3(0, 1, 0)\nelse\na = vec3(1, 0, 0)\n</code></pre> <p>We then take the cross product to get $\\mathbf{s}$ and $\\mathbf{t}$</p> <pre><code>vec3 s = unit_vector(cross(n, a));\nvec3 t = cross(n, s);\n</code></pre> <p>Note that we don't need to take the unit vector for $\\mathbf{t}$. Since $\\mathbf{n}$ and $\\mathbf{s}$ are both unit vectors, their cross product $\\mathbf{t}$ will be also. Once we have an ONB of $\\mathbf{s}$, $\\mathbf{t}$, and $\\mathbf{n}$, and we have a random $(x,y,z)$ relative to the $z$ axis, we can get the vector relative to $\\mathbf{n}$ with:</p> <pre><code>$$ \\mathit{Random vector} = x \\mathbf{s} + y \\mathbf{t} + z \\mathbf{n} $$\n</code></pre> <p>If you remember, we used similar math to produce rays from a camera. You can think of that as a change to the camera\u2019s natural coordinate system.</p>"},{"location":"the_rest_of_your_life/orthonormal_bases/#the-onb-class","title":"The ONB Class","text":"<p>Should we make a class for ONBs, or are utility functions enough? I\u2019m not sure, but let\u2019s make a class because it won't really be more complicated than utility functions:</p> Orthonormal basis class<pre><code>#ifndef ONB_H\n#define ONB_H\n#include \"rtweekend.h\"\nclass onb {\npublic:\nonb() {}\nvec3 operator[](int i) const { return axis[i]; }\nvec3&amp; operator[](int i) { return axis[i]; }\nvec3 u() const { return axis[0]; }\nvec3 v() const { return axis[1]; }\nvec3 w() const { return axis[2]; }\nvec3 local(double a, double b, double c) const {\nreturn a*u() + b*v() + c*w();\n}\nvec3 local(const vec3&amp; a) const {\nreturn a.x()*u() + a.y()*v() + a.z()*w();\n}\nvoid build_from_w(const vec3&amp; w) {\nvec3 unit_w = unit_vector(w);\nvec3 a = (fabs(unit_w.x()) &gt; 0.9) ? vec3(0,1,0) : vec3(1,0,0);\nvec3 v = unit_vector(cross(unit_w, a));\nvec3 u = cross(unit_w, v);\naxis[0] = u;\naxis[1] = v;\naxis[2] = unit_w;\n}\npublic:\nvec3 axis[3];\n};\n#endif\n</code></pre> <p>We can rewrite our Lambertian material using this to get:</p> Scatter function, with orthonormal basis<pre><code>class lambertian : public material {\npublic:\n...\nbool scatter(\nconst ray&amp; r_in, const hit_record&amp; rec, color&amp; alb, ray&amp; scattered, double&amp; pdf\n) const override {\nonb uvw;\nuvw.build_from_w(rec.normal);\nauto scatter_direction = uvw.local(random_cosine_direction());\nscattered = ray(rec.p, unit_vector(scatter_direction), r_in.time());\nalb = albedo-&gt;value(rec.u, rec.v, rec.p);\npdf = dot(uvw.w(), scattered.direction()) / pi;\nreturn true;\n}\n...\n};\n</code></pre> <p>Which produces:</p> <p></p> <p>Let\u2019s get rid of some of that noise.</p> <p>But first, let's quickly update the <code>isotropic</code> material:</p> Isotropic material, modified for importance sampling<pre><code>class isotropic : public material {\npublic:\nisotropic(color c) : albedo(make_shared&lt;solid_color&gt;(c)) {}\nisotropic(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(\nconst ray&amp; r_in, const hit_record&amp; rec, color&amp; alb, ray&amp; scattered, double&amp; pdf\n) const override {\nscattered = ray(rec.p, random_unit_vector(), r_in.time());\nattenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\npdf = 1 / (4 * pi);\nreturn true;\n}\ndouble scattering_pdf(const ray&amp; r_in, const hit_record&amp; rec, const ray&amp; scattered)\nconst override {\nreturn 1 / (4 * pi);\n}\nprivate:\nshared_ptr&lt;texture&gt; albedo;\n};\n</code></pre>"},{"location":"the_rest_of_your_life/overview/","title":"Overview","text":"<p>In Ray Tracing in One Weekend and Ray Tracing: the Next Week, you built a \u201creal\u201d ray tracer.</p> <p>If you are motivated, you can take the source and information contained in those books to implement any visual effect you want. The source provides a meaningful and robust foundation upon which to build out a raytracer for a small hobby project. Most of the visual effects found in commercial ray tracers rely on the techniques described in these first two books. However, your capacity to add increasingly complicated visual effects like subsurface scattering or nested dielectrics will be severely limited by a missing mathematical foundation. In this volume, I assume that you are either a highly interested student, or are someone who is pursuing a career related to ray tracing. We will be diving into the math of creating a very serious ray tracer. When you are done, you should be well equipped to use and modify the various commercial ray tracers found in many popular domains, such as the movie, television, product design, and architecture industries.</p> <p>There are many many things I do not cover in this short volume. For example, there are many ways of writing Monte Carlo rendering programs--I dive into only one of them. I don\u2019t cover shadow rays (deciding instead to make rays more likely to go toward lights), nor do I cover bidirectional methods, Metropolis methods, or photon mapping. You'll find many of these techniques in the so-called \"serious ray tracers\", but they are not covered here because it is more important to cover the concepts, math, and terms of the field. I think of this book as a deep exposure that should be your first of many, and it will equip you with some of the concepts, math, and terms that you'll need in order to study these and other interesting techniques.</p> <p>I hope that you find the math as fascinating as I do.</p> <p>As before, https://in1weekend.blogspot.com/ will have further readings and references.</p> <p>Thanks to everyone who lent a hand on this project. You can find them in the acknowledgments section at the end of this book.</p>"},{"location":"the_rest_of_your_life/playing_with_importance_sampling/","title":"Playing with Importance Sampling","text":"<p>Our goal over the next several chapters is to instrument our program to send a bunch of extra rays   toward light sources so that our picture is less noisy. Let\u2019s assume we can send a bunch of rays toward the light source using a PDF   $\\operatorname{pLight}(\\omega_o)$. Let\u2019s also assume we have a PDF related to $operatorname{pScatter}$, and let\u2019s call that   $operatorname{pSurface}(\\omega_o)$. A great thing about PDFs is that you can just use linear mixtures of them to form mixture densities   that are also PDFs. For example, the simplest would be:</p> <p>$$ p(\\omega_o) = \\frac{1}{2} \\operatorname{pSurface}(\\omega_o) +  \\frac{1}{2}       \\operatorname{pLight}(\\omega_o)$$</p> <p>As long as the weights are positive and add up to one, any such mixture of PDFs is a PDF. Remember, we can use any PDF: all PDFs eventually converge to the correct answer. So, the game is to figure out how to make the PDF larger where the product</p> <pre><code>$$ \\operatorname{pScatter}(\\mathbf{x}, \\omega_i, \\omega_o) \\cdot\n    \\operatorname{Color}_i(\\mathbf{x}, \\omega_i) $$\n</code></pre> <p>is largest. For diffuse surfaces, this is mainly a matter of guessing where   $\\operatorname{Color}_i(\\mathbf{x}, \\omega_i)$ is largest. Which is equivalent to guessing where the most light is coming from.</p> <p>For a mirror, $\\operatorname{pScatter}()$ is huge only near one direction, so   $\\operatorname{pScatter}()$ matters a lot more. In fact, most renderers just make mirrors a special case, and make the   $\\operatorname{pScatter}()/p()$ implicit -- our code currently does that.</p>"},{"location":"the_rest_of_your_life/playing_with_importance_sampling/#returning-to-the-cornell-box","title":"Returning to the Cornell Box","text":"<p>Let\u2019s adjust some parameters for the Cornell box:</p> Cornell box, refactored<pre><code>int main() {\n...\ncam.samples_per_pixel = 100;\n...\n}\n</code></pre> <p>At 600\u00d7600 my code produces this image in 15min on 1 core of my Macbook:</p> <p></p> <p>Reducing that noise is our goal. We\u2019ll do that by constructing a PDF that sends more rays to the light.</p> <p>First, let\u2019s instrument the code so that it explicitly samples some PDF and then normalizes for that. Remember Monte Carlo basics: $\\int f(x) \\approx \\sum f(r)/p(r)$. For the Lambertian material, let\u2019s sample like we do now: $p(\\omega_o) = \\cos(\\theta_o) / \\pi$.</p> <p>We modify the base-class <code>material</code> to enable this importance sampling:</p> The material class, adding importance sampling<pre><code>class material {\npublic:\n...\nvirtual double scattering_pdf(const ray&amp; r_in, const hit_record&amp; rec, const ray&amp; scattered)\nconst {\nreturn 0;\n}\n};\n</code></pre> <p>And the <code>lambertian</code> material becomes:</p> Lambertian material, modified for importance sampling<pre><code>class lambertian : public material {\npublic:\nlambertian(const color&amp; a) : albedo(make_shared&lt;solid_color&gt;(a)) {}\nlambertian(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nauto scatter_direction = rec.normal + random_unit_vector();\n// Catch degenerate scatter direction\nif (scatter_direction.near_zero())\nscatter_direction = rec.normal;\nscattered = ray(rec.p, scatter_direction, r_in.time());\nattenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\nreturn true;\n}\ndouble scattering_pdf(const ray&amp; r_in, const hit_record&amp; rec, const ray&amp; scattered) const {\nauto cos_theta = dot(rec.normal, unit_vector(scattered.direction()));\nreturn cos_theta &lt; 0 ? 0 : cos_theta/pi;\n}\nprivate:\nshared_ptr&lt;texture&gt; albedo;\n};\n</code></pre> <p>And the <code>camera::ray_color</code> function gets a minor modification:</p> The ray_color function, modified for importance sampling<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\n// If the ray hits nothing, return the background color.\nif (!world.hit(r, interval(0.001, infinity), rec))\nreturn background;\nray scattered;\ncolor attenuation;\ncolor color_from_emission = rec.mat-&gt;emitted(rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered))\nreturn color_from_emission;\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ndouble pdf = scattering_pdf;\ncolor color_from_scatter =\n(attenuation * scattering_pdf * ray_color(scattered, depth-1, world)) / pdf;\nreturn color_from_emission + color_from_scatter;\n}\n};\n</code></pre> <p>You should get exactly the same picture. Which should make sense, as the scattered part of <code>ray_color</code> is getting multiplied by <code>scattering_pdf / pdf</code>, and as <code>pdf</code> is equal to <code>scattering_pdf</code> is just the same as multiplying by one.</p>"},{"location":"the_rest_of_your_life/playing_with_importance_sampling/#using-a-uniform-pdf-instead-of-a-perfect-match","title":"Using a Uniform PDF Instead of a Perfect Match","text":"<p>Now, just for the experience, let's try using a different sampling PDF. We'll continue to have our reflected rays weighted by Lambertian, so $\\cos(\\theta_o)$, and we'll keep the scattering PDF as is, but we'll use a different PDF in the denominator. We will sample using a uniform PDF about the hemisphere, so we'll set the denominator to $1/2\\pi$. This will still converge on the correct answer, as all we've done is change the PDF, but since the PDF is now less of a perfect match for the real distribution, it will take longer to converge. Which, for the same number of samples means a noisier image:</p> The ray_color function, now with a uniform PDF in the denominator<pre><code>class camera {\n...\nprivate:\n...\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\n// If the ray hits nothing, return the background color.\nif (!world.hit(r, interval(0.001, infinity), rec))\nreturn background;\nray scattered;\ncolor attenuation;\ncolor color_from_emission = rec.mat-&gt;emitted(rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered))\nreturn color_from_emission;\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ndouble pdf = 1 / (2*pi);\ncolor color_from_scatter =\n(attenuation * scattering_pdf * ray_color(scattered, depth-1, world)) / pdf;\nreturn color_from_emission + color_from_scatter;\n}\n</code></pre> <p>You should get a very similar result to before, only with slightly more noise, it may be hard to see.</p> <p></p> <p>Make sure to return the PDF to the scattering PDF.</p> Return the PDF to the same as scattering PDF<pre><code>...\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ndouble pdf = scattering_pdf;\n...\n</code></pre>"},{"location":"the_rest_of_your_life/playing_with_importance_sampling/#random-hemispherical-sampling","title":"Random Hemispherical Sampling","text":"<p>To confirm our understanding, let's try a different scattering distribution. For this one, we'll attempt to repeat the uniform hemispherical scattering from the first book. There's nothing wrong with this technique, but we are no longer treating our objects as Lambertian. Lambertian is a specific type of diffuse material that requires a $\\cos(\\theta_o)$ scattering distribution. Uniform hemispherical scattering is a different diffuse material. If we keep the material the same but change the PDF, as we did in last section, we will still converge on the same answer, but our convergence may take more or less samples. However, if we change the material, we will have fundamentally changed the render and the algorithm will converge on a different answer. So when we replace Lambertian diffuse with uniform hemispherical diffuse we should expect the outcome of our render to be materially different. We're going to adjust our scattering direction and scattering PDF:</p> Modified PDF and scatter function<pre><code>class lambertian : public material {\npublic:\nlambertian(const color&amp; a) : albedo(make_shared&lt;solid_color&gt;(a)) {}\nlambertian(shared_ptr&lt;texture&gt; a) : albedo(a) {}\nbool scatter(const ray&amp; r_in, const hit_record&amp; rec, color&amp; attenuation, ray&amp; scattered)\nconst override {\nauto scatter_direction = random_in_hemisphere(rec.normal);\n// Catch degenerate scatter direction\nif (scatter_direction.near_zero())\nscatter_direction = rec.normal;\nscattered = ray(rec.p, scatter_direction, r_in.time());\nattenuation = albedo-&gt;value(rec.u, rec.v, rec.p);\nreturn true;\n}\ndouble scattering_pdf(const ray&amp; r_in, const hit_record&amp; rec, const ray&amp; scattered) const {\nreturn 1 / (2*pi);\n}\n...\n</code></pre> <p>This new diffuse material is actually just $p(\\omega_o) = \\frac{1}{2\\pi}$ for the scattering PDF. So our uniform PDF that was an imperfect match for Lambertian diffuse is actually a perfect match for our uniform hemispherical diffuse. When rendering, we should get a slightly different image.</p> <p></p> <p>It\u2019s pretty close to our old picture, but there are differences that are not just noise. The front of the tall box is much more uniform in color. If you aren't sure what the best sampling pattern for your material is, it's pretty reasonable to just go ahead and assume a uniform PDF, and while that might converge slowly, it's not going to ruin your render. That said, if you're not sure what the correct sampling pattern for your material is, your choice of PDF is not going to be your biggest concern, as incorrectly choosing your scattering function will ruin your render. At the very least it will produce an incorrect result. You may find yourself with the most difficult kind of bug to find in a Monte Carlo program -- a bug that produces a reasonable looking image! You won\u2019t know if the bug is in the first version of the program, or the second, or both!</p> <p>Let\u2019s build some infrastructure to address this.</p>"},{"location":"the_rest_of_your_life/sampling_lights_directly/","title":"Sampling Lights Directly","text":"<p>The problem with sampling uniformly over all directions is that lights are no more likely to be sampled than any arbirary or unimportant direction. We could use shadow rays to solve for the direct lighting at any given point. Instead, I\u2019ll just use a PDF that sends more rays to the light. We can then turn around and change that PDF to send more rays in whatever direction we want.</p> <p>It\u2019s really easy to pick a random direction toward the light; just pick a random point on the light and send a ray in that direction. But we'll need to know the PDF, $p(\\omega)$, so that we're not biasing our render. But what is that?</p>"},{"location":"the_rest_of_your_life/sampling_lights_directly/#getting-the-pdf-of-a-light","title":"Getting the PDF of a Light","text":"<p>For a light with a surface area of $A$, if we sample uniformly on that light, the PDF on the surface is just $\\frac{1}{A}$. How much area does the entire surface of the light take up if its projected back onto the unit sphere? Fortunately, there is a simple correspondence, as outlined in this diagram:</p> <p></p> <p>If we look at a small area $dA$ on the light, the probability of sampling it is   $\\operatorname{p_q}(q) \\cdot dA$. On the sphere, the probability of sampling the small area $d\\omega$ on the sphere is   $\\operatorname{p}(\\omega) \\cdot d\\omega$. There is a geometric relationship between $d\\omega$ and $dA$:</p> <pre><code>$$ d\\omega = \\frac{dA \\cdot \\cos(\\theta)}{\\operatorname{distance}^2(p,q)} $$\n</code></pre> <p>Since the probability of sampling $d\\omega$ and $dA$ must be the same, then</p> <pre><code>$$ \\operatorname{p}(\\omega) \\cdot d\\omega = \\operatorname{p_q}(q) \\cdot dA $$\n$$ \\operatorname{p}(\\omega)\n   \\cdot \\frac{dA \\cdot \\cos(\\theta)}{\\operatorname{distance}^2(p,q)}\n   = \\operatorname{p_q}(q) \\cdot dA $$\n</code></pre> <p>We know that if we sample uniformly on the light the PDF on the surface is $\\frac{1}{A}$:</p> <pre><code>$$ \\operatorname{p_q}(q) = \\frac{1}{A} $$\n$$ \\operatorname{p}(\\omega) \\cdot \\frac{dA \\cdot \\cos(\\theta)}{\\operatorname{distance}^2(p,q)}\n   =  \\frac{dA}{A} $$\n</code></pre> <p>So</p> <p>$$ \\operatorname{p}(\\omega) = \\frac{\\operatorname{distance}^2(p,q)}{\\cos(\\theta) \\cdot A} $$</p>"},{"location":"the_rest_of_your_life/sampling_lights_directly/#light-sampling","title":"Light Sampling","text":"<p>We can hack our <code>ray_color()</code> function to sample the light in a very hard-coded fashion just to check that we got the math and concept right:</p> Ray color with light sampling<pre><code>class camera {\n...\nprivate:\ncolor ray_color(const ray&amp; r, int depth, const hittable&amp; world) const {\nhit_record rec;\n// If we've exceeded the ray bounce limit, no more light is gathered.\nif (depth &lt;= 0)\nreturn color(0,0,0);\n// If the ray hits nothing, return the background color.\nif (!world.hit(r, interval(0.001, infinity), rec))\nreturn background;\nray scattered;\ncolor attenuation;\ndouble pdf;\ncolor color_from_emission = rec.mat-&gt;emitted(rec.u, rec.v, rec.p);\nif (!rec.mat-&gt;scatter(r, rec, attenuation, scattered, pdf))\nreturn color_from_emission;\nauto on_light = point3(random_double(213,343), 554, random_double(227,332));\nauto to_light = on_light - rec.p;\nauto distance_squared = to_light.length_squared();\nto_light = unit_vector(to_light);\nif (dot(to_light, rec.normal) &lt; 0)\nreturn color_from_emission;\ndouble light_area = (343-213)*(332-227);\nauto light_cosine = fabs(to_light.y());\nif (light_cosine &lt; 0.000001)\nreturn color_from_emission;\npdf = distance_squared / (light_cosine * light_area);\nscattered = ray(rec.p, to_light, r.time());\ndouble scattering_pdf = rec.mat-&gt;scattering_pdf(r, rec, scattered);\ncolor color_from_scatter =\n(attenuation * scattering_pdf * ray_color(scattered, depth-1, world)) / pdf;\nreturn color_from_emission + color_from_scatter;\n}\n};\n</code></pre> <p>With 10 samples per pixel this yields:</p> <p></p> <p>This is about what we would expect from something that samples only the light sources, so this appears to work.</p>"},{"location":"the_rest_of_your_life/sampling_lights_directly/#switching-to-unidirectional-light","title":"Switching to Unidirectional Light","text":"<p>The noisy pops around the light on the ceiling are because the light is two-sided and there is a small space between light and ceiling. We probably want to have the light just emit down. We can do that by letting the emitted member function of hittable take extra information:</p> Material emission, directional<pre><code>class material {\npublic:\n...\nvirtual color emitted(\nconst ray&amp; r_in, const hit_record&amp; rec, double u, double v, const point3&amp; p\n) const {\nreturn color(0,0,0);\n}\n...\n};\nclass diffuse_light : public material {\npublic:\n...\ncolor emitted(const ray&amp; r_in, const hit_record&amp; rec, double u, double v, const point3&amp; p)\nconst override {\nif (!rec.front_face)\nreturn color(0,0,0);\nreturn emit-&gt;value(u, v, p);\n}\n...\n};\n</code></pre> <p>This gives us:</p> <p></p>"},{"location":"the_rest_of_your_life/some_architectural_decisions/","title":"Some Architectural Decisions","text":"<p>We won't write any code in this chapter. We\u2019re at a crossroads and we need to make some architectural decisions.</p> <p>The mixture-density approach is an alternative to having more traditional shadow rays. These are rays that check for an unobstructed path from an intersection point to a given light source. Rays that intersect an object between a point and a given light source indicate that the intersection point is in the shadow of that particular light source. The mixture-density approach is something that I personally prefer, because in addition to lights, you can sample windows or bright cracks under doors or whatever else you think might be bright -- or important. But you'll still see shadow rays in most professional path tracers. Typically they'll have a predefined number of shadow rays (e.g 1, 4, 8, 16) where over the course of rendering, at each place where the path tracing ray intersects, they'll send these terminal shadow rays to random lights in the scene to determine if the intersection is lit by that random light. The intersection will either be lit by that light, or completely in shadow, where more shadow rays lead to a more accurate illumination. After all of the shadow rays terminate (either at a light or at an occluding surface), the inital path tracing ray continues on and more shadow rays are sent at the next intersection. You can't tell the shadow rays what is important, you can only tell them what is emissive, so shadow rays work best on simpler scenes that don't have overly complicated photon distribution. That said, shadow rays terminate at the first thing they run into and don't bounce around, so one shadow ray is cheaper than one path tracing ray, which is the reason that you'll typically see a lot more shadow rays than path tracing rays (e.g 1, 4, 8, 16). You could choose shadow rays over mixture-density in a more restricted scene; that\u2019s a personal design preference. Shadow rays tend to be cheaper for a crude result than mixture-density and is becoming increasingly common in realtime.</p> <p>There are some other issues with the code.</p> <p>The PDF construction is hard coded in the <code>ray_color()</code> function. We should clean that up.</p> <p>We've accidentally broken the specular rays (glass and metal), and they are no longer supported. The math would continue to work out if we just made their scattering function a delta function, but that would lead to all kinds of floating point disasters. We could either make specular reflection a special case that skips $f()/p()$, or we could set surface roughness to a very small -- but nonzero -- value and have almost-mirrors that look perfectly smooth but that don\u2019t generate NaNs. I don\u2019t have an opinion on which way to do it (I have tried both and they both have their advantages), but we have smooth metal and glass code anyway, so we'll add perfect specular surfaces that just skip over explicit $f()/p()$ calculations.</p> <p>We also lack a real background function infrastructure in case we want to add an environment map or a more interesting functional background. Some environment maps are HDR (the RGB components are normalized floats rather than 0\u2013255 bytes). Our output has been HDR all along; we\u2019ve just been truncating it.</p> <p>Finally, our renderer is RGB. A more physically based one -- like an automobile manufacturer might use -- would probably need to use spectral colors and maybe even polarization. For a movie renderer, most studios still get away with RGB. You can make a hybrid renderer that has both modes, but that is of course harder. I\u2019m going to stick to RGB for now, but I will touch on this at the end of the book.</p>"},{"location":"the_rest_of_your_life/the_rest_of_your_life/","title":"The Rest of Your Life","text":"<p>The purpose of this book was to walk through all of the little details (dotting all the i's and crossing all of the t's) necessary when organizing a physically based renderer\u2019s sampling approach. You should now be able to take all of this detail and explore a lot of different potential paths.</p> <p>If you want to explore Monte Carlo methods, look into bidirectional and path spaced approaches such as Metropolis. Your probability space won't be over solid angle, but will instead be over path space, where a path is a multidimensional point in a high-dimensional space. Don\u2019t let that scare you -- if you can describe an object with an array of numbers, mathematicians call it a point in the space of all possible arrays of such points. That\u2019s not just for show. Once you get a clean abstraction like that, your code can get clean too. Clean abstractions are what programming is all about!</p> <p>If you want to do movie renderers, look at the papers out of studios and Solid Angle. They are surprisingly open about their craft.</p> <p>If you want to do high-performance ray tracing, look first at papers from Intel and NVIDIA. They are also surprisingly open.</p> <p>If you want to do hard-core physically based renderers, convert your renderer from RGB to spectral. I am a big fan of each ray having a random wavelength and almost all the RGBs in your program turning into floats. It sounds inefficient, but it isn\u2019t!</p> <p>Regardless of what direction you take, add a glossy BRDF model. There are many to choose from, and each has its advantages.</p> <p>Have fun!</p> <p>[Peter Shirley][] Salt Lake City, March, 2016</p>"}]}